{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 4: Multi-Variant Test - Landing Page Optimization (A/B/C/D)\n",
    "\n",
    "## Scenario\n",
    "A marketing team wants to test **four different landing page designs** (A, B, C, D) simultaneously to determine which one drives the most sign-ups and highest engagement (time on page). Instead of running multiple A/B tests sequentially, they run a single multi-variant test.\n",
    "\n",
    "**Tests used:** ANOVA, Kruskal-Wallis, Chi-Square (for proportions), Tukey HSD post-hoc, Bonferroni correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Multi-Variant Testing?\n",
    "\n",
    "| Approach | Pros | Cons |\n",
    "|----------|------|------|\n",
    "| **Sequential A/B tests** (A vs B, then winner vs C...) | Simple to analyze | Takes much longer; environment may change between tests |\n",
    "| **Multi-variant A/B/n test** | Tests all variants simultaneously; faster overall | More complex analysis; requires more total traffic; multiple comparison problem |\n",
    "\n",
    "**Key challenge:** When comparing multiple groups, the probability of a false positive increases:\n",
    "\n",
    "$$P(\\text{at least 1 false positive}) = 1 - (1 - \\alpha)^k$$\n",
    "\n",
    "For 4 groups with 6 pairwise comparisons at $\\alpha = 0.05$: $1 - (0.95)^6 = 0.265$ (26.5% chance!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Simulated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_per_group = 500\n",
    "\n",
    "# Time on page (seconds) - continuous metric\n",
    "# Design A: Current (baseline) \n",
    "# Design B: Minor tweak (no real effect)\n",
    "# Design C: Significant improvement\n",
    "# Design D: Even better\n",
    "time_A = np.random.normal(45, 15, n_per_group)   # mean=45s, std=15s\n",
    "time_B = np.random.normal(46, 14, n_per_group)   # tiny difference from A\n",
    "time_C = np.random.normal(52, 16, n_per_group)   # meaningful improvement\n",
    "time_D = np.random.normal(55, 15, n_per_group)   # best\n",
    "\n",
    "# Clip negative values (time can't be negative)\n",
    "time_A = np.clip(time_A, 1, None)\n",
    "time_B = np.clip(time_B, 1, None)\n",
    "time_C = np.clip(time_C, 1, None)\n",
    "time_D = np.clip(time_D, 1, None)\n",
    "\n",
    "# Sign-up rate (binary metric)\n",
    "signup_A = np.random.binomial(1, 0.10, n_per_group)\n",
    "signup_B = np.random.binomial(1, 0.11, n_per_group)\n",
    "signup_C = np.random.binomial(1, 0.14, n_per_group)\n",
    "signup_D = np.random.binomial(1, 0.15, n_per_group)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'variant': ['A'] * n_per_group + ['B'] * n_per_group + ['C'] * n_per_group + ['D'] * n_per_group,\n",
    "    'time_on_page': np.concatenate([time_A, time_B, time_C, time_D]),\n",
    "    'signed_up': np.concatenate([signup_A, signup_B, signup_C, signup_D])\n",
    "})\n",
    "\n",
    "print(\"=== Summary Statistics ===\")\n",
    "summary = df.groupby('variant').agg(\n",
    "    n=('time_on_page', 'count'),\n",
    "    mean_time=('time_on_page', 'mean'),\n",
    "    std_time=('time_on_page', 'std'),\n",
    "    signup_rate=('signed_up', 'mean')\n",
    ").round(3)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time on page distributions\n",
    "colors = ['#3498db', '#e67e22', '#2ecc71', '#e74c3c']\n",
    "for variant, color in zip(['A', 'B', 'C', 'D'], colors):\n",
    "    data = df[df['variant'] == variant]['time_on_page']\n",
    "    axes[0].hist(data, bins=30, alpha=0.5, color=color, label=f'Variant {variant}', density=True)\n",
    "axes[0].set_xlabel('Time on Page (seconds)')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('Time on Page Distribution by Variant')\n",
    "axes[0].legend()\n",
    "\n",
    "# Sign-up rates\n",
    "signup_rates = df.groupby('variant')['signed_up'].mean()\n",
    "bars = axes[1].bar(signup_rates.index, signup_rates.values, color=colors, alpha=0.8)\n",
    "for bar, rate in zip(bars, signup_rates.values):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.002,\n",
    "                f'{rate:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "axes[1].set_ylabel('Sign-up Rate')\n",
    "axes[1].set_title('Sign-up Rate by Variant')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysis: Continuous Metric (Time on Page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 One-Way ANOVA\n",
    "\n",
    "Tests whether the means of the groups are all equal:\n",
    "- $H_0$: $\\mu_A = \\mu_B = \\mu_C = \\mu_D$\n",
    "- $H_a$: At least one mean is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check assumptions first\n",
    "# 1. Normality (Shapiro-Wilk) - with large n, CLT applies regardless\n",
    "print(\"=== Normality Check (Shapiro-Wilk) ===\")\n",
    "for variant in ['A', 'B', 'C', 'D']:\n",
    "    data = df[df['variant'] == variant]['time_on_page']\n",
    "    stat, pval = stats.shapiro(data)\n",
    "    print(f\"Variant {variant}: W={stat:.4f}, p={pval:.4f} {'(normal)' if pval > 0.05 else '(not normal)'}\")\n",
    "\n",
    "# 2. Equal variances (Levene's test)\n",
    "levene_stat, levene_pval = stats.levene(time_A, time_B, time_C, time_D)\n",
    "print(f\"\\n=== Levene's Test for Equal Variances ===\")\n",
    "print(f\"Statistic: {levene_stat:.4f}, p-value: {levene_pval:.4f}\")\n",
    "print(f\"{'Equal variances assumed' if levene_pval > 0.05 else 'Unequal variances - use Welch ANOVA'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Way ANOVA\n",
    "f_stat, anova_pval = stats.f_oneway(time_A, time_B, time_C, time_D)\n",
    "\n",
    "print(\"=== One-Way ANOVA ===\")\n",
    "print(f\"F-statistic: {f_stat:.4f}\")\n",
    "print(f\"p-value: {anova_pval:.6f}\")\n",
    "if anova_pval < 0.05:\n",
    "    print(\"REJECT H0: At least one group mean is significantly different.\")\n",
    "    print(\"But ANOVA doesn't tell us WHICH groups differ -> need post-hoc tests.\")\n",
    "else:\n",
    "    print(\"FAIL TO REJECT H0: No significant difference among group means.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Kruskal-Wallis Test (Non-parametric alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_stat, kw_pval = stats.kruskal(time_A, time_B, time_C, time_D)\n",
    "\n",
    "print(\"=== Kruskal-Wallis Test ===\")\n",
    "print(f\"H-statistic: {kw_stat:.4f}\")\n",
    "print(f\"p-value: {kw_pval:.6f}\")\n",
    "if kw_pval < 0.05:\n",
    "    print(\"REJECT H0: Distributions differ significantly.\")\n",
    "else:\n",
    "    print(\"FAIL TO REJECT H0: No significant difference.\")\n",
    "\n",
    "print(f\"\\nNote: Kruskal-Wallis is the non-parametric equivalent of ANOVA.\")\n",
    "print(f\"Use when data is not normally distributed or for ordinal data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Post-Hoc Test: Tukey HSD\n",
    "\n",
    "ANOVA tells us *that* groups differ. **Tukey's Honestly Significant Difference** tells us *which* pairs differ, while controlling for multiple comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tukey = pairwise_tukeyhsd(\n",
    "    endog=df['time_on_page'],\n",
    "    groups=df['variant'],\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "print(\"=== Tukey HSD Post-Hoc Test ===\")\n",
    "print(tukey)\n",
    "\n",
    "# Visualize\n",
    "fig = tukey.plot_simultaneous(figsize=(10, 5))\n",
    "plt.title('Tukey HSD: 95% Confidence Intervals for Pairwise Differences')\n",
    "plt.xlabel('Time on Page (seconds)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis: Binary Metric (Sign-up Rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Chi-Square Test for Multiple Proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contingency table\n",
    "contingency = pd.crosstab(df['variant'], df['signed_up'])\n",
    "contingency.columns = ['Not Signed Up', 'Signed Up']\n",
    "print(\"=== Contingency Table ===\")\n",
    "print(contingency)\n",
    "\n",
    "# Chi-square test\n",
    "chi2, chi2_pval, dof, expected = stats.chi2_contingency(contingency.values)\n",
    "\n",
    "print(f\"\\n=== Chi-Square Test ===\")\n",
    "print(f\"Chi-square statistic: {chi2:.4f}\")\n",
    "print(f\"Degrees of freedom: {dof}\")\n",
    "print(f\"p-value: {chi2_pval:.4f}\")\n",
    "print(f\"\\nAll expected frequencies >= 5: {(expected >= 5).all()}\")\n",
    "if chi2_pval < 0.05:\n",
    "    print(\"REJECT H0: Sign-up rates differ significantly across variants.\")\n",
    "else:\n",
    "    print(\"FAIL TO REJECT H0: No significant difference in sign-up rates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Pairwise Comparisons with Bonferroni Correction\n",
    "\n",
    "For proportions, Tukey HSD doesn't apply directly. We use pairwise Z-tests with Bonferroni correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = ['A', 'B', 'C', 'D']\n",
    "n_comparisons = len(variants) * (len(variants) - 1) // 2  # 6 pairwise comparisons\n",
    "bonferroni_alpha = 0.05 / n_comparisons\n",
    "\n",
    "print(f\"Number of pairwise comparisons: {n_comparisons}\")\n",
    "print(f\"Bonferroni-corrected alpha: {bonferroni_alpha:.4f}\")\n",
    "print(f\"\\n{'Pair':<10} {'p-value':<12} {'Corrected p':<14} {'Significant':<12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "pairwise_results = []\n",
    "for i in range(len(variants)):\n",
    "    for j in range(i+1, len(variants)):\n",
    "        v1, v2 = variants[i], variants[j]\n",
    "        d1 = df[df['variant'] == v1]['signed_up']\n",
    "        d2 = df[df['variant'] == v2]['signed_up']\n",
    "        \n",
    "        z, p = proportions_ztest(\n",
    "            [d1.sum(), d2.sum()],\n",
    "            [len(d1), len(d2)],\n",
    "            alternative='two-sided'\n",
    "        )\n",
    "        \n",
    "        corrected_p = min(p * n_comparisons, 1.0)  # Bonferroni correction\n",
    "        sig = corrected_p < 0.05\n",
    "        \n",
    "        print(f\"{v1} vs {v2:<5} {p:<12.4f} {corrected_p:<14.4f} {'Yes *' if sig else 'No'}\")\n",
    "        pairwise_results.append({'pair': f'{v1} vs {v2}', 'p_value': p, 'corrected_p': corrected_p, 'significant': sig})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary & Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY OF MULTI-VARIANT TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- Time on Page (Continuous Metric) ---\")\n",
    "print(f\"ANOVA p-value: {anova_pval:.6f} -> {'Significant' if anova_pval < 0.05 else 'Not significant'}\")\n",
    "print(\"\\nTukey HSD significant pairs:\")\n",
    "for i in range(len(tukey.reject)):\n",
    "    if tukey.reject[i]:\n",
    "        print(f\"  {tukey.groupsunique[tukey._results_table.data[i+1][0]]} vs \"\n",
    "              f\"{tukey.groupsunique[tukey._results_table.data[i+1][1]]}\")\n",
    "\n",
    "print(\"\\n--- Sign-up Rate (Binary Metric) ---\")\n",
    "print(f\"Chi-square p-value: {chi2_pval:.4f} -> {'Significant' if chi2_pval < 0.05 else 'Not significant'}\")\n",
    "print(\"\\nBonferroni-corrected significant pairs:\")\n",
    "for r in pairwise_results:\n",
    "    if r['significant']:\n",
    "        print(f\"  {r['pair']}\")\n",
    "\n",
    "print(\"\\n--- Recommendation ---\")\n",
    "print(\"Based on both metrics:\")\n",
    "print(f\"  Best variant for time on page: D (mean={time_D.mean():.1f}s)\")\n",
    "print(f\"  Best variant for sign-up rate: D (rate={signup_D.mean():.1%})\")\n",
    "print(\"  Variant D consistently outperforms across metrics -> Recommend launching D.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interview Follow-Up Questions & Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Why not just run multiple separate A/B tests (A vs B, A vs C, A vs D)?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Running separate tests has two major problems:\n",
    "\n",
    "1. **Multiple comparisons inflate false positives.** With 3 separate tests at $\\alpha = 0.05$, the probability of at least one false positive is $1 - (0.95)^3 = 14.3\\%$, nearly 3x the intended 5% rate.\n",
    "\n",
    "2. **Temporal confounds.** Running tests sequentially means the environment may change between tests (seasonality, marketing campaigns, product changes), making comparisons invalid.\n",
    "\n",
    "A multi-variant test with proper correction (Bonferroni, Tukey HSD) tests all variants simultaneously while controlling the family-wise error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: What is the difference between Bonferroni and Tukey HSD corrections?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "| Method | Approach | Strictness | Best For |\n",
    "|--------|----------|------------|----------|\n",
    "| **Bonferroni** | Divides $\\alpha$ by number of comparisons | Very conservative | Any type of test; few comparisons |\n",
    "| **Tukey HSD** | Uses studentized range distribution | Less conservative | All pairwise mean comparisons; balanced designs |\n",
    "| **Benjamini-Hochberg** | Controls False Discovery Rate (FDR) | Least conservative | Many comparisons; exploratory analysis |\n",
    "\n",
    "Bonferroni is **more conservative** (fewer false positives, but higher false negative rate). Tukey HSD is specifically designed for all pairwise mean comparisons after ANOVA and is generally preferred in that context. Benjamini-Hochberg is useful when you're running many tests and are willing to tolerate some false discoveries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: ANOVA is significant but none of the pairwise comparisons are. How is that possible?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "This can happen because:\n",
    "\n",
    "1. **ANOVA is an omnibus test** - it tests whether ANY group differs from ANY other. The overall F-test pools information across all groups, giving it more power.\n",
    "\n",
    "2. **Post-hoc tests apply corrections** that reduce power for individual comparisons. Bonferroni in particular can be very conservative.\n",
    "\n",
    "3. **The effect may be distributed** - small differences between many pairs can produce a significant overall F-test without any single pair being significant.\n",
    "\n",
    "**What to do:** Consider using a less conservative correction (Tukey HSD or Benjamini-Hochberg), or increase sample size. Report the overall ANOVA result honestly and note that specific pairwise differences were not detectable with the available power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: How much more traffic do you need for an A/B/C/D test vs an A/B test?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "With 4 variants:\n",
    "- **Total traffic** is roughly 4x a single A/B test (you need n per group for each of 4 groups)\n",
    "- If using Bonferroni correction with 6 pairwise comparisons, each comparison uses $\\alpha/6 \\approx 0.0083$, which requires **larger samples per comparison** to maintain power\n",
    "- Rule of thumb: expect to need about **2-3x the total traffic** compared to a simple A/B test to achieve similar power for detecting differences between any two groups\n",
    "\n",
    "**Trade-off:** More variants = more traffic needed = longer test duration, BUT you learn about all variants simultaneously rather than testing sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: When would you use Kruskal-Wallis instead of ANOVA?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Use Kruskal-Wallis when:\n",
    "\n",
    "1. **Data is not normally distributed** and sample sizes are small (CLT doesn't help)\n",
    "2. **Ordinal data** (e.g., user satisfaction: 1-5 stars)\n",
    "3. **Heavy outliers** that distort means\n",
    "4. **Variances are very unequal** across groups\n",
    "\n",
    "Kruskal-Wallis compares **rank distributions** rather than means, making it robust to the above issues. The trade-off: it has less statistical power than ANOVA when ANOVA assumptions hold, and its post-hoc test (Dunn's test) is less elegant than Tukey HSD."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
