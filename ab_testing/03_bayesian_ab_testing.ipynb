{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 3: Bayesian A/B Test - Subscription Page Pricing\n",
    "\n",
    "## Scenario\n",
    "A streaming platform wants to test whether a **new pricing page layout** increases the subscription sign-up rate. Instead of a traditional frequentist approach, the team wants to use **Bayesian A/B testing** to get a direct probability that variant B is better than A, and to potentially stop the test early if there's strong evidence.\n",
    "\n",
    "Current sign-up rate: ~8%. The team wants to know *\"What is the probability that the new page is better?\"*\n",
    "\n",
    "**Methods used:** Beta-Binomial model, Posterior analysis, Monte Carlo simulation, Expected Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bayesian vs Frequentist: Key Differences\n",
    "\n",
    "| Aspect | Frequentist | Bayesian |\n",
    "|--------|-------------|----------|\n",
    "| **Question answered** | \"What's the probability of this data given no effect?\" | \"What's the probability B is better than A?\" |\n",
    "| **Core output** | p-value, confidence interval | Posterior distribution, credible interval |\n",
    "| **Parameters** | Fixed (unknown) constants | Random variables with distributions |\n",
    "| **Prior knowledge** | Not incorporated | Incorporated through priors |\n",
    "| **Peeking** | Inflates false positives | Naturally handled |\n",
    "| **Sample size** | Must be fixed upfront | More flexible |\n",
    "| **Interpretation** | \"95% of such intervals contain the true value\" | \"95% probability the true value is in this interval\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Beta-Binomial Model\n",
    "\n",
    "For binary outcomes (signed up / didn't sign up), the conjugate prior is the **Beta distribution**:\n",
    "\n",
    "- **Prior:** $p \\sim \\text{Beta}(\\alpha, \\beta)$\n",
    "- **Likelihood:** $x \\sim \\text{Binomial}(n, p)$\n",
    "- **Posterior:** $p \\mid x \\sim \\text{Beta}(\\alpha + x, \\beta + n - x)$\n",
    "\n",
    "Where $x$ = number of successes, $n$ = number of trials.\n",
    "\n",
    "The beauty of conjugacy is that the posterior has a **closed-form solution** - no MCMC needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate experiment data\n",
    "n_control = 3000\n",
    "n_treatment = 3000\n",
    "\n",
    "true_p_control = 0.08\n",
    "true_p_treatment = 0.095  # ~19% relative lift\n",
    "\n",
    "# Simulate results\n",
    "conversions_control = np.random.binomial(n_control, true_p_control)\n",
    "conversions_treatment = np.random.binomial(n_treatment, true_p_treatment)\n",
    "\n",
    "print(\"=== Observed Data ===\")\n",
    "print(f\"Control:   {conversions_control} / {n_control} = {conversions_control/n_control:.4f}\")\n",
    "print(f\"Treatment: {conversions_treatment} / {n_treatment} = {conversions_treatment/n_treatment:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting the Prior\n",
    "\n",
    "The choice of prior reflects our **prior knowledge** about the conversion rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninformative prior: Beta(1, 1) = Uniform(0, 1)\n",
    "# This says \"we have no prior knowledge about the conversion rate\"\n",
    "prior_alpha = 1\n",
    "prior_beta = 1\n",
    "\n",
    "# Alternative: weakly informative prior centered around 8%\n",
    "# Beta(8, 92) -> mean = 8/(8+92) = 0.08, roughly equivalent to 100 prior observations\n",
    "informative_alpha = 8\n",
    "informative_beta = 92\n",
    "\n",
    "x = np.linspace(0, 0.25, 1000)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.plot(x, stats.beta.pdf(x, 1, 1), 'b--', label='Uninformative: Beta(1,1)', linewidth=2)\n",
    "ax.plot(x, stats.beta.pdf(x, 8, 92), 'r-', label='Weakly informative: Beta(8,92)', linewidth=2)\n",
    "ax.set_xlabel('Conversion Rate')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Prior Distributions')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Uninformative prior mean: {1/(1+1):.2f} (flat, no preference)\")\n",
    "print(f\"Informative prior mean: {8/(8+92):.2f} (centered on historical rate)\")\n",
    "print(f\"\\nWe'll use the uninformative prior Beta(1,1) to let the data speak.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Posterior Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior parameters (with uninformative prior)\n",
    "post_alpha_A = prior_alpha + conversions_control\n",
    "post_beta_A = prior_beta + n_control - conversions_control\n",
    "\n",
    "post_alpha_B = prior_alpha + conversions_treatment\n",
    "post_beta_B = prior_beta + n_treatment - conversions_treatment\n",
    "\n",
    "print(\"=== Posterior Distributions ===\")\n",
    "print(f\"Control:   Beta({post_alpha_A}, {post_beta_A})\")\n",
    "print(f\"  Mean: {post_alpha_A / (post_alpha_A + post_beta_A):.4f}\")\n",
    "print(f\"  95% Credible Interval: {stats.beta.ppf([0.025, 0.975], post_alpha_A, post_beta_A)}\")\n",
    "print(f\"\\nTreatment: Beta({post_alpha_B}, {post_beta_B})\")\n",
    "print(f\"  Mean: {post_alpha_B / (post_alpha_B + post_beta_B):.4f}\")\n",
    "print(f\"  95% Credible Interval: {stats.beta.ppf([0.025, 0.975], post_alpha_B, post_beta_B)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot posterior distributions\n",
    "x = np.linspace(0.04, 0.14, 1000)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(x, stats.beta.pdf(x, post_alpha_A, post_beta_A),\n",
    "        'b-', linewidth=2, label=f'Control posterior (mean={post_alpha_A/(post_alpha_A+post_beta_A):.4f})')\n",
    "ax.plot(x, stats.beta.pdf(x, post_alpha_B, post_beta_B),\n",
    "        'r-', linewidth=2, label=f'Treatment posterior (mean={post_alpha_B/(post_alpha_B+post_beta_B):.4f})')\n",
    "ax.fill_between(x, stats.beta.pdf(x, post_alpha_A, post_beta_A), alpha=0.2, color='blue')\n",
    "ax.fill_between(x, stats.beta.pdf(x, post_alpha_B, post_beta_B), alpha=0.2, color='red')\n",
    "ax.set_xlabel('Conversion Rate')\n",
    "ax.set_ylabel('Posterior Density')\n",
    "ax.set_title('Posterior Distributions of Conversion Rate')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Bayesian Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo simulation to compute P(B > A)\n",
    "n_simulations = 100_000\n",
    "\n",
    "samples_A = stats.beta.rvs(post_alpha_A, post_beta_A, size=n_simulations)\n",
    "samples_B = stats.beta.rvs(post_alpha_B, post_beta_B, size=n_simulations)\n",
    "\n",
    "# Probability that B is better than A\n",
    "prob_B_better = (samples_B > samples_A).mean()\n",
    "\n",
    "# Distribution of the uplift\n",
    "uplift = samples_B - samples_A\n",
    "relative_uplift = (samples_B - samples_A) / samples_A * 100\n",
    "\n",
    "# Expected loss (risk of choosing wrong variant)\n",
    "loss_choose_B = np.maximum(samples_A - samples_B, 0).mean()  # Loss if B is actually worse\n",
    "loss_choose_A = np.maximum(samples_B - samples_A, 0).mean()  # Loss if A is actually worse\n",
    "\n",
    "print(\"=== Bayesian Decision Metrics ===\")\n",
    "print(f\"\\nP(Treatment > Control): {prob_B_better:.4f} ({prob_B_better*100:.1f}%)\")\n",
    "print(f\"P(Control > Treatment): {1-prob_B_better:.4f} ({(1-prob_B_better)*100:.1f}%)\")\n",
    "print(f\"\\nExpected uplift (absolute): {uplift.mean():.4f} ({uplift.mean()*100:.2f}pp)\")\n",
    "print(f\"95% Credible Interval for uplift: ({np.percentile(uplift, 2.5):.4f}, {np.percentile(uplift, 97.5):.4f})\")\n",
    "print(f\"\\nExpected relative uplift: {relative_uplift.mean():.1f}%\")\n",
    "print(f\"\\nExpected loss if we choose Treatment: {loss_choose_B:.5f}\")\n",
    "print(f\"Expected loss if we choose Control:   {loss_choose_A:.5f}\")\n",
    "print(f\"\\n--> Choosing Treatment minimizes expected loss.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the uplift distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Absolute uplift\n",
    "axes[0].hist(uplift * 100, bins=80, color='#2ecc71', alpha=0.7, edgecolor='white')\n",
    "axes[0].axvline(x=0, color='red', linestyle='--', linewidth=2, label='No effect')\n",
    "axes[0].axvline(x=np.percentile(uplift * 100, 2.5), color='orange', linestyle=':', label='95% CI')\n",
    "axes[0].axvline(x=np.percentile(uplift * 100, 97.5), color='orange', linestyle=':')\n",
    "axes[0].set_xlabel('Uplift (percentage points)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title(f'Distribution of Uplift\\nP(B > A) = {prob_B_better:.1%}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Relative uplift\n",
    "axes[1].hist(relative_uplift, bins=80, color='#9b59b6', alpha=0.7, edgecolor='white')\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2, label='No effect')\n",
    "axes[1].set_xlabel('Relative Uplift (%)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title(f'Distribution of Relative Uplift\\nMean = {relative_uplift.mean():.1f}%')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sequential Monitoring (Bayesian Advantage)\n",
    "\n",
    "One key advantage of Bayesian testing: we can **monitor the posterior as data accumulates** without inflating false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate sequential data collection and track P(B > A) over time\n",
    "np.random.seed(42)\n",
    "n_total = 3000\n",
    "step_size = 100\n",
    "\n",
    "control_stream = np.random.binomial(1, true_p_control, n_total)\n",
    "treatment_stream = np.random.binomial(1, true_p_treatment, n_total)\n",
    "\n",
    "checkpoints = range(step_size, n_total + 1, step_size)\n",
    "prob_b_better_over_time = []\n",
    "\n",
    "for n in checkpoints:\n",
    "    # Cumulative conversions\n",
    "    conv_a = control_stream[:n].sum()\n",
    "    conv_b = treatment_stream[:n].sum()\n",
    "    \n",
    "    # Posteriors\n",
    "    samples_a = stats.beta.rvs(1 + conv_a, 1 + n - conv_a, size=10_000)\n",
    "    samples_b = stats.beta.rvs(1 + conv_b, 1 + n - conv_b, size=10_000)\n",
    "    \n",
    "    prob_b_better_over_time.append((samples_b > samples_a).mean())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(list(checkpoints), prob_b_better_over_time, 'b-', linewidth=2)\n",
    "ax.axhline(y=0.95, color='green', linestyle='--', alpha=0.7, label='95% threshold')\n",
    "ax.axhline(y=0.50, color='gray', linestyle=':', alpha=0.5, label='50% (no preference)')\n",
    "ax.fill_between(list(checkpoints), 0.95, 1.0, alpha=0.1, color='green')\n",
    "ax.set_xlabel('Sample Size per Group')\n",
    "ax.set_ylabel('P(Treatment > Control)')\n",
    "ax.set_title('Sequential Monitoring: Probability B is Better Over Time')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find first time we cross 95%\n",
    "crossed = [(n, p) for n, p in zip(checkpoints, prob_b_better_over_time) if p >= 0.95]\n",
    "if crossed:\n",
    "    print(f\"First crossed 95% threshold at n={crossed[0][0]} per group (P(B>A)={crossed[0][1]:.3f})\")\n",
    "    print(f\"Could have stopped {n_total - crossed[0][0]} observations early per group!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison with Frequentist Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "# Frequentist Z-test\n",
    "z_stat, z_pval = proportions_ztest(\n",
    "    [conversions_control, conversions_treatment],\n",
    "    [n_control, n_treatment],\n",
    "    alternative='two-sided'\n",
    ")\n",
    "\n",
    "print(\"=== Frequentist vs Bayesian ===\")\n",
    "print(f\"\\nFrequentist (Z-test):\")\n",
    "print(f\"  Z-statistic: {z_stat:.4f}\")\n",
    "print(f\"  p-value: {z_pval:.4f}\")\n",
    "print(f\"  Decision: {'Reject H0' if z_pval < 0.05 else 'Fail to reject H0'}\")\n",
    "print(f\"\\nBayesian:\")\n",
    "print(f\"  P(Treatment > Control): {prob_B_better:.4f}\")\n",
    "print(f\"  Expected loss (choosing Treatment): {loss_choose_B:.5f}\")\n",
    "print(f\"  Decision: {'Choose Treatment' if prob_B_better > 0.95 else 'Need more evidence'}\")\n",
    "print(f\"\\nBoth approaches agree in this case, but Bayesian gives directly\")\n",
    "print(f\"interpretable probabilities and expected loss for decision-making.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interview Follow-Up Questions & Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: What are the advantages of Bayesian A/B testing over frequentist?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "1. **Interpretability**: You get direct probability statements (\"82% chance B is better\") instead of p-values which are often misinterpreted.\n",
    "\n",
    "2. **Peeking is natural**: You can monitor results continuously without inflating false positive rates (though you should still be careful about stopping rules).\n",
    "\n",
    "3. **Incorporate prior knowledge**: If you have historical data about conversion rates, you can incorporate it through priors.\n",
    "\n",
    "4. **Decision-theoretic framework**: Expected loss lets you make cost-aware decisions. You can ask \"What's the maximum cost of choosing B if it's actually worse?\"\n",
    "\n",
    "5. **No fixed sample size required**: While you still need sufficient data, Bayesian methods are more flexible about when to stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: What are the disadvantages of Bayesian A/B testing?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "1. **Prior sensitivity**: Poor choice of priors can bias results, especially with small samples. Teams may disagree on what prior to use.\n",
    "\n",
    "2. **Computational cost**: For complex models (non-conjugate priors), you need MCMC sampling which is slower.\n",
    "\n",
    "3. **Less established in industry**: Most experimentation platforms default to frequentist methods. Bayesian requires more education for stakeholders.\n",
    "\n",
    "4. **False sense of flexibility**: While peeking is safer, stopping very early can still lead to unreliable decisions. The posterior with few data points has high uncertainty.\n",
    "\n",
    "5. **Harder to pre-register**: Frequentist tests have clearer pre-registration frameworks (alpha, power, sample size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: How do you choose the prior? What if you get it wrong?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Choosing the prior:**\n",
    "- **Uninformative / flat prior** (Beta(1,1)): When you have no prior knowledge. Lets data speak entirely.\n",
    "- **Weakly informative**: Based on historical data. E.g., if historical conversion rate is 8%, use Beta(8, 92) - equivalent to 100 prior observations.\n",
    "- **Skeptical prior**: Centers on zero effect to be conservative about detecting improvements.\n",
    "\n",
    "**What if you get it wrong:**\n",
    "- With sufficient data, the posterior is **dominated by the likelihood** (data overwhelms the prior). This is called \"prior washing out.\"\n",
    "- Run **sensitivity analysis**: Check if your conclusions change with different reasonable priors.\n",
    "- Rule of thumb: If your prior's \"effective sample size\" is small relative to actual data, the prior has minimal impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: What is \"expected loss\" and how do you use it for decisions?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Expected loss quantifies the **risk of making the wrong decision**:\n",
    "\n",
    "$$\\text{Expected Loss(choose B)} = E[\\max(p_A - p_B, 0)]$$\n",
    "\n",
    "This is the average amount of conversion rate you'd lose if B is actually worse than A.\n",
    "\n",
    "**Decision rule:** Choose the variant with the **lower expected loss**.\n",
    "\n",
    "**Threshold approach:** Set a \"loss threshold\" (e.g., 0.1%). If expected loss of choosing B < threshold, launch B. This naturally accounts for the magnitude of potential loss, not just significance.\n",
    "\n",
    "**Business advantage:** You can convert expected loss to dollars: if expected loss is 0.05pp and you have 1M monthly visitors with \\$50 AOV, the risk is 1M x 0.0005 x \\$50 = \\$25K/month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Can you peek at results in Bayesian testing without consequences?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Partially true, with caveats:**\n",
    "\n",
    "Unlike frequentist testing where repeated peeking inflates the Type I error rate, Bayesian posteriors are **always valid given the observed data**. Each time you compute P(B > A), it's a correct posterior probability.\n",
    "\n",
    "**However:**\n",
    "- Stopping very early means the **posterior has high uncertainty** (wide credible intervals)\n",
    "- If you stop the moment P(B > A) first exceeds 95%, you'll make more errors than if you wait for the posterior to stabilize\n",
    "- The **expected loss** metric helps here: even if P(B > A) is high, if expected loss is also high, you need more data\n",
    "\n",
    "**Best practice:** Monitor continuously but set a **minimum sample size** and use expected loss thresholds rather than just P(B > A)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6: A stakeholder asks: \"Is 90% probability good enough to launch?\" How do you respond?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "It depends on the **cost of being wrong**:\n",
    "\n",
    "1. **Low-cost, easily reversible change** (e.g., button color): 90% might be fine. The cost of being wrong is minimal and you can always revert.\n",
    "\n",
    "2. **High-cost, hard-to-reverse change** (e.g., pricing model): You'd want 95%+ and low expected loss. Getting pricing wrong can cause churn.\n",
    "\n",
    "3. **Look at expected loss, not just probability**: If P(B > A) = 90% but expected loss is only \\$100/month if wrong, it's probably fine. If expected loss is \\$1M/month, you need stronger evidence.\n",
    "\n",
    "4. **Consider opportunity cost**: How much do you lose by waiting for more data? If the potential gain is large and the risk is small, 90% might be sufficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
