{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 2: SaaS Session Duration A/B Test (Continuous Metrics)\n",
    "\n",
    "## Scenario\n",
    "A SaaS company redesigned its onboarding flow to improve user engagement. The product team wants to test whether the new onboarding increases **average session duration** (a continuous metric). Current average session duration is **5.2 minutes** with a standard deviation of ~3.1 minutes.\n",
    "\n",
    "**Tests used:** Two-sample t-test, Welch's t-test, Mann-Whitney U test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Understanding & Hypothesis\n",
    "\n",
    "**Business context:** Longer session duration correlates with feature adoption and retention. The new onboarding introduces interactive tutorials instead of static walkthroughs.\n",
    "\n",
    "**Hypotheses:**\n",
    "- $H_0$: The new onboarding has no effect on session duration ($\\mu_{treatment} = \\mu_{control}$)\n",
    "- $H_a$: The new onboarding changes session duration ($\\mu_{treatment} \\neq \\mu_{control}$)\n",
    "\n",
    "**Experiment parameters:**\n",
    "- $\\alpha = 0.05$\n",
    "- Power = 0.80\n",
    "- MDE: 0.5 minutes increase in average session duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment Design & Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "baseline_mean = 5.2      # minutes\n",
    "baseline_std = 3.1       # minutes\n",
    "mde = 0.5                # 0.5 minute increase\n",
    "alpha = 0.05\n",
    "power = 0.80\n",
    "\n",
    "# Cohen's d effect size\n",
    "cohens_d = mde / baseline_std\n",
    "print(f\"Cohen's d effect size: {cohens_d:.4f}\")\n",
    "print(f\"  (small: 0.2, medium: 0.5, large: 0.8)\")\n",
    "\n",
    "# Sample size calculation\n",
    "analysis = TTestIndPower()\n",
    "required_n = analysis.solve_power(\n",
    "    effect_size=cohens_d,\n",
    "    alpha=alpha,\n",
    "    power=power,\n",
    "    alternative='two-sided'\n",
    ")\n",
    "required_n = int(np.ceil(required_n))\n",
    "print(f\"\\nRequired sample size per group: {required_n:,}\")\n",
    "print(f\"Total sample size needed: {required_n * 2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Simulated Data\n",
    "\n",
    "Session duration data is typically **right-skewed** (some users have very long sessions). We simulate this with a gamma distribution, which is more realistic than normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_per_group = 600\n",
    "\n",
    "# Control: gamma distribution with mean=5.2, std~3.1\n",
    "# shape=k, scale=theta -> mean=k*theta, var=k*theta^2\n",
    "k_control = (baseline_mean / baseline_std) ** 2\n",
    "theta_control = baseline_std ** 2 / baseline_mean\n",
    "\n",
    "# Treatment: slight improvement -> mean=5.7, similar std\n",
    "treatment_mean = 5.7\n",
    "treatment_std = 3.2\n",
    "k_treatment = (treatment_mean / treatment_std) ** 2\n",
    "theta_treatment = treatment_std ** 2 / treatment_mean\n",
    "\n",
    "control = np.random.gamma(k_control, theta_control, n_per_group)\n",
    "treatment = np.random.gamma(k_treatment, theta_treatment, n_per_group)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'group': ['control'] * n_per_group + ['treatment'] * n_per_group,\n",
    "    'session_duration': np.concatenate([control, treatment])\n",
    "})\n",
    "\n",
    "print(\"=== Descriptive Statistics ===\")\n",
    "summary = df.groupby('group')['session_duration'].agg(['count', 'mean', 'std', 'median'])\n",
    "summary.columns = ['N', 'Mean', 'Std', 'Median']\n",
    "print(summary.round(3))\n",
    "print(f\"\\nDifference in means: {treatment.mean() - control.mean():.3f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Distribution plots\n",
    "axes[0].hist(control, bins=40, alpha=0.6, color='#3498db', label='Control', density=True)\n",
    "axes[0].hist(treatment, bins=40, alpha=0.6, color='#e74c3c', label='Treatment', density=True)\n",
    "axes[0].set_xlabel('Session Duration (min)')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('Distribution of Session Duration')\n",
    "axes[0].legend()\n",
    "\n",
    "# QQ plots\n",
    "stats.probplot(control, dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title('Q-Q Plot: Control')\n",
    "\n",
    "stats.probplot(treatment, dist=\"norm\", plot=axes[2])\n",
    "axes[2].set_title('Q-Q Plot: Treatment')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for normality (Shapiro-Wilk)\n",
    "shapiro_control = stats.shapiro(control)\n",
    "shapiro_treatment = stats.shapiro(treatment)\n",
    "print(\"=== Normality Tests (Shapiro-Wilk) ===\")\n",
    "print(f\"Control:   W={shapiro_control.statistic:.4f}, p={shapiro_control.pvalue:.4f}\")\n",
    "print(f\"Treatment: W={shapiro_treatment.statistic:.4f}, p={shapiro_treatment.pvalue:.4f}\")\n",
    "print(f\"\\nNote: With large samples, Shapiro-Wilk often rejects normality.\")\n",
    "print(f\"By CLT, the sampling distribution of the mean is approximately normal for n={n_per_group}.\")\n",
    "\n",
    "# Test for equal variances (Levene's test)\n",
    "levene_stat, levene_pval = stats.levene(control, treatment)\n",
    "print(f\"\\n=== Equal Variance Test (Levene's) ===\")\n",
    "print(f\"Statistic: {levene_stat:.4f}, p-value: {levene_pval:.4f}\")\n",
    "if levene_pval < 0.05:\n",
    "    print(\"Variances are significantly different -> Use Welch's t-test\")\n",
    "else:\n",
    "    print(\"No evidence of unequal variances -> Standard t-test is OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Two-Sample t-test (assumes equal variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_stat, t_pval = stats.ttest_ind(treatment, control, equal_var=True)\n",
    "\n",
    "print(\"=== Two-Sample t-test (equal variance) ===\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {t_pval:.4f}\")\n",
    "print(f\"Degrees of freedom: {n_per_group * 2 - 2}\")\n",
    "if t_pval < alpha:\n",
    "    print(f\"REJECT H0 (p={t_pval:.4f} < {alpha})\")\n",
    "else:\n",
    "    print(f\"FAIL TO REJECT H0 (p={t_pval:.4f} >= {alpha})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Welch's t-test (does NOT assume equal variance)\n",
    "\n",
    "This is the **recommended default** in practice because it is robust to unequal variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "welch_stat, welch_pval = stats.ttest_ind(treatment, control, equal_var=False)\n",
    "\n",
    "# Calculate Welch-Satterthwaite degrees of freedom\n",
    "s1, s2 = control.std(ddof=1), treatment.std(ddof=1)\n",
    "n1, n2 = len(control), len(treatment)\n",
    "welch_df = ((s1**2/n1 + s2**2/n2)**2) / ((s1**2/n1)**2/(n1-1) + (s2**2/n2)**2/(n2-1))\n",
    "\n",
    "print(\"=== Welch's t-test (unequal variance) ===\")\n",
    "print(f\"t-statistic: {welch_stat:.4f}\")\n",
    "print(f\"p-value: {welch_pval:.4f}\")\n",
    "print(f\"Welch-Satterthwaite df: {welch_df:.1f}\")\n",
    "if welch_pval < alpha:\n",
    "    print(f\"REJECT H0 (p={welch_pval:.4f} < {alpha})\")\n",
    "else:\n",
    "    print(f\"FAIL TO REJECT H0 (p={welch_pval:.4f} >= {alpha})\")\n",
    "\n",
    "# Confidence interval for the difference in means\n",
    "mean_diff = treatment.mean() - control.mean()\n",
    "se_diff = np.sqrt(s1**2/n1 + s2**2/n2)\n",
    "t_crit = stats.t.ppf(1 - alpha/2, welch_df)\n",
    "ci = (mean_diff - t_crit * se_diff, mean_diff + t_crit * se_diff)\n",
    "\n",
    "print(f\"\\nMean difference: {mean_diff:.3f} minutes\")\n",
    "print(f\"95% CI: ({ci[0]:.3f}, {ci[1]:.3f}) minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Mann-Whitney U Test (non-parametric alternative)\n",
    "\n",
    "Use when data is **non-normal or highly skewed**. It compares the **ranks** of observations rather than the raw values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_stat, u_pval = stats.mannwhitneyu(treatment, control, alternative='two-sided')\n",
    "\n",
    "# Calculate rank-biserial correlation (effect size)\n",
    "rank_biserial = 1 - (2 * u_stat) / (n1 * n2)\n",
    "\n",
    "print(\"=== Mann-Whitney U Test ===\")\n",
    "print(f\"U-statistic: {u_stat:.1f}\")\n",
    "print(f\"p-value: {u_pval:.4f}\")\n",
    "print(f\"Rank-biserial correlation (effect size): {rank_biserial:.4f}\")\n",
    "if u_pval < alpha:\n",
    "    print(f\"REJECT H0 (p={u_pval:.4f} < {alpha})\")\n",
    "else:\n",
    "    print(f\"FAIL TO REJECT H0 (p={u_pval:.4f} >= {alpha})\")\n",
    "print(f\"\\nNote: Mann-Whitney tests if one distribution is stochastically greater than the other.\")\n",
    "print(f\"It does NOT directly test means - it tests the probability that a random\")\n",
    "print(f\"observation from treatment is greater than a random observation from control.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Comparison of All Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Test': ['Two-sample t-test', \"Welch's t-test\", 'Mann-Whitney U'],\n",
    "    'Statistic': [t_stat, welch_stat, u_stat],\n",
    "    'p-value': [t_pval, welch_pval, u_pval],\n",
    "    'Significant': [t_pval < alpha, welch_pval < alpha, u_pval < alpha],\n",
    "    'Assumes Normality': ['Yes', 'Yes', 'No'],\n",
    "    'Assumes Equal Var': ['Yes', 'No', 'No']\n",
    "})\n",
    "print(\"=== Test Comparison ===\")\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot comparison\n",
    "bp = axes[0].boxplot([control, treatment], labels=['Control', 'Treatment'],\n",
    "                     patch_artist=True, showmeans=True,\n",
    "                     meanprops={'marker': 'D', 'markerfacecolor': 'red', 'markersize': 8})\n",
    "bp['boxes'][0].set_facecolor('#3498db')\n",
    "bp['boxes'][1].set_facecolor('#e74c3c')\n",
    "for box in bp['boxes']:\n",
    "    box.set_alpha(0.6)\n",
    "axes[0].set_ylabel('Session Duration (minutes)')\n",
    "axes[0].set_title('Session Duration by Group (diamond = mean)')\n",
    "\n",
    "# CI for difference in means\n",
    "axes[1].errorbar(0, mean_diff, yerr=[[mean_diff - ci[0]], [ci[1] - mean_diff]],\n",
    "                 fmt='o', color='#2ecc71', markersize=10, capsize=10, linewidth=2)\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', alpha=0.7, label='No effect')\n",
    "axes[1].axhline(y=mde, color='blue', linestyle=':', alpha=0.5, label=f'MDE = {mde} min')\n",
    "axes[1].set_xlim(-0.5, 0.5)\n",
    "axes[1].set_ylabel('Difference in Mean Session Duration (min)')\n",
    "axes[1].set_title(\"95% CI for Difference (Welch's t-test)\")\n",
    "axes[1].set_xticks([])\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interview Follow-Up Questions & Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Why is Welch's t-test preferred over the standard t-test in practice?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Welch's t-test is preferred because:\n",
    "\n",
    "1. **It does not assume equal variances** between groups, making it more robust.\n",
    "2. When variances ARE equal, Welch's t-test gives results nearly identical to the standard t-test (minimal power loss).\n",
    "3. When variances are NOT equal, the standard t-test can have inflated Type I error rates.\n",
    "\n",
    "The asymmetry in risk makes Welch's the safer default: you lose very little when variances are equal, but you avoid significant errors when they're not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: The data is right-skewed (not normal). Can you still use the t-test?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Yes, thanks to the **Central Limit Theorem (CLT)**. The t-test compares **sample means**, and the CLT guarantees that the distribution of sample means approaches normality as sample size increases, **regardless of the underlying distribution**.\n",
    "\n",
    "Rules of thumb:\n",
    "- **n > 30**: Usually sufficient for mild skew\n",
    "- **n > 100**: Adequate for moderate skew\n",
    "- **Very heavy tails / extreme outliers**: Consider Mann-Whitney U or bootstrapping even with large n\n",
    "\n",
    "In this case study with n=600 per group, the CLT provides strong protection, so the t-test is valid despite the skewed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: When would you choose Mann-Whitney U over the t-test?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Use Mann-Whitney U when:\n",
    "\n",
    "1. **Ordinal data** (e.g., satisfaction ratings 1-5) where means are not meaningful\n",
    "2. **Small samples with non-normal data** where CLT doesn't apply\n",
    "3. **Heavy outliers** that would distort the mean\n",
    "4. You care about the **overall distribution shift** rather than just the mean\n",
    "\n",
    "**Important caveat:** Mann-Whitney tests whether one group tends to have larger values than the other. It does NOT test means directly. If you specifically need to compare means, the t-test (with sufficient sample size) is more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: How would you handle heavy outliers in session duration data?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Several strategies:\n",
    "\n",
    "1. **Winsorization**: Cap extreme values at a percentile (e.g., 95th or 99th). This preserves the data point but limits its influence.\n",
    "\n",
    "2. **Trimmed mean**: Remove the top and bottom X% of observations before computing the mean.\n",
    "\n",
    "3. **Log transformation**: Apply $\\log(x + 1)$ to compress the right tail, making the distribution more symmetric.\n",
    "\n",
    "4. **Use robust test**: Mann-Whitney U or bootstrap-based tests are naturally less affected by outliers.\n",
    "\n",
    "5. **Investigate the outliers**: Are they real users or bots? Users who left the tab open? Understanding the cause informs the treatment.\n",
    "\n",
    "The key principle: **decide on outlier handling BEFORE looking at results** to avoid biasing the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Your test shows the new onboarding increases session duration by 0.5 min. Is this a good metric?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Session duration as a primary metric has limitations:\n",
    "\n",
    "**Pros:**\n",
    "- Easy to measure and understand\n",
    "- Correlates with engagement\n",
    "\n",
    "**Cons:**\n",
    "- **Not always directional**: Longer sessions could mean confusion, not engagement\n",
    "- **Can be gamed**: A confusing UI also increases time on page\n",
    "- **Doesn't capture value**: 5 minutes of frustrated searching â‰  5 minutes of productive use\n",
    "\n",
    "**Better approach:** Use session duration as a **guardrail metric** and pair it with **action-based metrics** like:\n",
    "- Feature adoption rate\n",
    "- Task completion rate\n",
    "- 7-day retention rate\n",
    "- Number of key actions completed per session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6: How do you calculate and interpret Cohen's d?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Cohen's d measures the **standardized effect size** - the difference in means expressed in units of standard deviation:\n",
    "\n",
    "$$d = \\frac{\\bar{X}_B - \\bar{X}_A}{s_{pooled}}$$\n",
    "\n",
    "Where $s_{pooled} = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}}$\n",
    "\n",
    "**Interpretation guidelines (Cohen's conventions):**\n",
    "\n",
    "| d | Interpretation |\n",
    "|---|---|\n",
    "| 0.2 | Small effect |\n",
    "| 0.5 | Medium effect |\n",
    "| 0.8 | Large effect |\n",
    "\n",
    "**Why it matters:** p-values depend on sample size, but effect size doesn't. A tiny effect can be \"significant\" with enormous samples. Cohen's d helps assess **practical importance** independently of sample size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
