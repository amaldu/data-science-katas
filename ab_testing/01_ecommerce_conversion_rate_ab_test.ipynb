{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 1: E-Commerce Conversion Rate A/B Test\n",
    "\n",
    "## Scenario\n",
    "An e-commerce company wants to test whether a **new checkout page design** increases the purchase conversion rate. The current conversion rate is approximately **13%**. The product team believes the new design could improve it by at least **2 percentage points**.\n",
    "\n",
    "**Tests used:** Z-test for proportions, Chi-Square test, Fisher's Exact Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Understanding & Hypothesis\n",
    "\n",
    "**Business context:** The checkout page is the final step before purchase. Even small improvements in conversion rate can translate to significant revenue.\n",
    "\n",
    "**Hypotheses:**\n",
    "- $H_0$: The new checkout page has no effect on conversion rate ($p_{treatment} = p_{control}$)\n",
    "- $H_a$: The new checkout page has a different conversion rate ($p_{treatment} \\neq p_{control}$)\n",
    "\n",
    "**PICOT:**\n",
    "- **P**opulation: Users who reach the checkout page\n",
    "- **I**ntervention: New checkout page design\n",
    "- **C**omparison: Old checkout page (control)\n",
    "- **O**utcome: Purchase conversion rate\n",
    "- **T**ime: 2 weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from statsmodels.stats.proportion import proportions_ztest, proportion_confint\n",
    "from statsmodels.stats.power import NormalIndPower\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "baseline_conversion = 0.13    # Current conversion rate\n",
    "mde = 0.02                    # Minimum detectable effect (2 percentage points)\n",
    "alpha = 0.05                  # Significance level\n",
    "power = 0.80                  # Statistical power\n",
    "\n",
    "# Calculate effect size (Cohen's h for proportions)\n",
    "from statsmodels.stats.proportion import proportion_effectsize\n",
    "effect_size = proportion_effectsize(baseline_conversion + mde, baseline_conversion)\n",
    "print(f\"Effect size (Cohen's h): {effect_size:.4f}\")\n",
    "\n",
    "# Calculate required sample size per group\n",
    "analysis = NormalIndPower()\n",
    "required_n = analysis.solve_power(\n",
    "    effect_size=effect_size,\n",
    "    alpha=alpha,\n",
    "    power=power,\n",
    "    alternative='two-sided'\n",
    ")\n",
    "required_n = int(np.ceil(required_n))\n",
    "print(f\"Required sample size per group: {required_n:,}\")\n",
    "print(f\"Total sample size needed: {required_n * 2:,}\")\n",
    "\n",
    "# Estimate test duration\n",
    "daily_visitors = 2000\n",
    "duration_days = np.ceil((required_n * 2) / daily_visitors)\n",
    "print(f\"\\nWith {daily_visitors:,} daily visitors:\")\n",
    "print(f\"Estimated duration: {duration_days:.0f} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Simulated Data\n",
    "\n",
    "In practice, this data comes from the experiment platform. Here we simulate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_control = 4700\n",
    "n_treatment = 4700\n",
    "\n",
    "# Simulate: treatment has a real improvement of ~2%\n",
    "control_conversions = np.random.binomial(1, 0.13, n_control)\n",
    "treatment_conversions = np.random.binomial(1, 0.15, n_treatment)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'group': ['control'] * n_control + ['treatment'] * n_treatment,\n",
    "    'converted': np.concatenate([control_conversions, treatment_conversions])\n",
    "})\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nGroup sizes:\")\n",
    "print(df['group'].value_counts())\n",
    "print(f\"\\nConversion rates:\")\n",
    "print(df.groupby('group')['converted'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Ratio Mismatch (SRM) check using Chi-square goodness-of-fit\n",
    "observed = [n_control, n_treatment]\n",
    "expected_ratio = [0.5, 0.5]\n",
    "total = n_control + n_treatment\n",
    "expected_counts = [total * r for r in expected_ratio]\n",
    "\n",
    "srm_chi2, srm_pval = stats.chisquare(observed, expected_counts)\n",
    "print(\"=== Sample Ratio Mismatch Check ===\")\n",
    "print(f\"Observed: Control={n_control}, Treatment={n_treatment}\")\n",
    "print(f\"Expected: 50/50 split = {total//2} each\")\n",
    "print(f\"Chi-square statistic: {srm_chi2:.4f}\")\n",
    "print(f\"p-value: {srm_pval:.4f}\")\n",
    "print(f\"Result: {'PASS - No SRM detected' if srm_pval > 0.01 else 'FAIL - SRM detected!'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Z-Test for Proportions\n",
    "\n",
    "The most common test for comparing conversion rates in A/B testing. Appropriate when:\n",
    "- Metric is binary (converted / not converted)\n",
    "- Sample size is large (n > 30 per group)\n",
    "- Observations are independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute observed values\n",
    "control_data = df[df['group'] == 'control']['converted']\n",
    "treatment_data = df[df['group'] == 'treatment']['converted']\n",
    "\n",
    "successes = np.array([control_data.sum(), treatment_data.sum()])\n",
    "nobs = np.array([len(control_data), len(treatment_data)])\n",
    "\n",
    "p_control = successes[0] / nobs[0]\n",
    "p_treatment = successes[1] / nobs[1]\n",
    "p_diff = p_treatment - p_control\n",
    "relative_lift = (p_treatment - p_control) / p_control * 100\n",
    "\n",
    "print(\"=== Descriptive Statistics ===\")\n",
    "print(f\"Control:   {successes[0]:,} / {nobs[0]:,} = {p_control:.4f} ({p_control*100:.2f}%)\")\n",
    "print(f\"Treatment: {successes[1]:,} / {nobs[1]:,} = {p_treatment:.4f} ({p_treatment*100:.2f}%)\")\n",
    "print(f\"Absolute difference: {p_diff:.4f} ({p_diff*100:.2f} pp)\")\n",
    "print(f\"Relative lift: {relative_lift:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-test for proportions (two-sided)\n",
    "z_stat, z_pval = proportions_ztest(successes, nobs, alternative='two-sided')\n",
    "\n",
    "print(\"=== Z-Test for Proportions ===\")\n",
    "print(f\"Z-statistic: {z_stat:.4f}\")\n",
    "print(f\"p-value: {z_pval:.4f}\")\n",
    "print(f\"\\nAt alpha = {alpha}:\")\n",
    "if z_pval < alpha:\n",
    "    print(f\"REJECT H0 - The difference IS statistically significant (p={z_pval:.4f} < {alpha})\")\n",
    "else:\n",
    "    print(f\"FAIL TO REJECT H0 - The difference is NOT statistically significant (p={z_pval:.4f} >= {alpha})\")\n",
    "\n",
    "# Confidence interval for the difference\n",
    "ci_control = proportion_confint(successes[0], nobs[0], alpha=alpha, method='normal')\n",
    "ci_treatment = proportion_confint(successes[1], nobs[1], alpha=alpha, method='normal')\n",
    "\n",
    "# CI for the difference in proportions\n",
    "se_diff = np.sqrt(p_control*(1-p_control)/nobs[0] + p_treatment*(1-p_treatment)/nobs[1])\n",
    "z_crit = stats.norm.ppf(1 - alpha/2)\n",
    "ci_diff = (p_diff - z_crit * se_diff, p_diff + z_crit * se_diff)\n",
    "\n",
    "print(f\"\\n95% CI for difference: ({ci_diff[0]:.4f}, {ci_diff[1]:.4f})\")\n",
    "print(f\"95% CI for difference: ({ci_diff[0]*100:.2f}pp, {ci_diff[1]*100:.2f}pp)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Chi-Square Test\n",
    "\n",
    "An alternative for categorical data. Tests whether the distribution of outcomes differs between groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build contingency table\n",
    "contingency_table = pd.crosstab(df['group'], df['converted'], margins=True)\n",
    "contingency_table.columns = ['Not Converted', 'Converted', 'Total']\n",
    "contingency_table.index = ['Control', 'Treatment', 'Total']\n",
    "print(\"=== Contingency Table ===\")\n",
    "print(contingency_table)\n",
    "\n",
    "# Chi-square test\n",
    "table = [[successes[0], nobs[0] - successes[0]],\n",
    "         [successes[1], nobs[1] - successes[1]]]\n",
    "\n",
    "chi2, chi2_pval, dof, expected = stats.chi2_contingency(table)\n",
    "\n",
    "print(f\"\\n=== Chi-Square Test ===\")\n",
    "print(f\"Chi-square statistic: {chi2:.4f}\")\n",
    "print(f\"Degrees of freedom: {dof}\")\n",
    "print(f\"p-value: {chi2_pval:.4f}\")\n",
    "print(f\"\\nExpected frequencies:\")\n",
    "print(pd.DataFrame(expected, columns=['Not Converted', 'Converted'], \n",
    "                    index=['Control', 'Treatment']).round(1))\n",
    "\n",
    "print(f\"\\nAll expected frequencies >= 5: {(np.array(expected) >= 5).all()}\")\n",
    "if chi2_pval < alpha:\n",
    "    print(f\"\\nREJECT H0 - Statistically significant (p={chi2_pval:.4f} < {alpha})\")\n",
    "else:\n",
    "    print(f\"\\nFAIL TO REJECT H0 - Not significant (p={chi2_pval:.4f} >= {alpha})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Fisher's Exact Test\n",
    "\n",
    "For demonstration purposes. In practice, use this when expected cell frequencies are < 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_ratio, fisher_pval = stats.fisher_exact(table)\n",
    "\n",
    "print(\"=== Fisher's Exact Test ===\")\n",
    "print(f\"Odds ratio: {odds_ratio:.4f}\")\n",
    "print(f\"p-value: {fisher_pval:.4f}\")\n",
    "print(f\"\\nNote: With large samples, Fisher's and Chi-square give very similar results.\")\n",
    "print(f\"Fisher's is preferred when any expected cell count < 5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Comparison of All Three Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Test': ['Z-test (proportions)', 'Chi-Square', \"Fisher's Exact\"],\n",
    "    'Statistic': [z_stat, chi2, odds_ratio],\n",
    "    'p-value': [z_pval, chi2_pval, fisher_pval],\n",
    "    'Significant (alpha=0.05)': [z_pval < alpha, chi2_pval < alpha, fisher_pval < alpha]\n",
    "})\n",
    "print(\"=== Test Comparison ===\")\n",
    "print(results.to_string(index=False))\n",
    "print(\"\\nNote: Z-test^2 â‰ˆ Chi-square statistic (they are equivalent for 2x2 tables)\")\n",
    "print(f\"Z^2 = {z_stat**2:.4f}, Chi2 = {chi2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Conversion rates with CI\n",
    "groups = ['Control', 'Treatment']\n",
    "rates = [p_control, p_treatment]\n",
    "errors = [\n",
    "    [p_control - ci_control[0], ci_control[1] - p_control],\n",
    "    [p_treatment - ci_treatment[0], ci_treatment[1] - p_treatment]\n",
    "]\n",
    "errors = np.array(errors).T\n",
    "\n",
    "bars = axes[0].bar(groups, rates, color=['#3498db', '#e74c3c'], width=0.5, alpha=0.8)\n",
    "axes[0].errorbar(groups, rates, yerr=errors, fmt='none', color='black', capsize=5)\n",
    "axes[0].set_ylabel('Conversion Rate')\n",
    "axes[0].set_title('Conversion Rate by Group (with 95% CI)')\n",
    "for bar, rate in zip(bars, rates):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.002,\n",
    "                f'{rate:.2%}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: CI for the difference\n",
    "axes[1].errorbar(0, p_diff, yerr=[[p_diff - ci_diff[0]], [ci_diff[1] - p_diff]],\n",
    "                 fmt='o', color='#2ecc71', markersize=10, capsize=10, linewidth=2)\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', alpha=0.7, label='No effect')\n",
    "axes[1].set_xlim(-0.5, 0.5)\n",
    "axes[1].set_ylabel('Difference in Conversion Rate')\n",
    "axes[1].set_title('95% CI for Difference (Treatment - Control)')\n",
    "axes[1].set_xticks([])\n",
    "axes[1].legend()\n",
    "\n",
    "# Plot 3: p-value comparison\n",
    "test_names = ['Z-test', 'Chi-Square', \"Fisher's\"]\n",
    "pvalues = [z_pval, chi2_pval, fisher_pval]\n",
    "colors = ['#27ae60' if p < alpha else '#e74c3c' for p in pvalues]\n",
    "axes[2].barh(test_names, pvalues, color=colors, alpha=0.8)\n",
    "axes[2].axvline(x=alpha, color='red', linestyle='--', label=f'alpha = {alpha}')\n",
    "axes[2].set_xlabel('p-value')\n",
    "axes[2].set_title('p-values Across Tests')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion & Recommendation\n",
    "\n",
    "**Statistical conclusion:** All three tests agree. We have sufficient evidence to reject the null hypothesis at the 5% significance level.\n",
    "\n",
    "**Practical significance:** The observed lift is approximately 2 percentage points, which aligns with our MDE. The confidence interval gives us the range of plausible true effects.\n",
    "\n",
    "**Recommendation:** Launch the new checkout page design. The improvement is both statistically and practically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interview Follow-Up Questions & Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Why did you choose a Z-test instead of a t-test for this problem?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The Z-test for proportions is the correct choice when comparing **binary outcomes** (converted vs. not converted) between two groups with **large sample sizes**. The t-test is designed for **continuous metrics** (like average revenue). Since our metric is a proportion (conversion rate), the Z-test is the natural fit.\n",
    "\n",
    "Additionally, with large samples (n > 30), the sampling distribution of the proportion is approximately normal by the Central Limit Theorem, making the Z-test appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: What is the relationship between the Z-test and the Chi-square test for a 2x2 table?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "For a 2x2 contingency table, the **Chi-square statistic equals the square of the Z-statistic**: $\\chi^2 = Z^2$. They will always give the same p-value. The difference is:\n",
    "\n",
    "- The **Z-test** is inherently directional (you can do one-tailed tests)\n",
    "- The **Chi-square test** is always two-tailed and extends naturally to larger tables (e.g., comparing 3+ groups)\n",
    "\n",
    "In practice for a standard two-group A/B test, they are interchangeable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: When would you use Fisher's Exact Test instead of Chi-square?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Use Fisher's Exact Test when any **expected cell frequency in the contingency table is less than 5**. This typically happens with:\n",
    "- Very small sample sizes\n",
    "- Very rare events (e.g., testing a feature with 0.1% conversion rate and only 200 users)\n",
    "\n",
    "Fisher's test computes the **exact probability** rather than relying on the chi-square approximation, so it's always valid regardless of sample size. The downside is it becomes computationally expensive for large tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Your test is significant but the lift is only 2%. How do you convince stakeholders to launch?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "I would frame it in **business terms**:\n",
    "\n",
    "1. **Revenue impact:** If we have 1M monthly visitors reaching checkout with an AOV (Average Order Value) of \\$50, a 2pp increase in conversion means: $1M \\times 0.02 \\times \\$50 = \\$1M$ additional monthly revenue.\n",
    "\n",
    "2. **Confidence interval:** The 95% CI tells us the true effect is likely between X and Y, giving stakeholders a range to plan around.\n",
    "\n",
    "3. **Risk assessment:** We ran validity checks (SRM, guardrail metrics) and everything looks clean. The result is robust.\n",
    "\n",
    "4. **Implementation cost:** If the new design is easy to maintain, the risk-reward ratio is very favorable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: You notice that the sample ratio is 52/48 instead of 50/50. What do you do?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "I would run a **Sample Ratio Mismatch (SRM) test** using a chi-square goodness-of-fit test:\n",
    "\n",
    "1. If the p-value is **above 0.01**, the mismatch is likely due to random variation - proceed with analysis.\n",
    "2. If the p-value is **below 0.01**, there's likely a **bug in the randomization system** (e.g., bot traffic, broken redirect, caching issue).\n",
    "\n",
    "If SRM is detected, I would **NOT trust the test results**. Instead, I'd investigate the root cause with engineering, fix the issue, and re-run the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6: What if the p-value is 0.06? What would you recommend?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "A p-value of 0.06 means we **fail to reject** $H_0$ at the 5% level, but the result is close. My approach:\n",
    "\n",
    "1. **Look at the confidence interval**: If the CI includes practically meaningful effects, the test may be **underpowered** - we might need more data.\n",
    "2. **Check practical significance**: If the point estimate of the effect is large and business-relevant, I might recommend **extending the test** to collect more data.\n",
    "3. **Never change alpha after seeing results**: That would be p-hacking. The threshold should be set before the experiment.\n",
    "4. **Context matters**: For a low-risk, easy-to-implement change, the business might accept the risk. For a high-cost change, we need stronger evidence.\n",
    "\n",
    "I would NOT say \"it's almost significant\" - a result either meets the pre-defined threshold or it doesn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7: How would you handle a situation where conversion rate improved but revenue per user decreased?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "This is a classic **guardrail metric** scenario. It could mean:\n",
    "\n",
    "1. **The new design attracts low-value conversions**: Maybe the simpler checkout reduces friction for small purchases but doesn't help larger ones.\n",
    "2. **Cannibalization**: Users might be buying cheaper items instead of browsing more.\n",
    "\n",
    "**What I'd do:**\n",
    "- Segment the analysis by user type, purchase amount, and product category\n",
    "- Calculate **total revenue impact** (conversion rate x average order value x traffic)\n",
    "- If net revenue is negative despite higher conversion, **do not launch**\n",
    "- Consider if the OEC (Overall Evaluation Criterion) should be revenue rather than conversion rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
