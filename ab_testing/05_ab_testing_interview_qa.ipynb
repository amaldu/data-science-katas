{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing - Interview Questions & Answers\n",
    "\n",
    "Comprehensive Q&A covering fundamentals, experiment design, metric selection, common pitfalls, and advanced topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: What is A/B testing and why is it important?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "A/B testing is a controlled experiment comparing two or more variants to measure the impact of a change on a specific metric. It's the gold standard for **causal inference** in product development because it eliminates confounders through randomization.\n",
    "\n",
    "Unlike observational studies, A/B tests let you attribute changes **directly** to your intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Explain the difference between Type I and Type II errors in A/B testing context.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "- **Type I Error** (false positive, $\\alpha$): Concluding the treatment works when it doesn't. You ship a change that has no real effect (or is harmful). Controlled by significance level $\\alpha$ (typically 5%).\n",
    "\n",
    "- **Type II Error** (false negative, $\\beta$): Missing a real effect. You fail to ship a beneficial change. Controlled by statistical power $= 1 - \\beta$ (typically 80%).\n",
    "\n",
    "**Trade-off:** Reducing $\\alpha$ (more conservative) increases $\\beta$ (more false negatives). The only way to improve both is to **increase sample size**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: What is a p-value? What is the most common misconception?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The p-value is the probability of observing data as extreme as (or more extreme than) what was observed, **assuming $H_0$ is true**.\n",
    "\n",
    "$$p\\text{-value} = P(\\text{data} \\mid H_0)$$\n",
    "\n",
    "**Common misconception:** \"The p-value is the probability that $H_0$ is true\" — this is **WRONG**. The p-value says nothing about the probability of hypotheses. It only tells you how surprising the data would be under $H_0$.\n",
    "\n",
    "Another misconception: $p = 0.04$ doesn't mean there's a 96% chance the treatment works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: What is statistical power and why does it matter?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "$$\\text{Power} = P(\\text{reject } H_0 \\mid H_0 \\text{ is false}) = 1 - \\beta$$\n",
    "\n",
    "It's the probability of detecting a real effect when one exists. Standard target is **80%**.\n",
    "\n",
    "Power depends on:\n",
    "1. **Sample size** — larger $n$ = more power\n",
    "2. **Effect size** — larger effect = easier to detect\n",
    "3. **Significance level** — higher $\\alpha$ = more power but more false positives\n",
    "4. **Variance** — lower variance = more power\n",
    "\n",
    "An underpowered test wastes resources because you're unlikely to detect the effect even if it exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Explain the difference between statistical significance and practical significance.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "- **Statistical significance** means the observed difference is unlikely due to chance ($p < \\alpha$).\n",
    "- **Practical significance** means the difference is large enough to matter for the business.\n",
    "\n",
    "They don't always align:\n",
    "1. **Large sample + tiny effect** = statistically significant but practically useless (e.g., 0.001% conversion lift on millions of users).\n",
    "2. **Small sample + large effect** = practically significant but not statistically significant (underpowered).\n",
    "\n",
    "Always evaluate **BOTH**. The confidence interval helps: it shows the range of plausible effect sizes, letting you assess practical importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6: How do you determine the sample size for an A/B test?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Sample size depends on four parameters:\n",
    "1. **Significance level** $\\alpha$ (typically 0.05)\n",
    "2. **Power** $1 - \\beta$ (typically 0.80)\n",
    "3. **Baseline metric value** (current conversion rate)\n",
    "4. **Minimum Detectable Effect (MDE)** — the smallest improvement worth detecting\n",
    "\n",
    "Formula for proportions:\n",
    "\n",
    "$$n = \\frac{(Z_{\\alpha/2} + Z_\\beta)^2 \\cdot [p_1(1 - p_1) + p_2(1 - p_2)]}{(p_1 - p_2)^2}$$\n",
    "\n",
    "Rule of thumb: $n \\approx 16\\sigma^2 / \\delta^2$ for $\\alpha = 0.05$, power $= 0.80$.\n",
    "\n",
    "**Key insight:** Halving the MDE **quadruples** the required sample size. Always calculate sample size **BEFORE** running the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7: How do you decide how long to run an A/B test?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "$$\\text{Duration} = \\frac{\\text{required\\_sample\\_size}}{\\text{daily\\_eligible\\_traffic}}$$\n",
    "\n",
    "But also consider:\n",
    "1. **Minimum 1 week** to capture weekday/weekend patterns\n",
    "2. **Avoid** holidays, sales events, or other anomalies\n",
    "3. **Account for novelty effect** — users react to ANY change initially\n",
    "4. **Account for primacy effect** — users resist change initially\n",
    "\n",
    "**Never** stop a test early just because results look significant (peeking problem). **Never** extend a test because results aren't significant yet (that's p-hacking)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8: What is the Minimum Detectable Effect (MDE) and how do you choose it?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "MDE is the smallest effect size that would be worthwhile to detect from a **business perspective**.\n",
    "\n",
    "Choosing MDE:\n",
    "1. Work with stakeholders to understand what improvement justifies the cost of implementing the change\n",
    "2. Consider the baseline metric and what's realistic\n",
    "3. Translate to business impact (e.g., 1pp conversion increase = \\$X revenue/month)\n",
    "4. Balance against duration — smaller MDE needs more data\n",
    "\n",
    "- **For large platforms** (millions of users): MDE can be small (0.5–1%) because even small effects have huge aggregate impact.\n",
    "- **For small companies**: MDE should be larger (5–10%) because you need bigger effects to justify the investment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9: What is an A/A test and why would you run one?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "An A/A test shows the **same experience** to both groups (no treatment).\n",
    "\n",
    "Purposes:\n",
    "1. **Validate** the randomization system works correctly\n",
    "2. **Verify** the logging and metric computation pipeline\n",
    "3. **Calibrate** false positive rates (you should reject $H_0$ about 5% of the time at $\\alpha = 0.05$)\n",
    "4. **Detect** selection bias or instrumentation issues\n",
    "\n",
    "If an A/A test shows significant differences, your experimentation platform has a bug. **Do NOT** run A/B tests until it's fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10: How do you choose the right metric for an A/B test?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "A good primary metric must be **MAST**:\n",
    "- **M**easurable — quantifiable\n",
    "- **A**ttributable — causally linked to the change\n",
    "- **S**ensitive — responsive to the change, low noise\n",
    "- **T**imely — observable within the test window\n",
    "\n",
    "Approach:\n",
    "1. Start with the business goal\n",
    "2. Choose **ONE** primary metric (decision metric)\n",
    "3. Define guardrail metrics (must not degrade)\n",
    "4. Add secondary metrics (nice to understand but don't drive decisions)\n",
    "\n",
    "**Example:** Testing a new checkout flow —\n",
    "- *Primary:* conversion rate\n",
    "- *Guardrails:* page load time, error rate, revenue per user\n",
    "- *Secondary:* cart abandonment rate, time to purchase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q11: What are guardrail metrics and why are they important?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Guardrail metrics are secondary metrics that **must NOT degrade**, even if the primary metric improves. They protect against unintended consequences.\n",
    "\n",
    "Examples:\n",
    "1. Testing a more aggressive notification strategy → *Primary:* engagement. *Guardrail:* unsubscribe rate.\n",
    "2. Testing a simpler checkout → *Primary:* conversion. *Guardrail:* average order value, revenue.\n",
    "\n",
    "**Rule:** If a guardrail metric degrades significantly, **DO NOT launch** the treatment regardless of the primary metric result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Pitfalls & Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q12: What is the peeking problem and how do you address it?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Peeking** = checking test results repeatedly before the planned end date and stopping early when you see significance.\n",
    "\n",
    "**Problem:** At any point during the test, random fluctuations can produce a \"significant\" result. If you check daily for 14 days, your effective false positive rate can be **25–30%** instead of 5%.\n",
    "\n",
    "**Solutions:**\n",
    "1. Pre-commit to a sample size and don't peek\n",
    "2. Use **sequential testing** methods (e.g., group sequential designs with O'Brien-Fleming bounds)\n",
    "3. Use **Bayesian methods** where peeking is more natural\n",
    "4. Use **always-valid confidence intervals**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q13: What is Simpson's Paradox and how can it affect A/B tests?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Simpson's Paradox occurs when a trend that appears in **aggregate** data **reverses** when the data is segmented.\n",
    "\n",
    "**Example:** Treatment wins overall, but loses in every individual segment (mobile, desktop, tablet) because of unequal traffic distribution across segments.\n",
    "\n",
    "How to handle:\n",
    "1. Always **segment** your analysis by key dimensions\n",
    "2. Check for **interaction effects** between the treatment and important covariates\n",
    "3. Ensure randomization is balanced across segments (**stratified randomization**)\n",
    "4. Use the segment-level analysis to understand **WHY**, but the overall metric for the decision (unless there's a specific reason to focus on a segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q14: What is network effect (interference) and how does it affect A/B tests?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Network effect / interference occurs when a user's experience in one group **affects users in another group**, violating the independence assumption (**SUTVA** — Stable Unit Treatment Value Assumption).\n",
    "\n",
    "Examples:\n",
    "1. **Social networks:** if treatment users share content differently, control users' feeds change too\n",
    "2. **Marketplace:** if treatment sellers change pricing, control buyers see different prices\n",
    "3. **Ride-sharing:** if treatment changes driver incentives, control riders are affected\n",
    "\n",
    "**Solutions:**\n",
    "1. **Cluster randomization** — randomize by geography, social cluster\n",
    "2. **Switchback experiments** — alternate treatment/control over time\n",
    "3. Use **ego-network randomization** for social products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q15: What is the multiple testing problem? How do you handle it?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "When running multiple tests or checking multiple metrics, the probability of at least one false positive increases:\n",
    "\n",
    "$$P(\\text{at least 1 FP}) = 1 - (1 - \\alpha)^k$$\n",
    "\n",
    "With 20 metrics at $\\alpha = 0.05$: **64% chance** of at least one false positive!\n",
    "\n",
    "**Correction methods:**\n",
    "1. **Bonferroni:** divide $\\alpha$ by number of tests. Simple but very conservative.\n",
    "2. **Holm-Bonferroni:** step-down procedure, less conservative.\n",
    "3. **Benjamini-Hochberg (FDR):** controls false discovery rate instead of family-wise error rate. Best for exploratory analysis with many metrics.\n",
    "4. **Pre-register ONE primary metric** and treat others as exploratory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q16: When would you use Bayesian A/B testing over frequentist?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Use Bayesian when:**\n",
    "1. Stakeholders want interpretable results (\"82% probability B is better\" vs \"$p = 0.03$\")\n",
    "2. You need to stop tests early or monitor continuously\n",
    "3. You have strong prior information to incorporate\n",
    "4. You want to make risk-aware decisions using expected loss\n",
    "\n",
    "**Use frequentist when:**\n",
    "1. You need well-established, widely accepted methodology\n",
    "2. Regulatory or compliance requirements demand frequentist methods\n",
    "3. Simplicity and computational efficiency matter\n",
    "4. You have clear pre-defined sample sizes and timelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q17: What is CUPED and why is it used?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**CUPED** (Controlled-experiment Using Pre-Experiment Data) is a **variance reduction** technique. It uses pre-experiment data (covariates) correlated with the metric to reduce noise.\n",
    "\n",
    "Formula:\n",
    "\n",
    "$$Y_{\\text{adjusted}} = Y - \\theta \\cdot (X - \\bar{X})$$\n",
    "\n",
    "where $X$ is the pre-experiment covariate.\n",
    "\n",
    "**Benefits:** Can reduce variance by **50%+**, which means you need much smaller samples (or shorter tests) to detect the same effect.\n",
    "\n",
    "It's widely used at Microsoft, Netflix, Uber, and other large tech companies. It's essentially applying **regression adjustment** to experimental data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q18: What is a Multi-Armed Bandit? How does it differ from A/B testing?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Multi-Armed Bandit (MAB)** dynamically allocates more traffic to better-performing variants during the experiment.\n",
    "\n",
    "Key differences from A/B testing:\n",
    "\n",
    "| | A/B Test | Multi-Armed Bandit |\n",
    "|---|---|---|\n",
    "| **Allocation** | Fixed (50/50) | Dynamic (adapts over time) |\n",
    "| **Optimizes for** | Statistical error minimization | Opportunity cost (regret) minimization |\n",
    "| **Inference** | Clear statistical inference | Trades inference quality for optimization |\n",
    "\n",
    "**Use MAB when:**\n",
    "1. Opportunity cost of showing inferior variant is high\n",
    "2. You care more about optimization than inference\n",
    "3. Short-term decisions with limited traffic\n",
    "\n",
    "**Use A/B when:**\n",
    "1. You need rigorous causal inference\n",
    "2. You want to understand the magnitude of the effect\n",
    "3. Long-term product decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q19: Your A/B test shows no significant result. What do you tell stakeholders?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "\"Fail to reject $H_0$\" does **NOT** mean \"no effect exists.\"\n",
    "\n",
    "My approach:\n",
    "1. **Check the confidence interval** — if it includes meaningful positive effects, the test may be underpowered. Recommend extending.\n",
    "2. **Check if the CI is narrow and centered around zero** — then confidently say \"the effect, if any, is too small to matter.\"\n",
    "3. **Check for bugs** — SRM, instrumentation issues, incorrect metric computation.\n",
    "4. **Segment analysis** — maybe the effect exists in a subgroup.\n",
    "5. **Be honest:** \"We don't have enough evidence to conclude the treatment is better. Here's what the data tells us about the range of possible effects.\"\n",
    "\n",
    "**Never** say \"the treatment doesn't work\" — that's accepting $H_0$, which is different from failing to reject it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q20: Walk me through how you would design an A/B test from scratch for a new feature.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Step-by-step framework:\n",
    "\n",
    "1. **UNDERSTAND:** What business problem does this solve? What's the expected user behavior change?\n",
    "2. **HYPOTHESIZE:** Define $H_0$ and $H_a$. Use PICOT framework.\n",
    "3. **METRICS:** Choose primary metric (one!), guardrail metrics, secondary metrics.\n",
    "4. **DESIGN:** Set $\\alpha$ (0.05), power (0.80), calculate MDE with stakeholders, compute sample size, estimate duration.\n",
    "5. **VALIDATE:** Run A/A test to verify the platform. Check for SRM.\n",
    "6. **RUN:** Deploy to randomized groups. Monitor guardrail metrics daily. Don't peek at primary metric.\n",
    "7. **ANALYZE:** After planned duration, compute test statistic and p-value. Calculate confidence interval. Check practical significance.\n",
    "8. **DECIDE:**\n",
    "   - Significant + practical → **launch**\n",
    "   - Not significant + narrow CI around 0 → **don't launch**\n",
    "   - Inconclusive → **extend or redesign**\n",
    "   - Guardrails degraded → **don't launch**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
