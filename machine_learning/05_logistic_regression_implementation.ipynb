{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Implementation from Scratch\n",
    "\n",
    "## Overview\n",
    "Binary classification using the **sigmoid function** and **gradient descent**.\n",
    "\n",
    "### Key Formulas\n",
    "- **Sigmoid:** $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "- **Hypothesis:** $\\hat{y} = \\sigma(X\\theta)$\n",
    "- **Cost (Log Loss):** $J(\\theta) = -\\frac{1}{m}\\sum[y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})]$\n",
    "- **Gradient:** $\\nabla J = \\frac{1}{m}X^T(\\hat{y} - y)$\n",
    "- **Update:** $\\theta := \\theta - \\alpha \\nabla J$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate and Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate binary classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=300, n_features=2, n_informative=2,\n",
    "    n_redundant=0, n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f'Training: {X_train.shape[0]} samples')\n",
    "print(f'Test: {X_test.shape[0]} samples')\n",
    "print(f'Class distribution (train): {np.bincount(y_train)}')\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train_scaled[y_train == 0, 0], X_train_scaled[y_train == 0, 1],\n",
    "            alpha=0.6, label='Class 0', marker='o')\n",
    "plt.scatter(X_train_scaled[y_train == 1, 0], X_train_scaled[y_train == 1, 1],\n",
    "            alpha=0.6, label='Class 1', marker='s')\n",
    "plt.xlabel('Feature 1 (scaled)')\n",
    "plt.ylabel('Feature 2 (scaled)')\n",
    "plt.title('Binary Classification Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    \"\"\"Add column of ones for intercept term.\"\"\"\n",
    "    return np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    \n",
    "    sigma(z) = 1 / (1 + e^(-z))\n",
    "    \n",
    "    Clips z to avoid overflow in exp.\n",
    "    \"\"\"\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "# Visualize sigmoid\n",
    "z_vals = np.linspace(-10, 10, 200)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(z_vals, sigmoid(z_vals), 'b-', linewidth=2)\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Threshold = 0.5')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('\u03c3(z)')\n",
    "plt.title('Sigmoid Function')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Binary cross-entropy (log loss) cost function.\n",
    "    \n",
    "    J(theta) = -(1/m) * sum[ y*log(h) + (1-y)*log(1-h) ]\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    h = sigmoid(X @ theta)\n",
    "    # Clip to avoid log(0)\n",
    "    h = np.clip(h, 1e-10, 1 - 1e-10)\n",
    "    cost = -(1/m) * (y @ np.log(h) + (1 - y) @ np.log(1 - h))\n",
    "    return cost\n",
    "\n",
    "\n",
    "def compute_gradient(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute gradient of cost function.\n",
    "    \n",
    "    gradient = (1/m) * X^T * (sigmoid(X*theta) - y)\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    h = sigmoid(X @ theta)\n",
    "    gradient = (1/m) * X.T @ (h - y)\n",
    "    return gradient\n",
    "\n",
    "\n",
    "print('Core functions defined: sigmoid, compute_cost, compute_gradient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, learning_rate, n_iterations):\n",
    "    \"\"\"\n",
    "    Optimize theta using gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix with intercept (m, n+1)\n",
    "        y: Target labels (m,)\n",
    "        theta: Initial parameters (n+1,)\n",
    "        learning_rate: Step size alpha\n",
    "        n_iterations: Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        theta: Optimized parameters\n",
    "        cost_history: Cost at each iteration\n",
    "    \"\"\"\n",
    "    cost_history = []\n",
    "    theta = theta.copy()\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Compute gradient\n",
    "        gradient = compute_gradient(X, y, theta)\n",
    "        \n",
    "        # Update parameters\n",
    "        theta -= learning_rate * gradient\n",
    "        \n",
    "        # Record cost\n",
    "        cost = compute_cost(X, y, theta)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f'Iteration {i+1:4d}: Cost = {cost:.6f}')\n",
    "    \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_train_b = add_intercept(X_train_scaled)\n",
    "X_test_b = add_intercept(X_test_scaled)\n",
    "\n",
    "# Initialize parameters to zeros\n",
    "initial_theta = np.zeros(X_train_b.shape[1])\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "n_iterations = 1000\n",
    "\n",
    "print(f'Training with lr={learning_rate}, iterations={n_iterations}\\n')\n",
    "\n",
    "# Train\n",
    "theta_opt, cost_history = gradient_descent(\n",
    "    X_train_b, y_train, initial_theta, learning_rate, n_iterations\n",
    ")\n",
    "\n",
    "print(f'\\nOptimal parameters:')\n",
    "print(f'  theta_0 (intercept): {theta_opt[0]:.4f}')\n",
    "print(f'  theta_1: {theta_opt[1]:.4f}')\n",
    "print(f'  theta_2: {theta_opt[2]:.4f}')\n",
    "print(f'  Final cost: {cost_history[-1]:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Cost Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(cost_history, 'b-', linewidth=2)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost J(theta)')\n",
    "plt.title('Cost Function Convergence (Log Loss)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "print(f'Cost: {cost_history[0]:.4f} -> {cost_history[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make Predictions and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(X, theta):\n",
    "    \"\"\"Predict class probabilities.\"\"\"\n",
    "    return sigmoid(X @ theta)\n",
    "\n",
    "def predict(X, theta, threshold=0.5):\n",
    "    \"\"\"Predict class labels.\"\"\"\n",
    "    return (predict_proba(X, theta) >= threshold).astype(int)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = predict(X_train_b, theta_opt)\n",
    "y_test_pred = predict(X_test_b, theta_opt)\n",
    "\n",
    "# Metrics\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print('=' * 50)\n",
    "print('MODEL PERFORMANCE')\n",
    "print('=' * 50)\n",
    "print(f'Training Accuracy: {train_acc:.4f} ({train_acc*100:.1f}%)')\n",
    "print(f'Test Accuracy:     {test_acc:.4f} ({test_acc*100:.1f}%)')\n",
    "print('\\nClassification Report (Test Set):')\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print('Confusion Matrix (Test Set):')\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, theta, title='Decision Boundary'):\n",
    "    \"\"\"Plot data points and decision boundary.\"\"\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    # Create mesh grid\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200),\n",
    "                           np.linspace(x2_min, x2_max, 200))\n",
    "    \n",
    "    # Predict on grid\n",
    "    grid = np.c_[np.ones(xx1.ravel().shape[0]), xx1.ravel(), xx2.ravel()]\n",
    "    probs = predict_proba(grid, theta).reshape(xx1.shape)\n",
    "    \n",
    "    # Plot probability contours\n",
    "    plt.contourf(xx1, xx2, probs, levels=50, cmap='RdYlBu', alpha=0.4)\n",
    "    plt.colorbar(label='P(class=1)')\n",
    "    \n",
    "    # Decision boundary (where P = 0.5)\n",
    "    plt.contour(xx1, xx2, probs, levels=[0.5], colors='black', linewidths=2)\n",
    "    \n",
    "    # Data points\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', label='Class 0',\n",
    "                edgecolors='k', alpha=0.7)\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', label='Class 1',\n",
    "                edgecolors='k', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(X_train_scaled, y_train, theta_opt, 'Training Set - Decision Boundary')\n",
    "plot_decision_boundary(X_test_scaled, y_test, theta_opt, 'Test Set - Decision Boundary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Full Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionScratch:\n",
    "    \"\"\"\n",
    "    Logistic Regression from scratch using gradient descent.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.1, n_iterations=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iter = n_iterations\n",
    "        self.theta = None\n",
    "        self.cost_history = []\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def _add_intercept(self, X):\n",
    "        return np.c_[np.ones(X.shape[0]), X]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X_b = self._add_intercept(X)\n",
    "        m, n = X_b.shape\n",
    "        self.theta = np.zeros(n)\n",
    "        self.cost_history = []\n",
    "        \n",
    "        for _ in range(self.n_iter):\n",
    "            h = self._sigmoid(X_b @ self.theta)\n",
    "            gradient = (1/m) * X_b.T @ (h - y)\n",
    "            self.theta -= self.lr * gradient\n",
    "            \n",
    "            h_clipped = np.clip(h, 1e-10, 1 - 1e-10)\n",
    "            cost = -(1/m) * (y @ np.log(h_clipped) + (1-y) @ np.log(1-h_clipped))\n",
    "            self.cost_history.append(cost)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X_b = self._add_intercept(X)\n",
    "        return self._sigmoid(X_b @ self.theta)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)\n",
    "\n",
    "\n",
    "# Use the class\n",
    "model = LogisticRegressionScratch(learning_rate=0.1, n_iterations=1000)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f'Training Accuracy: {model.score(X_train_scaled, y_train):.4f}')\n",
    "print(f'Test Accuracy:     {model.score(X_test_scaled, y_test):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparison with Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Sklearn model\n",
    "sk_model = LogisticRegression(random_state=42)\n",
    "sk_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Parameter Comparison:')\n",
    "print('=' * 50)\n",
    "print(f'Our model  -> intercept: {model.theta[0]:.4f}, coefs: {model.theta[1:]}')\n",
    "print(f'Sklearn    -> intercept: {sk_model.intercept_[0]:.4f}, coefs: {sk_model.coef_[0]}')\n",
    "\n",
    "print(f'\\nAccuracy Comparison (Test):')\n",
    "print(f'Our model:  {model.score(X_test_scaled, y_test):.4f}')\n",
    "print(f'Sklearn:    {sk_model.score(X_test_scaled, y_test):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Threshold Tuning\n",
    "\n",
    "The default threshold is 0.5, but it can be adjusted based on the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate different thresholds\n",
    "thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "results = []\n",
    "\n",
    "for t in thresholds:\n",
    "    preds = (predict_proba(X_test_b, theta_opt) >= t).astype(int)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    tp = cm[1, 1] if cm.shape == (2, 2) else 0\n",
    "    fp = cm[0, 1] if cm.shape == (2, 2) else 0\n",
    "    fn = cm[1, 0] if cm.shape == (2, 2) else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    results.append((t, acc, precision, recall))\n",
    "    print(f'Threshold {t:.1f}: Acc={acc:.3f}, Precision={precision:.3f}, Recall={recall:.3f}')\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ts = [r[0] for r in results]\n",
    "ax.plot(ts, [r[1] for r in results], 'b-o', label='Accuracy')\n",
    "ax.plot(ts, [r[2] for r in results], 'g-s', label='Precision')\n",
    "ax.plot(ts, [r[3] for r in results], 'r-^', label='Recall')\n",
    "ax.set_xlabel('Threshold')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Metrics vs Decision Threshold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Sigmoid function** maps linear output to probability [0, 1]\n",
    "2. **Log loss** (binary cross-entropy) is the correct cost function, not MSE\n",
    "3. **Gradient formula** looks like linear regression: $(1/m)X^T(\\hat{y} - y)$, but $\\hat{y} = \\sigma(X\\theta)$\n",
    "4. **No closed-form solution** - must use gradient descent or similar optimizer\n",
    "5. **Feature scaling** is important for convergence\n",
    "6. **Decision boundary** is linear (hyperplane where $X\\theta = 0$)\n",
    "7. **Threshold tuning** lets you trade off precision vs recall\n",
    "8. Logistic Regression is a strong **baseline** for any classification task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}