{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression - Normal Equation Implementation\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates the implementation of Linear Regression using the **Normal Equation** (closed-form solution).\n",
    "\n",
    "### Normal Equation Formula\n",
    "$$\\theta = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "Where:\n",
    "- $\\theta$ = parameter vector (weights)\n",
    "- $X$ = feature matrix\n",
    "- $y$ = target vector\n",
    "\n",
    "### Advantages\n",
    "- ✅ No need to choose learning rate\n",
    "- ✅ No iterations required\n",
    "- ✅ Gives exact solution in one computation\n",
    "\n",
    "### Disadvantages\n",
    "- ❌ Slow for large datasets (O(n³) complexity)\n",
    "- ❌ Requires matrix inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, alpha=0.6, label='Training data')\n",
    "plt.scatter(X_test, y_test, alpha=0.6, color='orange', label='Test data')\n",
    "plt.xlabel('Feature (X)')\n",
    "plt.ylabel('Target (y)')\n",
    "plt.title('Training and Test Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation from Scratch\n",
    "\n",
    "### Step 1: Add Bias Term (Intercept)\n",
    "We add a column of ones to X to account for the intercept term $\\theta_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    \"\"\"\n",
    "    Add a column of ones to the feature matrix for the intercept term.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (m, n)\n",
    "    \n",
    "    Returns:\n",
    "        Feature matrix with intercept column, shape (m, n+1)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    intercept = np.ones((m, 1))\n",
    "    return np.concatenate([intercept, X], axis=1)\n",
    "\n",
    "# Add intercept to training and test data\n",
    "X_train_b = add_intercept(X_train)\n",
    "X_test_b = add_intercept(X_test)\n",
    "\n",
    "print(f\"Original X_train shape: {X_train.shape}\")\n",
    "print(f\"X_train with bias shape: {X_train_b.shape}\")\n",
    "print(f\"\\nFirst 3 rows of X_train_b:\\n{X_train_b[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation(X, y):\n",
    "    \"\"\"\n",
    "    Compute the closed-form solution for linear regression.\n",
    "    \n",
    "    Formula: θ = (X^T X)^(-1) X^T y\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix with intercept, shape (m, n+1)\n",
    "        y: Target vector, shape (m,) or (m, 1)\n",
    "    \n",
    "    Returns:\n",
    "        theta: Optimal parameters, shape (n+1, 1)\n",
    "    \"\"\"\n",
    "    # Ensure y is a column vector\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "    \n",
    "    # Normal equation: θ = (X^T X)^(-1) X^T y\n",
    "    theta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    \n",
    "    return theta\n",
    "\n",
    "# Calculate optimal parameters\n",
    "theta_optimal = normal_equation(X_train_b, y_train)\n",
    "\n",
    "print(\"Optimal Parameters:\")\n",
    "print(f\"θ₀ (intercept): {theta_optimal[0][0]:.4f}\")\n",
    "print(f\"θ₁ (slope): {theta_optimal[1][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Using Moore-Penrose Pseudo-Inverse\n",
    "\n",
    "For better numerical stability, especially when $X^T X$ is close to singular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation_pinv(X, y):\n",
    "    \"\"\"\n",
    "    Compute the solution using pseudo-inverse for better numerical stability.\n",
    "    \n",
    "    Formula: θ = X⁺ y, where X⁺ is the Moore-Penrose pseudo-inverse\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix with intercept, shape (m, n+1)\n",
    "        y: Target vector, shape (m,) or (m, 1)\n",
    "    \n",
    "    Returns:\n",
    "        theta: Optimal parameters, shape (n+1, 1)\n",
    "    \"\"\"\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "    \n",
    "    # Using pseudo-inverse\n",
    "    theta = np.linalg.pinv(X) @ y\n",
    "    \n",
    "    return theta\n",
    "\n",
    "# Calculate using pseudo-inverse\n",
    "theta_pinv = normal_equation_pinv(X_train_b, y_train)\n",
    "\n",
    "print(\"\\nParameters using Pseudo-inverse:\")\n",
    "print(f\"θ₀ (intercept): {theta_pinv[0][0]:.4f}\")\n",
    "print(f\"θ₁ (slope): {theta_pinv[1][0]:.4f}\")\n",
    "\n",
    "# Compare results\n",
    "print(f\"\\nDifference between methods: {np.linalg.norm(theta_optimal - theta_pinv):.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    \"\"\"\n",
    "    Make predictions using learned parameters.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix with intercept, shape (m, n+1)\n",
    "        theta: Parameters, shape (n+1, 1)\n",
    "    \n",
    "    Returns:\n",
    "        predictions: Predicted values, shape (m,)\n",
    "    \"\"\"\n",
    "    return (X @ theta).flatten()\n",
    "\n",
    "# Predictions on training and test sets\n",
    "y_train_pred = predict(X_train_b, theta_optimal)\n",
    "y_test_pred = predict(X_test_b, theta_optimal)\n",
    "\n",
    "print(\"First 5 predictions vs actual values (Test set):\")\n",
    "for i in range(min(5, len(y_test))):\n",
    "    print(f\"Predicted: {y_test_pred[i]:7.2f}, Actual: {y_test[i]:7.2f}, Error: {abs(y_test_pred[i] - y_test[i]):6.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL PERFORMANCE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  MSE:  {train_mse:.4f}\")\n",
    "print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "print(f\"  R²:   {train_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  MSE:  {test_mse:.4f}\")\n",
    "print(f\"  RMSE: {test_rmse:.4f}\")\n",
    "print(f\"  R²:   {test_r2:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Regression line with data points\n",
    "axes[0].scatter(X_train, y_train, alpha=0.6, label='Training data')\n",
    "axes[0].scatter(X_test, y_test, alpha=0.6, color='orange', label='Test data')\n",
    "\n",
    "# Plot regression line\n",
    "X_range = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "X_range_b = add_intercept(X_range)\n",
    "y_range_pred = predict(X_range_b, theta_optimal)\n",
    "axes[0].plot(X_range, y_range_pred, 'r-', linewidth=2, label='Regression line')\n",
    "\n",
    "axes[0].set_xlabel('Feature (X)')\n",
    "axes[0].set_ylabel('Target (y)')\n",
    "axes[0].set_title('Linear Regression Fit (Normal Equation)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Residuals\n",
    "residuals_train = y_train - y_train_pred\n",
    "residuals_test = y_test - y_test_pred\n",
    "\n",
    "axes[1].scatter(y_train_pred, residuals_train, alpha=0.6, label='Training residuals')\n",
    "axes[1].scatter(y_test_pred, residuals_test, alpha=0.6, color='orange', label='Test residuals')\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted values')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "axes[1].set_title('Residual Plot')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multiple Features Example\n",
    "\n",
    "Let's demonstrate with multiple features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with multiple features\n",
    "X_multi, y_multi = make_regression(n_samples=200, n_features=5, noise=15, random_state=42)\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_multi, y_multi, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Multiple features dataset:\")\n",
    "print(f\"  Training: {X_train_m.shape}\")\n",
    "print(f\"  Test: {X_test_m.shape}\")\n",
    "\n",
    "# Add intercept\n",
    "X_train_m_b = add_intercept(X_train_m)\n",
    "X_test_m_b = add_intercept(X_test_m)\n",
    "\n",
    "# Apply normal equation\n",
    "theta_multi = normal_equation(X_train_m_b, y_train_m)\n",
    "\n",
    "print(f\"\\nOptimal parameters (θ):\")\n",
    "print(f\"  Intercept (θ₀): {theta_multi[0][0]:.4f}\")\n",
    "for i in range(1, len(theta_multi)):\n",
    "    print(f\"  θ{i}: {theta_multi[i][0]:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_m = predict(X_train_m_b, theta_multi)\n",
    "y_test_pred_m = predict(X_test_m_b, theta_multi)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Training R²: {r2_score(y_train_m, y_train_pred_m):.4f}\")\n",
    "print(f\"  Test R²: {r2_score(y_test_m, y_test_pred_m):.4f}\")\n",
    "print(f\"  Test RMSE: {np.sqrt(mean_squared_error(y_test_m, y_test_pred_m)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison with Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Fit sklearn model\n",
    "sklearn_model = LinearRegression()\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "\n",
    "# Compare parameters\n",
    "print(\"Parameter Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Our Implementation:\")\n",
    "print(f\"  Intercept: {theta_optimal[0][0]:.6f}\")\n",
    "print(f\"  Slope:     {theta_optimal[1][0]:.6f}\")\n",
    "\n",
    "print(f\"\\nScikit-learn:\")\n",
    "print(f\"  Intercept: {sklearn_model.intercept_:.6f}\")\n",
    "print(f\"  Slope:     {sklearn_model.coef_[0]:.6f}\")\n",
    "\n",
    "print(f\"\\nDifference:\")\n",
    "print(f\"  Intercept: {abs(theta_optimal[0][0] - sklearn_model.intercept_):.10f}\")\n",
    "print(f\"  Slope:     {abs(theta_optimal[1][0] - sklearn_model.coef_[0]):.10f}\")\n",
    "\n",
    "# Compare predictions\n",
    "sklearn_pred = sklearn_model.predict(X_test)\n",
    "print(f\"\\nTest R² Score:\")\n",
    "print(f\"  Our Implementation: {test_r2:.6f}\")\n",
    "print(f\"  Scikit-learn:       {sklearn_model.score(X_test, y_test):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Normal Equation** provides an exact, closed-form solution in one computation\n",
    "2. **Formula**: $\\theta = (X^T X)^{-1} X^T y$\n",
    "3. **No hyperparameters** to tune (no learning rate, no iterations)\n",
    "4. **Computational complexity**: O(n³) due to matrix inversion\n",
    "5. **Best for**: Small to medium datasets with moderate number of features\n",
    "6. **Use pseudo-inverse** (`np.linalg.pinv`) for better numerical stability\n",
    "7. **Works for multiple features** without modification\n",
    "\n",
    "### When to Use Normal Equation:\n",
    "- ✅ Small datasets (< 10,000 samples)\n",
    "- ✅ Moderate number of features (< 10,000)\n",
    "- ✅ Need exact solution quickly\n",
    "- ✅ Don't want to tune hyperparameters\n",
    "\n",
    "### When NOT to Use:\n",
    "- ❌ Very large datasets\n",
    "- ❌ Many features (> 10,000)\n",
    "- ❌ Online learning scenarios\n",
    "- ❌ When $X^T X$ is singular/non-invertible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
