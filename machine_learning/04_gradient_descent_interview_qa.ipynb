{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent - Interview Q&A\n",
    "\n",
    "Interview-level questions and answers about Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: What is Gradient Descent? Explain the intuition.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Gradient Descent is an **iterative optimization algorithm** used to minimize a function by moving in the direction of steepest descent (negative gradient).\n",
    "\n",
    "**Intuition:** Imagine standing on a hill in fog. You can't see the valley, but you can feel the slope under your feet. Gradient Descent says: take a step in the steepest downhill direction. Repeat until you reach the bottom.\n",
    "\n",
    "**Update Rule:**\n",
    "$$\\theta := \\theta - \\alpha \\nabla J(\\theta)$$\n",
    "\n",
    "Where:\n",
    "- $\\theta$ = parameters\n",
    "- $\\alpha$ = learning rate (step size)\n",
    "- $\\nabla J(\\theta)$ = gradient of cost function\n",
    "\n",
    "**For Linear Regression:**\n",
    "$$\\theta := \\theta - \\frac{\\alpha}{m} X^T(X\\theta - y)$$\n",
    "\n",
    "**Key idea:** The gradient tells us the direction of steepest **ascent**, so we subtract it to go **downhill**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: What is the learning rate? What happens if it's too large or too small?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The **learning rate** ($\\alpha$) controls how big each step is during gradient descent.\n",
    "\n",
    "**Too small ($\\alpha$ = 0.0001):**\n",
    "- Very slow convergence\n",
    "- May take thousands of iterations\n",
    "- More computation time\n",
    "- But will likely converge\n",
    "\n",
    "**Too large ($\\alpha$ = 10):**\n",
    "- Overshoots the minimum\n",
    "- Oscillates around the minimum\n",
    "- May diverge (cost increases)\n",
    "- Algorithm fails to converge\n",
    "\n",
    "**Just right ($\\alpha$ = 0.01):**\n",
    "- Steady decrease in cost\n",
    "- Converges in reasonable iterations\n",
    "- Smooth convergence curve\n",
    "\n",
    "**How to choose:**\n",
    "1. Start small (0.001) and increase\n",
    "2. Try values on log scale: 0.001, 0.003, 0.01, 0.03, 0.1, 0.3\n",
    "3. Plot cost vs iterations - look for smooth decrease\n",
    "4. Use learning rate schedules (decay over time)\n",
    "5. Use adaptive methods (Adam, RMSProp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: What are the three types of Gradient Descent?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**1. Batch Gradient Descent:**\n",
    "- Uses ALL training samples per update\n",
    "- Computes exact gradient\n",
    "- Smooth convergence\n",
    "- Slow for large datasets\n",
    "\n",
    "```python\n",
    "# Batch GD\n",
    "gradient = (1/m) * X.T @ (X @ theta - y)  # all m samples\n",
    "theta = theta - lr * gradient\n",
    "```\n",
    "\n",
    "**2. Stochastic Gradient Descent (SGD):**\n",
    "- Uses ONE random sample per update\n",
    "- Noisy gradient estimate\n",
    "- Faster updates, but noisy convergence\n",
    "- Can escape local minima (noise helps)\n",
    "- Good for online learning\n",
    "\n",
    "```python\n",
    "# SGD\n",
    "i = np.random.randint(m)  # pick one sample\n",
    "gradient = X[i:i+1].T @ (X[i:i+1] @ theta - y[i:i+1])\n",
    "theta = theta - lr * gradient\n",
    "```\n",
    "\n",
    "**3. Mini-Batch Gradient Descent:**\n",
    "- Uses a small batch (32, 64, 128 samples) per update\n",
    "- Best of both worlds\n",
    "- Most commonly used in practice\n",
    "- Leverages GPU parallelism\n",
    "\n",
    "```python\n",
    "# Mini-Batch GD\n",
    "batch = random_sample(X, y, batch_size=32)\n",
    "gradient = (1/batch_size) * X_batch.T @ (X_batch @ theta - y_batch)\n",
    "theta = theta - lr * gradient\n",
    "```\n",
    "\n",
    "**Comparison:**\n",
    "\n",
    "| Type | Speed | Stability | Memory | Usage |\n",
    "|------|-------|-----------|--------|-------|\n",
    "| Batch | Slow | Stable | High | Small data |\n",
    "| SGD | Fast | Noisy | Low | Online |\n",
    "| Mini-Batch | Medium | Medium | Medium | Most common |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Why is feature scaling important for Gradient Descent?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Without scaling:**\n",
    "- Features on different scales create elongated contours in the cost function\n",
    "- Gradient descent oscillates (zigzags) across the narrow dimension\n",
    "- Takes many more iterations to converge\n",
    "- May need very small learning rate\n",
    "\n",
    "**With scaling:**\n",
    "- Contours become more circular\n",
    "- Gradient points more directly toward the minimum\n",
    "- Converges much faster\n",
    "- Same learning rate works well for all features\n",
    "\n",
    "**Example:**\n",
    "- Feature 1: House size (0 - 5000 sq ft)\n",
    "- Feature 2: Number of bedrooms (1 - 5)\n",
    "- Without scaling, the gradient is dominated by the larger scale feature\n",
    "\n",
    "**Common scaling methods:**\n",
    "1. **Standardization (Z-score):** $x' = \\frac{x - \\mu}{\\sigma}$ (mean=0, std=1)\n",
    "2. **Min-Max normalization:** $x' = \\frac{x - x_{min}}{x_{max} - x_{min}}$ (range 0-1)\n",
    "\n",
    "**Note:** Feature scaling is NOT needed for the Normal Equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: How do you know if Gradient Descent has converged?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Methods to check convergence:**\n",
    "\n",
    "1. **Plot cost vs iterations:**\n",
    "   - Cost should decrease with each iteration\n",
    "   - Curve flattens when converged\n",
    "   - If cost increases, learning rate is too high\n",
    "\n",
    "2. **Automatic convergence test:**\n",
    "   - Stop when cost decreases by less than a threshold $\\epsilon$\n",
    "   - Example: Stop if $J(\\theta^{(t-1)}) - J(\\theta^{(t)}) < 10^{-6}$\n",
    "\n",
    "3. **Gradient magnitude:**\n",
    "   - At minimum, gradient approaches zero\n",
    "   - Stop when $||\\nabla J(\\theta)|| < \\epsilon$\n",
    "\n",
    "4. **Parameter change:**\n",
    "   - Stop when $||\\theta^{(t)} - \\theta^{(t-1)}|| < \\epsilon$\n",
    "\n",
    "**Red flags:**\n",
    "- Cost increases -> learning rate too high\n",
    "- Cost fluctuates wildly -> learning rate too high or SGD noise\n",
    "- Cost decreases very slowly -> learning rate too low\n",
    "- Cost plateaus early -> may be stuck in local minimum (not an issue for linear regression since MSE is convex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6: What is the difference between convex and non-convex cost functions? Why does it matter?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Convex function:**\n",
    "- Has exactly ONE global minimum\n",
    "- No local minima\n",
    "- Bowl-shaped (any line between two points lies above the function)\n",
    "- **Linear regression MSE is convex**\n",
    "- Gradient descent guaranteed to find global minimum\n",
    "\n",
    "**Non-convex function:**\n",
    "- Has multiple local minima\n",
    "- Gradient descent may get stuck in a local minimum\n",
    "- Example: Neural network cost functions\n",
    "- Need techniques like momentum, random restarts, simulated annealing\n",
    "\n",
    "**Why it matters for linear regression:**\n",
    "- MSE with linear model is **always convex**\n",
    "- No local minima to worry about\n",
    "- GD will always converge to global minimum (with proper learning rate)\n",
    "- Both Normal Equation and GD give same result\n",
    "\n",
    "**Mathematical proof of convexity:**\n",
    "- Second derivative (Hessian) of MSE = $\\frac{2}{m}X^TX$\n",
    "- $X^TX$ is positive semi-definite\n",
    "- Therefore MSE is convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7: Explain the gradient computation step-by-step for linear regression.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Cost function:**\n",
    "$$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "Where $h_\\theta(x) = \\theta^Tx = X\\theta$\n",
    "\n",
    "**Step 1 - Compute predictions:**\n",
    "$$\\hat{y} = X\\theta$$\n",
    "\n",
    "**Step 2 - Compute errors:**\n",
    "$$e = \\hat{y} - y = X\\theta - y$$\n",
    "\n",
    "**Step 3 - Compute gradient:**\n",
    "\n",
    "Take the partial derivative of $J$ with respect to $\\theta_j$:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_j} = \\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "In vectorized form:\n",
    "$$\\nabla J(\\theta) = \\frac{1}{m}X^T(X\\theta - y)$$\n",
    "\n",
    "**Step 4 - Update parameters:**\n",
    "$$\\theta := \\theta - \\alpha \\cdot \\nabla J(\\theta)$$\n",
    "\n",
    "**Python code:**\n",
    "```python\n",
    "predictions = X @ theta           # Step 1\n",
    "errors = predictions - y          # Step 2\n",
    "gradient = (1/m) * X.T @ errors   # Step 3\n",
    "theta = theta - alpha * gradient  # Step 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8: What is the computational complexity of Gradient Descent vs Normal Equation?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Normal Equation:**\n",
    "- $O(n^3)$ - dominated by matrix inversion of $(X^TX)^{-1}$\n",
    "- $X^TX$ computation: $O(mn^2)$\n",
    "- Matrix inversion: $O(n^3)$\n",
    "- Where n = number of features, m = number of samples\n",
    "- **Bottleneck: n**\n",
    "\n",
    "**Gradient Descent (one iteration):**\n",
    "- Matrix multiplication $X\\theta$: $O(mn)$\n",
    "- Gradient $X^T \\cdot errors$: $O(mn)$\n",
    "- Total per iteration: $O(mn)$\n",
    "- Total for k iterations: $O(kmn)$\n",
    "- **Bottleneck: number of iterations**\n",
    "\n",
    "**Comparison:**\n",
    "- When n is small: Normal Equation wins (one-shot)\n",
    "- When n is large (>10,000): GD wins ($mn$ vs $n^3$)\n",
    "- When m is very large: GD can use mini-batches\n",
    "\n",
    "**Memory:**\n",
    "- Normal Equation: Must store $X^TX$ ($n \\times n$ matrix)\n",
    "- GD: Only needs current batch in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9: What are learning rate schedules? Name a few.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Learning rate schedules **reduce the learning rate during training** to improve convergence.\n",
    "\n",
    "**Why?** Start with large steps (fast progress), end with small steps (precision).\n",
    "\n",
    "**Common schedules:**\n",
    "\n",
    "1. **Step Decay:**\n",
    "   - Reduce by factor every N epochs\n",
    "   - $\\alpha_t = \\alpha_0 \\cdot \\gamma^{\\lfloor t/N \\rfloor}$\n",
    "\n",
    "2. **Exponential Decay:**\n",
    "   - $\\alpha_t = \\alpha_0 \\cdot e^{-kt}$\n",
    "\n",
    "3. **1/t Decay:**\n",
    "   - $\\alpha_t = \\frac{\\alpha_0}{1 + kt}$\n",
    "\n",
    "4. **Cosine Annealing:**\n",
    "   - $\\alpha_t = \\alpha_{min} + \\frac{1}{2}(\\alpha_{max} - \\alpha_{min})(1 + \\cos(\\frac{t\\pi}{T}))$\n",
    "\n",
    "**Adaptive optimizers** (automatically adjust learning rate):\n",
    "- **AdaGrad**: Adapts per-parameter based on historical gradients\n",
    "- **RMSProp**: Uses exponential moving average of squared gradients\n",
    "- **Adam**: Combines momentum + RMSProp (most popular)\n",
    "\n",
    "**For linear regression:** Usually a fixed learning rate with feature scaling is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10: Implement Gradient Descent from scratch in a coding interview.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegressionGD:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iter = n_iterations\n",
    "        self.theta = None\n",
    "        self.cost_history = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        X_b = np.c_[np.ones((m, 1)), X]   # add intercept\n",
    "        self.theta = np.zeros((n + 1, 1))  # init to zeros\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "        for _ in range(self.n_iter):\n",
    "            predictions = X_b @ self.theta\n",
    "            errors = predictions - y\n",
    "            gradient = (1/m) * X_b.T @ errors\n",
    "            self.theta -= self.lr * gradient\n",
    "            cost = (1/(2*m)) * np.sum(errors**2)\n",
    "            self.cost_history.append(cost)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        m = X.shape[0]\n",
    "        X_b = np.c_[np.ones((m, 1)), X]\n",
    "        return (X_b @ self.theta).flatten()\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred)**2)\n",
    "        ss_tot = np.sum((y - y.mean())**2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "```\n",
    "\n",
    "**Key points to mention:**\n",
    "- Feature scaling before training\n",
    "- Initialize weights to zeros\n",
    "- Vectorized operations (no loops over features)\n",
    "- Track cost history for debugging\n",
    "- Could add early stopping for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q11: What is momentum in Gradient Descent?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Momentum** accelerates gradient descent by adding a fraction of the previous update to the current one.\n",
    "\n",
    "**Standard GD:**\n",
    "$$\\theta := \\theta - \\alpha \\nabla J(\\theta)$$\n",
    "\n",
    "**GD with Momentum:**\n",
    "$$v_t = \\beta v_{t-1} + \\alpha \\nabla J(\\theta)$$\n",
    "$$\\theta := \\theta - v_t$$\n",
    "\n",
    "Where $\\beta$ is the momentum coefficient (typically 0.9).\n",
    "\n",
    "**Analogy:** A ball rolling downhill accumulates velocity. Momentum lets the update accumulate speed in consistent directions.\n",
    "\n",
    "**Benefits:**\n",
    "- Faster convergence (especially in narrow valleys)\n",
    "- Dampens oscillations across steep dimensions\n",
    "- Helps escape shallow local minima\n",
    "- Reduces zigzagging with SGD\n",
    "\n",
    "**Used in:** SGD with momentum, Adam optimizer, Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q12: What is the difference between Gradient Descent and Stochastic Gradient Descent?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "| Aspect | Batch GD | SGD |\n",
    "|--------|----------|-----|\n",
    "| **Samples per update** | All m samples | 1 sample |\n",
    "| **Gradient quality** | Exact | Noisy estimate |\n",
    "| **Convergence path** | Smooth | Noisy / Zigzag |\n",
    "| **Speed per epoch** | Slow | Fast |\n",
    "| **Memory** | High | Low |\n",
    "| **Online learning** | No | Yes |\n",
    "| **Local minima** | Can get stuck | Noise helps escape |\n",
    "\n",
    "**SGD advantages:**\n",
    "- Much faster per iteration\n",
    "- Can handle datasets that don't fit in memory\n",
    "- Noise acts as implicit regularization\n",
    "- Can update model as new data arrives\n",
    "\n",
    "**SGD challenges:**\n",
    "- Noisy updates (may not converge smoothly)\n",
    "- Need learning rate schedule for convergence\n",
    "- Final solution oscillates around minimum\n",
    "\n",
    "**In practice:** Mini-batch GD (batch_size = 32-256) is most common. It balances the benefits of both approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q13: Can Gradient Descent get stuck in local minima for Linear Regression?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**No**, not for linear regression.\n",
    "\n",
    "**Reason:** The MSE cost function for linear regression is **convex** (bowl-shaped). A convex function has exactly one minimum, which is the global minimum. There are no local minima to get stuck in.\n",
    "\n",
    "**Proof:** The Hessian matrix (second derivative) of the MSE cost function is:\n",
    "$$H = \\frac{2}{m}X^TX$$\n",
    "\n",
    "$X^TX$ is always positive semi-definite, which means the cost function is convex.\n",
    "\n",
    "**However, GD can still fail to converge if:**\n",
    "- Learning rate is too large (diverges)\n",
    "- Not enough iterations\n",
    "- Numerical issues (overflow/underflow)\n",
    "\n",
    "**Note:** For neural networks and other non-linear models, the cost function IS non-convex, and getting stuck in local minima (or saddle points) is a real concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q14: What is the vanishing/exploding gradient problem? Is it relevant to Linear Regression?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Vanishing Gradient:**\n",
    "- Gradients become extremely small during backpropagation\n",
    "- Parameters barely update\n",
    "- Training stalls\n",
    "\n",
    "**Exploding Gradient:**\n",
    "- Gradients become extremely large\n",
    "- Parameters update too aggressively\n",
    "- Numerical overflow, NaN values\n",
    "\n",
    "**Relevant to Linear Regression?**\n",
    "\n",
    "**Vanishing gradients: No.** Linear regression has no activation functions or deep layers that cause gradients to diminish.\n",
    "\n",
    "**Exploding gradients: Partially.** Can happen if:\n",
    "- Features are not scaled (very large values)\n",
    "- Learning rate is too high\n",
    "\n",
    "**Solution for linear regression:**\n",
    "- Feature scaling (standardization)\n",
    "- Appropriate learning rate\n",
    "- Gradient clipping (cap gradient magnitude)\n",
    "\n",
    "**This problem is mainly a deep learning concern** (deep neural networks with many layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q15: Explain the Adam optimizer. Why is it so popular?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Adam (Adaptive Moment Estimation)** combines two ideas:\n",
    "1. **Momentum** (first moment: running average of gradients)\n",
    "2. **RMSProp** (second moment: running average of squared gradients)\n",
    "\n",
    "**Algorithm:**\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1)g_t$$\n",
    "$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2$$\n",
    "$$\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$$\n",
    "$$\\theta := \\theta - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$$\n",
    "\n",
    "**Default hyperparameters:** $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon = 10^{-8}$\n",
    "\n",
    "**Why popular:**\n",
    "- Adaptive per-parameter learning rates\n",
    "- Works well out-of-the-box with default settings\n",
    "- Combines benefits of momentum and RMSProp\n",
    "- Handles sparse gradients well\n",
    "- Bias correction for initial iterations\n",
    "- Fast convergence\n",
    "\n",
    "**For linear regression:** Adam is overkill. Simple batch GD works fine. Adam shines in deep learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
