{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression & Normal Equation - Interview Q&A\n",
    "\n",
    "This notebook contains interview-level questions and answers about Linear Regression and the Normal Equation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: What is Linear Regression? When would you use it?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Linear Regression is a supervised learning algorithm used to predict a continuous target variable based on one or more independent variables by fitting a linear equation to the observed data.\n",
    "\n",
    "**Mathematical form:**\n",
    "- Simple: $y = \\beta_0 + \\beta_1x + \\varepsilon$\n",
    "- Multiple: $y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\varepsilon$\n",
    "\n",
    "**Use cases:**\n",
    "- Predicting house prices based on features\n",
    "- Forecasting sales based on advertising spend\n",
    "- Estimating continuous outcomes (temperature, stock prices, etc.)\n",
    "- When relationship between variables is approximately linear\n",
    "\n",
    "**Key requirement:** The target variable must be continuous (not categorical)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: What are the four key assumptions of Linear Regression?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "1. **Linearity**: The relationship between independent and dependent variables is linear\n",
    "   - Check: Scatter plots, residual plots\n",
    "\n",
    "2. **Independence**: Observations are independent of each other\n",
    "   - Violation example: Time series data with autocorrelation\n",
    "   - Check: Durbin-Watson test\n",
    "\n",
    "3. **Homoscedasticity**: Constant variance of residuals across all levels of independent variables\n",
    "   - Violation: Residuals increase/decrease with fitted values (heteroscedasticity)\n",
    "   - Check: Residual plot (should show random scatter)\n",
    "\n",
    "4. **Normality**: Residuals are normally distributed\n",
    "   - Check: Q-Q plot, histogram of residuals, Shapiro-Wilk test\n",
    "\n",
    "**Consequences of violations:**\n",
    "- Biased or inefficient parameter estimates\n",
    "- Invalid confidence intervals and p-values\n",
    "- Poor prediction performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Explain the difference between Simple and Multiple Linear Regression.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Simple Linear Regression:**\n",
    "- One independent variable\n",
    "- Form: $y = \\beta_0 + \\beta_1x$\n",
    "- Example: Predicting salary based only on years of experience\n",
    "- Visualization: 2D line\n",
    "\n",
    "**Multiple Linear Regression:**\n",
    "- Two or more independent variables\n",
    "- Form: $y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n$\n",
    "- Example: Predicting salary based on experience, education, location\n",
    "- Visualization: Hyperplane in n-dimensional space\n",
    "\n",
    "**Key differences:**\n",
    "- Multiple regression can capture more complex relationships\n",
    "- Multiple regression is prone to multicollinearity\n",
    "- Interpretation: In multiple regression, each coefficient represents the effect of that variable while holding others constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: What is the Normal Equation? Derive or explain the formula.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The Normal Equation is a closed-form analytical solution to find optimal parameters for linear regression without iteration.\n",
    "\n",
    "**Formula:**\n",
    "$$\\theta = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "**Derivation (high-level):**\n",
    "\n",
    "1. Start with cost function (MSE):\n",
    "   $$J(\\theta) = \\frac{1}{2m}(X\\theta - y)^T(X\\theta - y)$$\n",
    "\n",
    "2. To minimize, take derivative with respect to $\\theta$ and set to zero:\n",
    "   $$\\frac{\\partial J}{\\partial \\theta} = 0$$\n",
    "\n",
    "3. Expand and solve:\n",
    "   $$\\frac{1}{m}X^T(X\\theta - y) = 0$$\n",
    "   $$X^TX\\theta = X^Ty$$\n",
    "   $$\\theta = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "**Key insight:** This gives the exact optimal parameters in one computation (no iterations needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: What are the advantages and disadvantages of the Normal Equation?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Advantages:**\n",
    "- ✅ **No hyperparameters** - No learning rate to tune\n",
    "- ✅ **Exact solution** - Finds global optimum in one step\n",
    "- ✅ **No iterations** - Single computation\n",
    "- ✅ **No feature scaling needed** - Works with unnormalized data\n",
    "- ✅ **Guaranteed convergence** - Always finds solution (if invertible)\n",
    "\n",
    "**Disadvantages:**\n",
    "- ❌ **Computational complexity** - O(n³) due to matrix inversion\n",
    "- ❌ **Slow for large n** - Impractical when features > 10,000\n",
    "- ❌ **Memory intensive** - Must compute and store $X^TX$\n",
    "- ❌ **Matrix inversion required** - Fails if $X^TX$ is singular (non-invertible)\n",
    "- ❌ **Not suitable for online learning** - Requires all data at once\n",
    "\n",
    "**Rule of thumb:** Use Normal Equation when n < 10,000 features; use Gradient Descent otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6: When is $(X^TX)$ not invertible? What can you do about it?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**When $X^TX$ is singular (non-invertible):**\n",
    "\n",
    "1. **Redundant features**: Linearly dependent columns\n",
    "   - Example: Feature 1 = Temperature in Celsius, Feature 2 = Temperature in Fahrenheit\n",
    "\n",
    "2. **More features than samples**: $n > m$\n",
    "   - System is underdetermined\n",
    "\n",
    "3. **Perfect multicollinearity**: One feature is exact linear combination of others\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Remove redundant features**\n",
    "   - Identify and eliminate linearly dependent features\n",
    "   - Use correlation matrix or VIF (Variance Inflation Factor)\n",
    "\n",
    "2. **Use pseudo-inverse (Moore-Penrose)**\n",
    "   ```python\n",
    "   theta = np.linalg.pinv(X) @ y\n",
    "   ```\n",
    "   - Works even when $X^TX$ is singular\n",
    "   - More numerically stable\n",
    "\n",
    "3. **Regularization** (Ridge/Lasso)\n",
    "   - Add penalty term to make matrix invertible\n",
    "   - Ridge: $(X^TX + \\lambda I)^{-1}X^Ty$\n",
    "\n",
    "4. **Use Gradient Descent** instead\n",
    "   - Doesn't require matrix inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7: Compare Normal Equation vs Gradient Descent. When would you use each?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "| Aspect | Normal Equation | Gradient Descent |\n",
    "|--------|----------------|------------------|\n",
    "| **Algorithm type** | Analytical/Direct | Iterative |\n",
    "| **Iterations** | None (1 computation) | Many (100s-1000s) |\n",
    "| **Learning rate** | Not needed | Required |\n",
    "| **Feature scaling** | Not needed | Highly recommended |\n",
    "| **Time complexity** | O(n³) | O(kmn), k=iterations |\n",
    "| **Works for large n** | No (n > 10,000) | Yes |\n",
    "| **Works for large m** | Yes | Yes |\n",
    "| **Singularity issue** | Yes (if $X^TX$ singular) | No |\n",
    "| **Online learning** | No | Yes |\n",
    "| **Optimization** | Global optimum | Global optimum* |\n",
    "\n",
    "*Linear regression cost function is convex, so GD always finds global optimum\n",
    "\n",
    "**Use Normal Equation when:**\n",
    "- Small to medium datasets (m < 100,000)\n",
    "- Few features (n < 10,000)\n",
    "- Need exact solution quickly\n",
    "- Don't want to tune hyperparameters\n",
    "\n",
    "**Use Gradient Descent when:**\n",
    "- Large datasets (m > 100,000)\n",
    "- Many features (n > 10,000)\n",
    "- Online learning required\n",
    "- Memory constrained\n",
    "- Want to generalize to other algorithms (logistic regression, neural networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation & Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8: What is R² score? How do you interpret it?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**R² (Coefficient of Determination)** measures the proportion of variance in the dependent variable explained by the independent variables.\n",
    "\n",
    "**Formula:**\n",
    "$$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}$$\n",
    "\n",
    "Where:\n",
    "- $SS_{res}$ = Residual sum of squares (model error)\n",
    "- $SS_{tot}$ = Total sum of squares (variance in data)\n",
    "\n",
    "**Interpretation:**\n",
    "- **Range**: 0 to 1 (can be negative for very poor models)\n",
    "- **R² = 0.85**: Model explains 85% of variance in target\n",
    "- **R² = 1.0**: Perfect fit (likely overfitting)\n",
    "- **R² = 0.0**: Model no better than predicting mean\n",
    "- **R² < 0**: Model worse than predicting mean\n",
    "\n",
    "**Limitations:**\n",
    "- Always increases with more features (even irrelevant ones)\n",
    "- Use **Adjusted R²** for multiple regression\n",
    "- High R² doesn't mean causation\n",
    "- Can be misleading with non-linear relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9: What is multicollinearity? How do you detect and handle it?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Multicollinearity**: High correlation between two or more independent variables.\n",
    "\n",
    "**Problems it causes:**\n",
    "- Unstable coefficient estimates (small data changes → large coefficient changes)\n",
    "- Difficult to interpret individual feature importance\n",
    "- Inflated standard errors\n",
    "- May lead to singular $X^TX$ matrix\n",
    "\n",
    "**Detection methods:**\n",
    "\n",
    "1. **Correlation Matrix**\n",
    "   - Check pairwise correlations\n",
    "   - |correlation| > 0.8-0.9 indicates problem\n",
    "\n",
    "2. **Variance Inflation Factor (VIF)**\n",
    "   $$VIF_j = \\frac{1}{1 - R_j^2}$$\n",
    "   - $R_j^2$ = R² from regressing feature j on all other features\n",
    "   - **VIF > 5-10**: Problematic multicollinearity\n",
    "   - **VIF = 1**: No multicollinearity\n",
    "\n",
    "**Solutions:**\n",
    "1. **Remove correlated features** - Drop one of the correlated variables\n",
    "2. **Combine features** - Create interaction term or ratio\n",
    "3. **Principal Component Analysis (PCA)** - Transform to uncorrelated components\n",
    "4. **Regularization** - Ridge regression (L2) handles multicollinearity well\n",
    "5. **Collect more data** - Sometimes helps reduce correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10: What is the difference between MSE, RMSE, and MAE? When would you use each?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Mean Squared Error (MSE):**\n",
    "$$MSE = \\frac{1}{m}\\sum_{i=1}^{m}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "- Squares errors (penalizes large errors more)\n",
    "- Units: squared units of target variable\n",
    "- Not interpretable in original units\n",
    "- Sensitive to outliers\n",
    "\n",
    "**Root Mean Squared Error (RMSE):**\n",
    "$$RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "- Square root of MSE\n",
    "- **Units: same as target variable** (interpretable!)\n",
    "- Still sensitive to outliers\n",
    "- Most commonly used\n",
    "\n",
    "**Mean Absolute Error (MAE):**\n",
    "$$MAE = \\frac{1}{m}\\sum_{i=1}^{m}|y_i - \\hat{y}_i|$$\n",
    "\n",
    "- Average absolute error\n",
    "- Units: same as target variable\n",
    "- **Less sensitive to outliers**\n",
    "- All errors weighted equally\n",
    "\n",
    "**When to use:**\n",
    "- **RMSE**: General purpose, when large errors should be penalized more\n",
    "- **MAE**: When outliers shouldn't dominate, need robust metric\n",
    "- **MSE**: Theoretical work, optimization (easier to differentiate)\n",
    "\n",
    "**Example:**\n",
    "- Errors: [1, 1, 1, 10]\n",
    "- MAE = 3.25\n",
    "- RMSE = 5.22 (heavily influenced by the 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q11: What is the difference between population regression line and sample regression line?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Population Regression Line:**\n",
    "- True relationship in the entire population\n",
    "- Unknown in practice (we don't have access to entire population)\n",
    "- Represented by true parameters: $\\beta_0, \\beta_1, ..., \\beta_n$\n",
    "- Example: True relationship between all people's education and income\n",
    "\n",
    "**Sample Regression Line:**\n",
    "- Estimated from a sample of data\n",
    "- What we actually compute in practice\n",
    "- Represented by estimated parameters: $\\hat{\\beta}_0, \\hat{\\beta}_1, ..., \\hat{\\beta}_n$ (or $\\theta$)\n",
    "- **Goal**: Estimate the population line as accurately as possible\n",
    "- Different samples → slightly different sample regression lines\n",
    "\n",
    "**Key relationship:**\n",
    "- Sample line is an **estimator** of population line\n",
    "- As sample size increases, sample line approaches population line\n",
    "- Standard errors quantify uncertainty in our estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q12: How would you handle outliers in Linear Regression?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Detection:**\n",
    "1. **Residual analysis** - Large residuals indicate outliers\n",
    "2. **Cook's distance** - Measures influence of each point\n",
    "3. **Leverage** - Points far from mean of X\n",
    "4. **Visualization** - Scatter plots, box plots\n",
    "\n",
    "**Handling strategies:**\n",
    "\n",
    "1. **Remove outliers** (if justified)\n",
    "   - Data entry errors\n",
    "   - Measurement errors\n",
    "   - Document removal decision\n",
    "\n",
    "2. **Transform variables**\n",
    "   - Log transformation reduces impact\n",
    "   - Box-Cox transformation\n",
    "\n",
    "3. **Use robust regression**\n",
    "   - RANSAC (Random Sample Consensus)\n",
    "   - Huber regression\n",
    "   - Minimize MAE instead of MSE\n",
    "\n",
    "4. **Cap/Winsorize**\n",
    "   - Replace extreme values with percentile values\n",
    "   - Example: Cap at 95th percentile\n",
    "\n",
    "5. **Add indicator variable**\n",
    "   - Binary variable indicating outlier status\n",
    "   - Allows model to treat differently\n",
    "\n",
    "**Important**: Don't automatically remove outliers - they might contain valuable information!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q13: Explain overfitting and underfitting in Linear Regression context.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Underfitting (High Bias):**\n",
    "- Model is too simple\n",
    "- Poor performance on both training and test data\n",
    "- High training error, high test error\n",
    "\n",
    "**Causes:**\n",
    "- Too few features\n",
    "- Model doesn't capture relationship (maybe non-linear)\n",
    "- Too much regularization\n",
    "\n",
    "**Solutions:**\n",
    "- Add polynomial features\n",
    "- Add more relevant features\n",
    "- Reduce regularization\n",
    "- Try more complex model\n",
    "\n",
    "**Overfitting (High Variance):**\n",
    "- Model is too complex\n",
    "- Excellent performance on training, poor on test\n",
    "- Low training error, high test error\n",
    "- Model learns noise in training data\n",
    "\n",
    "**Causes:**\n",
    "- Too many features\n",
    "- Polynomial features with high degree\n",
    "- Too little regularization\n",
    "- Small training set\n",
    "\n",
    "**Solutions:**\n",
    "- Add more training data\n",
    "- Feature selection (remove irrelevant features)\n",
    "- Regularization (Ridge/Lasso)\n",
    "- Cross-validation\n",
    "- Reduce model complexity\n",
    "\n",
    "**Bias-Variance Tradeoff:**\n",
    "- Need to balance model complexity\n",
    "- Total Error = Bias² + Variance + Irreducible Error\n",
    "- Goal: Find sweet spot (minimum total error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q14: What is regularization? Explain Ridge and Lasso regression.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Regularization**: Technique to prevent overfitting by adding a penalty term to the cost function.\n",
    "\n",
    "**Ridge Regression (L2 Regularization):**\n",
    "$$J(\\theta) = MSE + \\lambda\\sum_{j=1}^{n}\\theta_j^2$$\n",
    "\n",
    "- Penalty: Sum of squared coefficients\n",
    "- **Effect**: Shrinks coefficients toward zero (but never exactly zero)\n",
    "- Keeps all features\n",
    "- Works well with multicollinearity\n",
    "- Normal equation: $\\theta = (X^TX + \\lambda I)^{-1}X^Ty$\n",
    "- Note: $\\lambda I$ makes matrix invertible even if $X^TX$ is singular!\n",
    "\n",
    "**Lasso Regression (L1 Regularization):**\n",
    "$$J(\\theta) = MSE + \\lambda\\sum_{j=1}^{n}|\\theta_j|$$\n",
    "\n",
    "- Penalty: Sum of absolute coefficients\n",
    "- **Effect**: Can shrink coefficients to exactly zero\n",
    "- Performs automatic feature selection\n",
    "- Creates sparse models\n",
    "- No closed-form solution (use gradient descent)\n",
    "\n",
    "**Elastic Net:**\n",
    "$$J(\\theta) = MSE + r\\lambda\\sum|\\theta_j| + \\frac{(1-r)}{2}\\lambda\\sum\\theta_j^2$$\n",
    "\n",
    "- Combines L1 and L2\n",
    "- Best of both worlds\n",
    "\n",
    "**Choosing λ:**\n",
    "- Use cross-validation\n",
    "- Higher λ → more regularization → simpler model\n",
    "- λ = 0 → standard linear regression\n",
    "\n",
    "**When to use:**\n",
    "- **Ridge**: Many features, multicollinearity, want to keep all features\n",
    "- **Lasso**: Want feature selection, sparse model\n",
    "- **Elastic Net**: Many correlated features, want feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q15: How would you implement Linear Regression from scratch in a coding interview?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Here's a concise implementation using Normal Equation:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train using Normal Equation.\"\"\"\n",
    "        # Add intercept term\n",
    "        m = X.shape[0]\n",
    "        X_b = np.c_[np.ones((m, 1)), X]\n",
    "        \n",
    "        # Normal equation: θ = (X^T X)^-1 X^T y\n",
    "        self.theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        m = X.shape[0]\n",
    "        X_b = np.c_[np.ones((m, 1)), X]\n",
    "        return X_b @ self.theta\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Calculate R² score.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        ss_tot = np.sum((y - y.mean()) ** 2)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Usage\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "r2 = model.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "**Key points to mention:**\n",
    "- Add bias/intercept term (column of ones)\n",
    "- Use vectorized operations\n",
    "- Could use `np.linalg.pinv()` for stability\n",
    "- Should handle edge cases (empty data, singular matrix)\n",
    "- Could add input validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
