{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Regression & Normal Equation - Interview Q&A\n",
        "\n",
        "This notebook contains interview-level questions and answers about Linear Regression and the Normal Equation method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q1: What is Linear Regression? When would you use it?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Linear Regression is a supervised learning algorithm used to predict a continuous target variable based on one or more independent variables by fitting a linear equation to the observed data.\n",
        "\n",
        "**Mathematical form:**\n",
        "- Simple: $y = \\beta_0 + \\beta_1x + \\varepsilon$\n",
        "- Multiple: $y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\varepsilon$\n",
        "\n",
        "**Use cases:**\n",
        "- Predicting house prices based on features\n",
        "- Forecasting sales based on advertising spend\n",
        "- Estimating continuous outcomes (temperature, stock prices, etc.)\n",
        "- When relationship between variables is approximately linear\n",
        "\n",
        "**Key requirement:** The target variable must be continuous (not categorical)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2: What are the four key assumptions of Linear Regression?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "1. **Linearity**: The relationship between independent and dependent variables is linear\n",
        "   - Check: Scatter plots, residual plots\n",
        "\n",
        "2. **Independence**: Observations are independent of each other\n",
        "   - Violation example: Time series data with autocorrelation\n",
        "   - Check: Durbin-Watson test\n",
        "\n",
        "3. **Homoscedasticity**: Constant variance of residuals across all levels of independent variables\n",
        "   - Violation: Residuals increase/decrease with fitted values (heteroscedasticity)\n",
        "   - Check: Residual plot (should show random scatter)\n",
        "\n",
        "4. **Normality**: Residuals are normally distributed\n",
        "   - Check: Q-Q plot, histogram of residuals, Shapiro-Wilk test\n",
        "\n",
        "**Consequences of violations:**\n",
        "- Biased or inefficient parameter estimates\n",
        "- Invalid confidence intervals and p-values\n",
        "- Poor prediction performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q3: Explain the difference between Simple and Multiple Linear Regression.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Simple Linear Regression:**\n",
        "- One independent variable\n",
        "- Form: $y = \\beta_0 + \\beta_1x$\n",
        "- Example: Predicting salary based only on years of experience\n",
        "- Visualization: 2D line\n",
        "\n",
        "**Multiple Linear Regression:**\n",
        "- Two or more independent variables\n",
        "- Form: $y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n$\n",
        "- Example: Predicting salary based on experience, education, location\n",
        "- Visualization: Hyperplane in n-dimensional space\n",
        "\n",
        "**Key differences:**\n",
        "- Multiple regression can capture more complex relationships\n",
        "- Multiple regression is prone to multicollinearity\n",
        "- Interpretation: In multiple regression, each coefficient represents the effect of that variable while holding others constant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normal Equation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q4: What is the Normal Equation? Derive or explain the formula.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The Normal Equation is a closed-form analytical solution to find optimal parameters for linear regression without iteration.\n",
        "\n",
        "**Formula:**\n",
        "$$\\theta = (X^TX)^{-1}X^Ty$$\n",
        "\n",
        "**Derivation (high-level):**\n",
        "\n",
        "1. Start with cost function (MSE):\n",
        "   $$J(\\theta) = \\frac{1}{2m}(X\\theta - y)^T(X\\theta - y)$$\n",
        "\n",
        "2. To minimize, take derivative with respect to $\\theta$ and set to zero:\n",
        "   $$\\frac{\\partial J}{\\partial \\theta} = 0$$\n",
        "\n",
        "3. Expand and solve:\n",
        "   $$\\frac{1}{m}X^T(X\\theta - y) = 0$$\n",
        "   $$X^TX\\theta = X^Ty$$\n",
        "   $$\\theta = (X^TX)^{-1}X^Ty$$\n",
        "\n",
        "**Key insight:** This gives the exact optimal parameters in one computation (no iterations needed)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q5: What are the advantages and disadvantages of the Normal Equation?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Advantages:**\n",
        "- ✅ **No hyperparameters** - No learning rate to tune\n",
        "- ✅ **Exact solution** - Finds global optimum in one step\n",
        "- ✅ **No iterations** - Single computation\n",
        "- ✅ **No feature scaling needed** - Works with unnormalized data\n",
        "- ✅ **Guaranteed convergence** - Always finds solution (if invertible)\n",
        "\n",
        "**Disadvantages:**\n",
        "- ❌ **Computational complexity** - O(n³) due to matrix inversion\n",
        "- ❌ **Slow for large n** - Impractical when features > 10,000\n",
        "- ❌ **Memory intensive** - Must compute and store $X^TX$\n",
        "- ❌ **Matrix inversion required** - Fails if $X^TX$ is singular (non-invertible)\n",
        "- ❌ **Not suitable for online learning** - Requires all data at once\n",
        "\n",
        "**Rule of thumb:** Use Normal Equation when n < 10,000 features; use Gradient Descent otherwise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q6: When is $(X^TX)$ not invertible? What can you do about it?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**When $X^TX$ is singular (non-invertible):**\n",
        "\n",
        "1. **Redundant features**: Linearly dependent columns\n",
        "   - Example: Feature 1 = Temperature in Celsius, Feature 2 = Temperature in Fahrenheit\n",
        "\n",
        "2. **More features than samples**: $n > m$\n",
        "   - System is underdetermined\n",
        "\n",
        "3. **Perfect multicollinearity**: One feature is exact linear combination of others\n",
        "\n",
        "**Solutions:**\n",
        "\n",
        "1. **Remove redundant features**\n",
        "   - Identify and eliminate linearly dependent features\n",
        "   - Use correlation matrix or VIF (Variance Inflation Factor)\n",
        "\n",
        "2. **Use pseudo-inverse (Moore-Penrose)**\n",
        "   ```python\n",
        "   theta = np.linalg.pinv(X) @ y\n",
        "   ```\n",
        "   - Works even when $X^TX$ is singular\n",
        "   - More numerically stable\n",
        "\n",
        "3. **Regularization** (Ridge/Lasso)\n",
        "   - Add penalty term to make matrix invertible\n",
        "   - Ridge: $(X^TX + \\lambda I)^{-1}X^Ty$\n",
        "\n",
        "4. **Use Gradient Descent** instead\n",
        "   - Doesn't require matrix inversion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q7: Compare Normal Equation vs Gradient Descent. When would you use each?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "| Aspect | Normal Equation | Gradient Descent |\n",
        "|--------|----------------|------------------|\n",
        "| **Algorithm type** | Analytical/Direct | Iterative |\n",
        "| **Iterations** | None (1 computation) | Many (100s-1000s) |\n",
        "| **Learning rate** | Not needed | Required |\n",
        "| **Feature scaling** | Not needed | Highly recommended |\n",
        "| **Time complexity** | O(n³) | O(kmn), k=iterations |\n",
        "| **Works for large n** | No (n > 10,000) | Yes |\n",
        "| **Works for large m** | Yes | Yes |\n",
        "| **Singularity issue** | Yes (if $X^TX$ singular) | No |\n",
        "| **Online learning** | No | Yes |\n",
        "| **Optimization** | Global optimum | Global optimum* |\n",
        "\n",
        "*Linear regression cost function is convex, so GD always finds global optimum\n",
        "\n",
        "**Use Normal Equation when:**\n",
        "- Small to medium datasets (m < 100,000)\n",
        "- Few features (n < 10,000)\n",
        "- Need exact solution quickly\n",
        "- Don't want to tune hyperparameters\n",
        "\n",
        "**Use Gradient Descent when:**\n",
        "- Large datasets (m > 100,000)\n",
        "- Many features (n > 10,000)\n",
        "- Online learning required\n",
        "- Memory constrained\n",
        "- Want to generalize to other algorithms (logistic regression, neural networks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation & Interpretation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q8: What is R² score? How do you interpret it?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**R² (Coefficient of Determination)** measures the proportion of variance in the dependent variable explained by the independent variables.\n",
        "\n",
        "**Formula:**\n",
        "$$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}$$\n",
        "\n",
        "Where:\n",
        "- $SS_{res}$ = Residual sum of squares (model error)\n",
        "- $SS_{tot}$ = Total sum of squares (variance in data)\n",
        "\n",
        "**Interpretation:**\n",
        "- **Range**: 0 to 1 (can be negative for very poor models)\n",
        "- **R² = 0.85**: Model explains 85% of variance in target\n",
        "- **R² = 1.0**: Perfect fit (likely overfitting)\n",
        "- **R² = 0.0**: Model no better than predicting mean\n",
        "- **R² < 0**: Model worse than predicting mean\n",
        "\n",
        "**Limitations:**\n",
        "- Always increases with more features (even irrelevant ones)\n",
        "- Use **Adjusted R²** for multiple regression\n",
        "- High R² doesn't mean causation\n",
        "- Can be misleading with non-linear relationships"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q9: What is multicollinearity? How do you detect and handle it?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Multicollinearity**: High correlation between two or more independent variables.\n",
        "\n",
        "**Problems it causes:**\n",
        "- Unstable coefficient estimates (small data changes → large coefficient changes)\n",
        "- Difficult to interpret individual feature importance\n",
        "- Inflated standard errors\n",
        "- May lead to singular $X^TX$ matrix\n",
        "\n",
        "**Detection methods:**\n",
        "\n",
        "1. **Correlation Matrix**\n",
        "   - Check pairwise correlations\n",
        "   - |correlation| > 0.8-0.9 indicates problem\n",
        "\n",
        "2. **Variance Inflation Factor (VIF)**\n",
        "   $$VIF_j = \\frac{1}{1 - R_j^2}$$\n",
        "   - $R_j^2$ = R² from regressing feature j on all other features\n",
        "   - **VIF > 5-10**: Problematic multicollinearity\n",
        "   - **VIF = 1**: No multicollinearity\n",
        "\n",
        "**Solutions:**\n",
        "1. **Remove correlated features** - Drop one of the correlated variables\n",
        "2. **Combine features** - Create interaction term or ratio\n",
        "3. **Principal Component Analysis (PCA)** - Transform to uncorrelated components\n",
        "4. **Regularization** - Ridge regression (L2) handles multicollinearity well\n",
        "5. **Collect more data** - Sometimes helps reduce correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q10: What is the difference between MSE, RMSE, and MAE? When would you use each?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Mean Squared Error (MSE):**\n",
        "$$MSE = \\frac{1}{m}\\sum_{i=1}^{m}(y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "- Squares errors (penalizes large errors more)\n",
        "- Units: squared units of target variable\n",
        "- Not interpretable in original units\n",
        "- Sensitive to outliers\n",
        "\n",
        "**Root Mean Squared Error (RMSE):**\n",
        "$$RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(y_i - \\hat{y}_i)^2}$$\n",
        "\n",
        "- Square root of MSE\n",
        "- **Units: same as target variable** (interpretable!)\n",
        "- Still sensitive to outliers\n",
        "- Most commonly used\n",
        "\n",
        "**Mean Absolute Error (MAE):**\n",
        "$$MAE = \\frac{1}{m}\\sum_{i=1}^{m}|y_i - \\hat{y}_i|$$\n",
        "\n",
        "- Average absolute error\n",
        "- Units: same as target variable\n",
        "- **Less sensitive to outliers**\n",
        "- All errors weighted equally\n",
        "\n",
        "**When to use:**\n",
        "- **RMSE**: General purpose, when large errors should be penalized more\n",
        "- **MAE**: When outliers shouldn't dominate, need robust metric\n",
        "- **MSE**: Theoretical work, optimization (easier to differentiate)\n",
        "\n",
        "**Example:**\n",
        "- Errors: [1, 1, 1, 10]\n",
        "- MAE = 3.25\n",
        "- RMSE = 5.22 (heavily influenced by the 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q11: What is the difference between population regression line and sample regression line?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Population Regression Line:**\n",
        "- True relationship in the entire population\n",
        "- Unknown in practice (we don't have access to entire population)\n",
        "- Represented by true parameters: $\\beta_0, \\beta_1, ..., \\beta_n$\n",
        "- Example: True relationship between all people's education and income\n",
        "\n",
        "**Sample Regression Line:**\n",
        "- Estimated from a sample of data\n",
        "- What we actually compute in practice\n",
        "- Represented by estimated parameters: $\\hat{\\beta}_0, \\hat{\\beta}_1, ..., \\hat{\\beta}_n$ (or $\\theta$)\n",
        "- **Goal**: Estimate the population line as accurately as possible\n",
        "- Different samples → slightly different sample regression lines\n",
        "\n",
        "**Key relationship:**\n",
        "- Sample line is an **estimator** of population line\n",
        "- As sample size increases, sample line approaches population line\n",
        "- Standard errors quantify uncertainty in our estimates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q12: How would you handle outliers in Linear Regression?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Detection:**\n",
        "1. **Residual analysis** - Large residuals indicate outliers\n",
        "2. **Cook's distance** - Measures influence of each point\n",
        "3. **Leverage** - Points far from mean of X\n",
        "4. **Visualization** - Scatter plots, box plots\n",
        "\n",
        "**Handling strategies:**\n",
        "\n",
        "1. **Remove outliers** (if justified)\n",
        "   - Data entry errors\n",
        "   - Measurement errors\n",
        "   - Document removal decision\n",
        "\n",
        "2. **Transform variables**\n",
        "   - Log transformation reduces impact\n",
        "   - Box-Cox transformation\n",
        "\n",
        "3. **Use robust regression**\n",
        "   - RANSAC (Random Sample Consensus)\n",
        "   - Huber regression\n",
        "   - Minimize MAE instead of MSE\n",
        "\n",
        "4. **Cap/Winsorize**\n",
        "   - Replace extreme values with percentile values\n",
        "   - Example: Cap at 95th percentile\n",
        "\n",
        "5. **Add indicator variable**\n",
        "   - Binary variable indicating outlier status\n",
        "   - Allows model to treat differently\n",
        "\n",
        "**Important**: Don't automatically remove outliers - they might contain valuable information!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q13: Explain overfitting and underfitting in Linear Regression context.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Underfitting (High Bias):**\n",
        "- Model is too simple\n",
        "- Poor performance on both training and test data\n",
        "- High training error, high test error\n",
        "\n",
        "**Causes:**\n",
        "- Too few features\n",
        "- Model doesn't capture relationship (maybe non-linear)\n",
        "- Too much regularization\n",
        "\n",
        "**Solutions:**\n",
        "- Add polynomial features\n",
        "- Add more relevant features\n",
        "- Reduce regularization\n",
        "- Try more complex model\n",
        "\n",
        "**Overfitting (High Variance):**\n",
        "- Model is too complex\n",
        "- Excellent performance on training, poor on test\n",
        "- Low training error, high test error\n",
        "- Model learns noise in training data\n",
        "\n",
        "**Causes:**\n",
        "- Too many features\n",
        "- Polynomial features with high degree\n",
        "- Too little regularization\n",
        "- Small training set\n",
        "\n",
        "**Solutions:**\n",
        "- Add more training data\n",
        "- Feature selection (remove irrelevant features)\n",
        "- Regularization (Ridge/Lasso)\n",
        "- Cross-validation\n",
        "- Reduce model complexity\n",
        "\n",
        "**Bias-Variance Tradeoff:**\n",
        "- Need to balance model complexity\n",
        "- Total Error = Bias² + Variance + Irreducible Error\n",
        "- Goal: Find sweet spot (minimum total error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q14: What is regularization? Explain Ridge and Lasso regression.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Regularization**: Technique to prevent overfitting by adding a penalty term to the cost function.\n",
        "\n",
        "**Ridge Regression (L2 Regularization):**\n",
        "$$J(\\theta) = MSE + \\lambda\\sum_{j=1}^{n}\\theta_j^2$$\n",
        "\n",
        "- Penalty: Sum of squared coefficients\n",
        "- **Effect**: Shrinks coefficients toward zero (but never exactly zero)\n",
        "- Keeps all features\n",
        "- Works well with multicollinearity\n",
        "- Normal equation: $\\theta = (X^TX + \\lambda I)^{-1}X^Ty$\n",
        "- Note: $\\lambda I$ makes matrix invertible even if $X^TX$ is singular!\n",
        "\n",
        "**Lasso Regression (L1 Regularization):**\n",
        "$$J(\\theta) = MSE + \\lambda\\sum_{j=1}^{n}|\\theta_j|$$\n",
        "\n",
        "- Penalty: Sum of absolute coefficients\n",
        "- **Effect**: Can shrink coefficients to exactly zero\n",
        "- Performs automatic feature selection\n",
        "- Creates sparse models\n",
        "- No closed-form solution (use gradient descent)\n",
        "\n",
        "**Elastic Net:**\n",
        "$$J(\\theta) = MSE + r\\lambda\\sum|\\theta_j| + \\frac{(1-r)}{2}\\lambda\\sum\\theta_j^2$$\n",
        "\n",
        "- Combines L1 and L2\n",
        "- Best of both worlds\n",
        "\n",
        "**Choosing λ:**\n",
        "- Use cross-validation\n",
        "- Higher λ → more regularization → simpler model\n",
        "- λ = 0 → standard linear regression\n",
        "\n",
        "**When to use:**\n",
        "- **Ridge**: Many features, multicollinearity, want to keep all features\n",
        "- **Lasso**: Want feature selection, sparse model\n",
        "- **Elastic Net**: Many correlated features, want feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q15: How would you implement Linear Regression from scratch in a coding interview?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Here's a concise implementation using Normal Equation:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "class LinearRegression:\n",
        "    def __init__(self):\n",
        "        self.theta = None\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train using Normal Equation.\"\"\"\n",
        "        # Add intercept term\n",
        "        m = X.shape[0]\n",
        "        X_b = np.c_[np.ones((m, 1)), X]\n",
        "        \n",
        "        # Normal equation: θ = (X^T X)^-1 X^T y\n",
        "        self.theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions.\"\"\"\n",
        "        m = X.shape[0]\n",
        "        X_b = np.c_[np.ones((m, 1)), X]\n",
        "        return X_b @ self.theta\n",
        "    \n",
        "    def score(self, X, y):\n",
        "        \"\"\"Calculate R² score.\"\"\"\n",
        "        y_pred = self.predict(X)\n",
        "        ss_tot = np.sum((y - y.mean()) ** 2)\n",
        "        ss_res = np.sum((y - y_pred) ** 2)\n",
        "        return 1 - (ss_res / ss_tot)\n",
        "\n",
        "# Usage\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "r2 = model.score(X_test, y_test)\n",
        "```\n",
        "\n",
        "**Key points to mention:**\n",
        "- Add bias/intercept term (column of ones)\n",
        "- Use vectorized operations\n",
        "- Could use `np.linalg.pinv()` for stability\n",
        "- Should handle edge cases (empty data, singular matrix)\n",
        "- Could add input validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Polynomial Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q16: What is Polynomial Regression? Is it a linear model or a non-linear model?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Polynomial Regression extends Linear Regression by adding polynomial terms (powers of the original features) to capture **non-linear relationships** between features and the target.\n",
        "\n",
        "For a single feature $x$ with degree $d$:\n",
        "\n",
        "$$\\hat{y} = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\theta_3 x^3 + \\cdots + \\theta_d x^d$$\n",
        "\n",
        "**Is it linear?** Yes — despite fitting curves, Polynomial Regression is **linear in the parameters** $\\theta$. We simply create new features ($x^2, x^3, \\ldots$) and fit a standard linear model. The model is non-linear in $x$, but the optimization (Normal Equation, Gradient Descent) works identically to regular Linear Regression.\n",
        "\n",
        "**Key distinction:**\n",
        "- **Linear in features** = model outputs a straight line/hyperplane → standard LR\n",
        "- **Linear in parameters** = the output is a linear combination of parameters → Polynomial Regression, still solvable with the same methods\n",
        "- **Non-linear model** = parameters appear non-linearly (e.g., neural networks) → requires different optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q17: How do you choose the polynomial degree? What happens if it's too high or too low?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The polynomial degree $d$ is a hyperparameter that controls model complexity:\n",
        "\n",
        "| Degree | Effect | Risk |\n",
        "|:---:|:---|:---|\n",
        "| Too low (e.g., 1 for curved data) | Can't capture the true pattern | **Underfitting** (high bias) |\n",
        "| Just right (e.g., 2–3) | Captures the relationship without memorizing noise | Good generalization |\n",
        "| Too high (e.g., 10+ for small data) | Fits training data perfectly, including noise | **Overfitting** (high variance) |\n",
        "\n",
        "**How to choose the degree:**\n",
        "\n",
        "1. **Cross-validation:** Try degrees 1, 2, 3, ..., $k$ and pick the one with the lowest cross-validated error (e.g., RMSE or MAE).\n",
        "\n",
        "2. **Learning curves:** Plot training error and validation error vs. degree:\n",
        "   - If both are high → underfitting → increase degree\n",
        "   - If training is low but validation is high → overfitting → decrease degree or add regularization\n",
        "\n",
        "3. **Domain knowledge:** If you know the physical relationship is quadratic (e.g., projectile motion), use degree 2.\n",
        "\n",
        "4. **Start simple:** Begin with degree 2. Only increase if the model underfits.\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Test different degrees\n",
        "for degree in range(1, 8):\n",
        "    model = Pipeline([\n",
        "        ('poly', PolynomialFeatures(degree=degree, include_bias=False)),\n",
        "        ('lr', LinearRegression())\n",
        "    ])\n",
        "    scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
        "    rmse = (-scores.mean()) ** 0.5\n",
        "    print(f\"Degree {degree}: RMSE = {rmse:.4f}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q18: How does the number of features grow with polynomial degree? Why is this a problem?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "With $n$ original features and polynomial degree $d$, the number of expanded features (including all cross terms) is:\n",
        "\n",
        "$$\\text{Number of features} = \\binom{n + d}{d}$$\n",
        "\n",
        "This grows **combinatorially**:\n",
        "\n",
        "| Original features ($n$) | Degree ($d$) | Polynomial features |\n",
        "|:---:|:---:|:---:|\n",
        "| 2 | 2 | 6 |\n",
        "| 2 | 3 | 10 |\n",
        "| 10 | 2 | 66 |\n",
        "| 10 | 3 | 286 |\n",
        "| 100 | 2 | 5,151 |\n",
        "| 100 | 3 | 176,851 |\n",
        "\n",
        "**Why is this a problem?**\n",
        "\n",
        "1. **Overfitting:** More features means more parameters to fit, and with limited training data the model can memorize noise instead of learning patterns.\n",
        "\n",
        "2. **Computational cost:** Training time increases with the number of features. The Normal Equation has $O(p^3)$ complexity where $p$ is the number of polynomial features. Gradient Descent has $O(kmp)$ per iteration.\n",
        "\n",
        "3. **Memory:** Storing the expanded feature matrix can become prohibitive. With $n=100$ and $d=3$, you have ~177K features per sample.\n",
        "\n",
        "4. **Curse of dimensionality:** In high-dimensional feature spaces, data points become sparse. The model needs exponentially more data to generalize well.\n",
        "\n",
        "**Solutions:**\n",
        "- Keep degree low (2–3)\n",
        "- Use **regularization** (Ridge, Lasso, Elastic Net) to constrain coefficients\n",
        "- Use Lasso or Elastic Net for **feature selection** — they zero out irrelevant polynomial terms\n",
        "- Consider `interaction_only=True` in scikit-learn to skip pure powers and keep only cross terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q19: Why is feature scaling especially important for Polynomial Regression?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Feature scaling is critical because polynomial features **amplify scale differences enormously**.\n",
        "\n",
        "**Example:** Suppose a feature $x$ has values in the range $[100, 1000]$:\n",
        "- $x$: range $[100, 1000]$\n",
        "- $x^2$: range $[10\\,000, \\; 1\\,000\\,000]$\n",
        "- $x^3$: range $[1\\,000\\,000, \\; 1\\,000\\,000\\,000]$\n",
        "\n",
        "**What happens without scaling:**\n",
        "1. **Gradient Descent** struggles: the cost surface becomes extremely elongated. Gradients for higher-degree terms are huge while gradients for lower-degree terms are tiny. The algorithm either diverges (if LR is tuned for low-degree terms) or barely moves (if tuned for high-degree terms).\n",
        "\n",
        "2. **Numerical instability:** Very large values ($10^9$) can cause floating-point overflow, especially when computing $X^TX$ for the Normal Equation.\n",
        "\n",
        "3. **Regularization is biased:** The penalty $\\lambda\\sum\\theta_j^2$ treats all coefficients equally. Without scaling, the model unfairly penalizes coefficients of small-scale features.\n",
        "\n",
        "**Best practice — scale before adding polynomial features:**\n",
        "\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "model = Pipeline([\n",
        "    ('scaler', StandardScaler()),        # Scale FIRST\n",
        "    ('poly', PolynomialFeatures(degree=3, include_bias=False)),\n",
        "    ('ridge', Ridge(alpha=1.0))\n",
        "])\n",
        "```\n",
        "\n",
        "**Note:** Some practitioners scale after polynomial expansion instead, which is also valid. The key point is that the features seen by the model must be on comparable scales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q20: When should you use Polynomial Regression vs. a non-linear model like Decision Trees?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "| Criterion | Polynomial Regression | Decision Trees / Random Forests |\n",
        "|:---|:---|:---|\n",
        "| **Relationship type** | Smooth, continuous curves | Arbitrary, step-like patterns |\n",
        "| **Interpretability** | Moderate (coefficients have meaning if degree is low) | Low for ensembles, high for single trees |\n",
        "| **Feature count** | Works well with few features + low degree | Handles many features natively |\n",
        "| **Extrapolation** | Extrapolates (but dangerously — polynomials blow up outside training range) | Cannot extrapolate (predicts constant outside training range) |\n",
        "| **Overfitting control** | Regularization + degree tuning | Max depth, min samples, pruning |\n",
        "| **Training speed** | Fast (linear algebra) | Fast (greedy splits) |\n",
        "| **Feature interactions** | Must explicitly create cross terms | Discovers interactions automatically |\n",
        "\n",
        "**Use Polynomial Regression when:**\n",
        "- You believe the true relationship is a smooth curve (e.g., physics, engineering).\n",
        "- You have few features and want interpretable coefficients.\n",
        "- You need to extrapolate slightly beyond training data (with caution).\n",
        "- You want a simple extension of your existing Linear Regression pipeline.\n",
        "\n",
        "**Use Tree-based models when:**\n",
        "- The relationship is complex, discontinuous, or unknown.\n",
        "- You have many features and don't want to worry about polynomial explosion.\n",
        "- You need automatic feature interaction discovery.\n",
        "- You don't need to extrapolate.\n",
        "\n",
        "**Important warning about extrapolation:**\n",
        "Polynomial models can produce wildly wrong predictions outside the training range. A degree-10 polynomial that fits beautifully within $[0, 10]$ can output absurd values at $x = 11$. Always be cautious when using polynomial models for prediction beyond the observed data range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q21: Implement Polynomial Regression from scratch and show how different degrees affect the fit.\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applied Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Training with Millions of Features\n",
        "\n",
        "**Question:** What Linear Regression training algorithm can you use if you have a training set with millions of features?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "When the number of features is very large (millions), the **Normal Equation** becomes impractical because it requires computing $(X^T X)^{-1}$, which involves inverting an $n \\times n$ matrix (where $n$ is the number of features). This inversion has a computational complexity of approximately $O(n^{2.4})$ to $O(n^3)$, making it extremely slow or even impossible for millions of features.\n",
        "\n",
        "Instead, you should use **Gradient Descent** — specifically:\n",
        "\n",
        "| Algorithm | Why It Works |\n",
        "|---|---|\n",
        "| **Batch Gradient Descent** | Computes gradients over the full dataset. Each iteration is $O(m \\times n)$, which is linear in $n$. Feasible if $m$ is moderate. |\n",
        "| **Stochastic Gradient Descent (SGD)** | Updates parameters using one sample at a time — $O(n)$ per step. Very efficient for large $n$ and large $m$. |\n",
        "| **Mini-batch Gradient Descent** | Compromise between Batch and SGD. Efficient, benefits from vectorized hardware. |\n",
        "\n",
        "**Key points:**\n",
        "- Gradient Descent scales linearly with the number of features, making it suitable for high-dimensional datasets.\n",
        "- SGD or Mini-batch GD are preferred when both $m$ and $n$ are large.\n",
        "- Regularization (Ridge, Lasso, Elastic Net) becomes important to prevent overfitting in high-dimensional settings.\n",
        "- Lasso (L1) is especially useful because it can perform **feature selection** by driving irrelevant feature coefficients to zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 8: Large Gap Between Training and Validation Error in Polynomial Regression\n",
        "\n",
        "**Question:** Suppose you are using Polynomial Regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "A **large gap** between training error (low) and validation error (high) is the classic sign of **overfitting** (high variance). The model has learned the noise and specific patterns of the training data rather than the underlying relationship, so it generalizes poorly to unseen data.\n",
        "\n",
        "**What is happening:**\n",
        "- The polynomial degree is too high, giving the model too many parameters relative to the amount of training data.\n",
        "- The model memorizes the training set (low training error) but fails on new data (high validation error).\n",
        "\n",
        "**Three ways to solve it:**\n",
        "\n",
        "1. **Reduce model complexity:**\n",
        "   - Lower the polynomial degree. A simpler model has fewer parameters and is less prone to overfitting.\n",
        "   - This directly addresses the root cause — the model is too flexible.\n",
        "\n",
        "2. **Add regularization:**\n",
        "   - Apply **Ridge (L2)**, **Lasso (L1)**, or **Elastic Net** regularization to penalize large coefficients.\n",
        "   - Regularization constrains the model, effectively reducing its capacity to overfit.\n",
        "   - Tune the regularization hyperparameter $\\alpha$ using cross-validation.\n",
        "\n",
        "3. **Increase the training set size:**\n",
        "   - With more data, the model has less opportunity to memorize specific examples.\n",
        "   - The training error will increase slightly, but the validation error will decrease — the gap closes.\n",
        "   - If collecting more data is not feasible, consider **data augmentation** techniques.\n",
        "\n",
        "**Bonus approaches:**\n",
        "- Use **cross-validation** to select the best polynomial degree.\n",
        "- Apply **early stopping** if using an iterative training algorithm.\n",
        "- Perform **feature selection** to remove irrelevant polynomial features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 9: Ridge Regression with High Training and Validation Error\n",
        "\n",
        "**Question:** Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter $\\alpha$ or reduce it?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Diagnosis: High Bias (Underfitting)**\n",
        "\n",
        "When both training error and validation error are **high and close to each other**, the model is **underfitting**. It is too simple to capture the underlying patterns in the data. This is the hallmark of **high bias**.\n",
        "\n",
        "| Scenario | Training Error | Validation Error | Gap | Problem |\n",
        "|---|---|---|---|---|\n",
        "| High bias (underfitting) | High | High | Small | Model is too constrained |\n",
        "| High variance (overfitting) | Low | High | Large | Model is too flexible |\n",
        "\n",
        "**What to do with $\\alpha$:**\n",
        "\n",
        "You should **reduce** the regularization hyperparameter $\\alpha$.\n",
        "\n",
        "**Reasoning:**\n",
        "- Ridge Regression adds a penalty of $\\alpha \\sum_{j=1}^{n} \\theta_j^2$ to the cost function.\n",
        "- A **large $\\alpha$** penalizes the coefficients heavily, forcing them toward zero. This makes the model too simple — it cannot fit the training data well.\n",
        "- **Reducing $\\alpha$** relaxes the constraint on the coefficients, allowing the model to learn more complex patterns and fit the data better.\n",
        "- In the extreme case, $\\alpha = 0$ gives plain Linear Regression (no regularization).\n",
        "\n",
        "**Additional strategies if reducing $\\alpha$ alone isn't enough:**\n",
        "- Add more features or polynomial features to increase model capacity.\n",
        "- Use a more complex model (e.g., decision trees, neural networks).\n",
        "- Engineer better features from domain knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 10: When to Use Ridge, Lasso, or Elastic Net\n",
        "\n",
        "**Question:** Why would you want to use:\n",
        "- Ridge Regression instead of plain Linear Regression?\n",
        "- Lasso instead of Ridge Regression?\n",
        "- Elastic Net instead of Lasso?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "#### Ridge Regression instead of plain Linear Regression\n",
        "\n",
        "Use Ridge when you suspect **overfitting** or when the model has **many features** relative to the number of observations.\n",
        "\n",
        "| Aspect | Plain Linear Regression | Ridge Regression |\n",
        "|---|---|---|\n",
        "| Regularization | None | L2 penalty: $\\alpha \\sum \\theta_j^2$ |\n",
        "| Overfitting risk | High with many features | Controlled by $\\alpha$ |\n",
        "| Coefficient behavior | Can be arbitrarily large | Shrunk toward zero (but never exactly zero) |\n",
        "| Multicollinearity | Unstable estimates | Stabilized estimates |\n",
        "\n",
        "**Use Ridge when:**\n",
        "- You have multicollinearity among features.\n",
        "- You want to keep all features but prevent any from dominating.\n",
        "- The number of features is large relative to samples.\n",
        "- You want a regularized model as a default improvement over OLS.\n",
        "\n",
        "---\n",
        "\n",
        "#### Lasso instead of Ridge Regression\n",
        "\n",
        "Use Lasso when you want **automatic feature selection** — Lasso can drive coefficients to exactly zero, effectively removing irrelevant features.\n",
        "\n",
        "| Aspect | Ridge (L2) | Lasso (L1) |\n",
        "|---|---|---|\n",
        "| Penalty | $\\alpha \\sum \\theta_j^2$ | $\\alpha \\sum |\\theta_j|$ |\n",
        "| Feature selection | No (shrinks but keeps all) | Yes (sets some coefficients to exactly 0) |\n",
        "| Sparse models | No | Yes |\n",
        "| Correlated features | Handles well (keeps all) | Arbitrarily picks one, drops others |\n",
        "\n",
        "**Use Lasso when:**\n",
        "- You believe only a few features are truly relevant (sparse solution).\n",
        "- You want an interpretable model with fewer features.\n",
        "- Feature selection is part of the modeling goal.\n",
        "\n",
        "---\n",
        "\n",
        "#### Elastic Net instead of Lasso\n",
        "\n",
        "Use Elastic Net when you want the **feature selection** ability of Lasso but also the **stability** of Ridge, especially when features are correlated.\n",
        "\n",
        "| Aspect | Lasso (L1) | Elastic Net (L1 + L2) |\n",
        "|---|---|---|\n",
        "| Penalty | $\\alpha \\sum |\\theta_j|$ | $\\alpha \\left( r \\sum |\\theta_j| + \\frac{1-r}{2} \\sum \\theta_j^2 \\right)$ |\n",
        "| Correlated features | Unstable (picks one randomly) | Groups correlated features together |\n",
        "| Feature selection | Yes | Yes (but softer) |\n",
        "| Stability | Can be erratic | More stable |\n",
        "\n",
        "**Use Elastic Net when:**\n",
        "- Features are **correlated** with each other (groups of related features).\n",
        "- You want feature selection but Lasso's behavior with correlated features is problematic.\n",
        "- The number of features is larger than the number of samples ($n > m$) — Lasso selects at most $m$ features, while Elastic Net does not have this limitation.\n",
        "- **As a general default**, Elastic Net is almost always preferred over pure Lasso.\n",
        "\n",
        "---\n",
        "\n",
        "**Summary decision flow:**\n",
        "\n",
        "```\n",
        "Plain LR → worried about overfitting? → Ridge\n",
        "Ridge → want feature selection? → Lasso\n",
        "Lasso → correlated features or n > m? → Elastic Net\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
