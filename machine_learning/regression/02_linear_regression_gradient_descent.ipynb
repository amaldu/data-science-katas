{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression - Gradient Descent Implementation\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates the implementation of Linear Regression using **Gradient Descent**.\n",
    "\n",
    "### Gradient Descent Algorithm\n",
    "Iteratively updates parameters to minimize the cost function:\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}$$\n",
    "\n",
    "**Vectorized form:**\n",
    "$$\\theta := \\theta - \\frac{\\alpha}{m} X^T(X\\theta - y)$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = learning rate\n",
    "- $m$ = number of samples\n",
    "- $X$ = feature matrix\n",
    "- $y$ = target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, alpha=0.6, label='Training')\n",
    "plt.scatter(X_test, y_test, alpha=0.6, color='orange', label='Test')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Scaling\n",
    "\n",
    "**Important:** Feature scaling speeds up gradient descent convergence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Before scaling:\")\n",
    "print(f\"  Mean: {X_train.mean():.4f}, Std: {X_train.std():.4f}\")\n",
    "print(\"\\nAfter scaling:\")\n",
    "print(f\"  Mean: {X_train_scaled.mean():.4f}, Std: {X_train_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Intercept Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    \"\"\"Add column of ones for intercept.\"\"\"\n",
    "    m = X.shape[0]\n",
    "    return np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "\n",
    "X_train_b = add_intercept(X_train_scaled)\n",
    "X_test_b = add_intercept(X_test_scaled)\n",
    "\n",
    "print(f\"Shape with intercept: {X_train_b.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cost Function (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute Mean Squared Error cost.\n",
    "    \n",
    "    J(θ) = (1/2m) * sum((h(x) - y)^2)\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    predictions = X @ theta\n",
    "    errors = predictions - y.reshape(-1, 1)\n",
    "    cost = (1 / (2 * m)) * np.sum(errors ** 2)\n",
    "    return cost\n",
    "\n",
    "# Test with random theta\n",
    "test_theta = np.random.randn(X_train_b.shape[1], 1)\n",
    "initial_cost = compute_cost(X_train_b, y_train, test_theta)\n",
    "print(f\"Initial cost (random theta): {initial_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, learning_rate, num_iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to learn theta.\n",
    "    \n",
    "    Update rule: θ := θ - α * (1/m) * X^T * (Xθ - y)\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix with intercept (m, n+1)\n",
    "        y: Target vector (m,)\n",
    "        theta: Initial parameters (n+1, 1)\n",
    "        learning_rate: Step size α\n",
    "        num_iterations: Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        theta: Optimized parameters\n",
    "        cost_history: Cost at each iteration\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    theta = theta.copy()\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Compute predictions\n",
    "        predictions = X @ theta\n",
    "        \n",
    "        # Compute errors\n",
    "        errors = predictions - y.reshape(-1, 1)\n",
    "        \n",
    "        # Compute gradient\n",
    "        gradient = (1 / m) * (X.T @ errors)\n",
    "        \n",
    "        # Update parameters\n",
    "        theta = theta - learning_rate * gradient\n",
    "        \n",
    "        # Store cost\n",
    "        cost = compute_cost(X, y, theta)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        # Print progress\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Iteration {i+1:4d}: Cost = {cost:.4f}\")\n",
    "    \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "initial_theta = np.zeros((X_train_b.shape[1], 1))\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "print(f\"Training with learning rate = {learning_rate}, iterations = {num_iterations}\\n\")\n",
    "\n",
    "# Run gradient descent\n",
    "theta_optimal, cost_history = gradient_descent(\n",
    "    X_train_b, y_train, initial_theta, learning_rate, num_iterations\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal parameters:\")\n",
    "print(f\"  θ₀ (intercept): {theta_optimal[0][0]:.4f}\")\n",
    "print(f\"  θ₁ (slope):     {theta_optimal[1][0]:.4f}\")\n",
    "print(f\"\\nFinal cost: {cost_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Cost Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Full history\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(cost_history, 'b-', linewidth=2)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost J(θ)')\n",
    "plt.title('Cost Function Convergence')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Last 50% of iterations (to see final convergence)\n",
    "plt.subplot(1, 2, 2)\n",
    "start_idx = len(cost_history) // 2\n",
    "plt.plot(range(start_idx, len(cost_history)), cost_history[start_idx:], 'g-', linewidth=2)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost J(θ)')\n",
    "plt.title('Cost Convergence (Last 50% of iterations)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Cost decreased from {cost_history[0]:.4f} to {cost_history[-1]:.4f}\")\n",
    "print(f\"Reduction: {((cost_history[0] - cost_history[-1]) / cost_history[0] * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Make Predictions and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    \"\"\"Make predictions.\"\"\"\n",
    "    return (X @ theta).flatten()\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = predict(X_train_b, theta_optimal)\n",
    "y_test_pred = predict(X_test_b, theta_optimal)\n",
    "\n",
    "# Metrics\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL PERFORMANCE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training Set:\")\n",
    "print(f\"  R²:   {train_r2:.4f}\")\n",
    "print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  R²:   {test_r2:.4f}\")\n",
    "print(f\"  RMSE: {test_rmse:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Regression line\n",
    "axes[0].scatter(X_train_scaled, y_train, alpha=0.6, label='Training')\n",
    "axes[0].scatter(X_test_scaled, y_test, alpha=0.6, color='orange', label='Test')\n",
    "\n",
    "X_range = np.linspace(X_train_scaled.min(), X_train_scaled.max(), 100).reshape(-1, 1)\n",
    "X_range_b = add_intercept(X_range)\n",
    "y_range_pred = predict(X_range_b, theta_optimal)\n",
    "axes[0].plot(X_range, y_range_pred, 'r-', linewidth=2, label='Regression line')\n",
    "\n",
    "axes[0].set_xlabel('X (scaled)')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Linear Regression (Gradient Descent)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Residuals\n",
    "residuals_test = y_test - y_test_pred\n",
    "axes[1].scatter(y_test_pred, residuals_test, alpha=0.6)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "axes[1].set_title('Residual Plot')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Experiment with Different Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "iterations = 500\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for lr in learning_rates:\n",
    "    theta_init = np.zeros((X_train_b.shape[1], 1))\n",
    "    _, cost_hist = gradient_descent(X_train_b, y_train, theta_init, lr, iterations)\n",
    "    plt.plot(cost_hist, label=f'α = {lr}', linewidth=2)\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost J(θ)')\n",
    "plt.title('Effect of Learning Rate on Convergence')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- Too small (0.001): Slow convergence\")\n",
    "print(\"- Just right (0.01-0.1): Fast, stable convergence\")\n",
    "print(\"- Too large (0.5): May oscillate or diverge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Multiple Features Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multi-feature data\n",
    "X_multi, y_multi = make_regression(n_samples=200, n_features=5, noise=15, random_state=42)\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_multi, y_multi, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale\n",
    "scaler_m = StandardScaler()\n",
    "X_train_m_scaled = scaler_m.fit_transform(X_train_m)\n",
    "X_test_m_scaled = scaler_m.transform(X_test_m)\n",
    "\n",
    "# Add intercept\n",
    "X_train_m_b = add_intercept(X_train_m_scaled)\n",
    "X_test_m_b = add_intercept(X_test_m_scaled)\n",
    "\n",
    "# Train\n",
    "theta_init_m = np.zeros((X_train_m_b.shape[1], 1))\n",
    "theta_multi, cost_hist_m = gradient_descent(X_train_m_b, y_train_m, theta_init_m, 0.01, 1000)\n",
    "\n",
    "# Evaluate\n",
    "y_test_pred_m = predict(X_test_m_b, theta_multi)\n",
    "print(f\"\\nMultiple Features Performance:\")\n",
    "print(f\"  Test R²:   {r2_score(y_test_m, y_test_pred_m):.4f}\")\n",
    "print(f\"  Test RMSE: {np.sqrt(mean_squared_error(y_test_m, y_test_pred_m)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Gradient Descent Algorithm:\n",
    "1. **Iterative optimization** - Updates parameters gradually\n",
    "2. **Learning rate (α)** - Controls step size (critical hyperparameter)\n",
    "3. **Convergence** - Cost decreases over iterations\n",
    "4. **Feature scaling** - Essential for faster convergence\n",
    "\n",
    "### Advantages:\n",
    "- ✅ Scales to large datasets\n",
    "- ✅ Works with many features\n",
    "- ✅ Memory efficient\n",
    "- ✅ Can be used for online learning\n",
    "\n",
    "### Challenges:\n",
    "- ❌ Requires tuning learning rate\n",
    "- ❌ Needs multiple iterations\n",
    "- ❌ May need feature scaling\n",
    "- ❌ Convergence depends on initialization\n",
    "\n",
    "### Best Practices:\n",
    "1. **Always scale features** (standardization or normalization)\n",
    "2. **Start with small learning rate** (0.001 - 0.01)\n",
    "3. **Plot cost history** to verify convergence\n",
    "4. **Monitor for oscillation** or divergence\n",
    "5. **Use vectorized operations** for efficiency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
