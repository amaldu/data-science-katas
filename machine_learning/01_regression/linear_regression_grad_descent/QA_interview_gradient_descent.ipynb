{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gradient Descent. Interview Q&A\n",
        "\n",
        "Interview-level questions and answers about Gradient Descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q1: What is Gradient Descent? Explain the intuition.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Gradient Descent is an **iterative optimization algorithm** used to minimize a function by moving in the direction of steepest descent (negative gradient).\n",
        "\n",
        "**Intuition:** Imagine standing on a hill in fog. You can't see the valley, but you can feel the slope under your feet. Gradient Descent says: take a step in the steepest downhill direction. Repeat until you reach the bottom.\n",
        "\n",
        "**Update Rule:**\n",
        "$$\\theta := \\theta - \\alpha \\nabla J(\\theta)$$\n",
        "\n",
        "Where:\n",
        "- $\\theta$ = parameters\n",
        "- $\\alpha$ = learning rate (step size)\n",
        "- $\\nabla J(\\theta)$ = gradient of cost function\n",
        "\n",
        "**For Linear Regression:**\n",
        "$$\\theta := \\theta - \\frac{\\alpha}{m} X^T(X\\theta - y)$$\n",
        "\n",
        "**Key idea:** The gradient tells us the direction of steepest **ascent**, so we subtract it to go **downhill**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2: What is the learning rate? What happens if it's too large or too small?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The **learning rate** ($\\alpha$) controls how big each step is during gradient descent.\n",
        "\n",
        "**Too small ($\\alpha$ = 0.0001):**\n",
        "- Very slow convergence\n",
        "- May take thousands of iterations\n",
        "- More computation time\n",
        "- But will likely converge\n",
        "\n",
        "**Too large ($\\alpha$ = 10):**\n",
        "- Overshoots the minimum\n",
        "- Oscillates around the minimum\n",
        "- May diverge (cost increases)\n",
        "- Algorithm fails to converge\n",
        "\n",
        "**Just right ($\\alpha$ = 0.01):**\n",
        "- Steady decrease in cost\n",
        "- Converges in reasonable iterations\n",
        "- Smooth convergence curve\n",
        "\n",
        "**How to choose:**\n",
        "1. Start small (0.001) and increase\n",
        "2. Try values on log scale: 0.001, 0.003, 0.01, 0.03, 0.1, 0.3\n",
        "3. Plot cost vs iterations - look for smooth decrease\n",
        "4. Use learning rate schedules (decay over time)\n",
        "5. Use adaptive methods (Adam, RMSProp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q3: What are the three types of Gradient Descent?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**1. Batch Gradient Descent:**\n",
        "- Uses ALL training samples per update\n",
        "- Computes exact gradient\n",
        "- Smooth convergence\n",
        "- Slow for large datasets\n",
        "\n",
        "**2. Stochastic Gradient Descent (SGD):**\n",
        "- Uses ONE random sample per update\n",
        "- Noisy gradient estimate\n",
        "- Faster updates, but noisy convergence\n",
        "- Can escape local minima (noise helps)\n",
        "- Good for online learning\n",
        "\n",
        "**3. Mini-Batch Gradient Descent:**\n",
        "- Uses a small batch (32, 64, 128 samples) per update\n",
        "- Best of both worlds\n",
        "- Most commonly used in practice\n",
        "- Leverages GPU parallelism\n",
        "\n",
        "**Comparison:**\n",
        "\n",
        "| Type | Speed | Stability | Memory | Usage |\n",
        "|------|-------|-----------|--------|-------|\n",
        "| Batch | Slow | Stable | High | Small data |\n",
        "| SGD | Fast | Noisy | Low | Online |\n",
        "| Mini-Batch | Medium | Medium | Medium | Most common |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q4: Why is feature scaling important for Gradient Descent?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Without scaling:**\n",
        "- Features on different scales create elongated contours in the cost function\n",
        "- Gradient descent oscillates (zigzags) across the narrow dimension\n",
        "- Takes many more iterations to converge\n",
        "- May need very small learning rate\n",
        "\n",
        "**With scaling:**\n",
        "- Contours become more circular\n",
        "- Gradient points more directly toward the minimum\n",
        "- Converges much faster\n",
        "- Same learning rate works well for all features\n",
        "\n",
        "**Example:**\n",
        "- Feature 1: House size (0 - 5000 sq ft)\n",
        "- Feature 2: Number of bedrooms (1 - 5)\n",
        "- Without scaling, the gradient is dominated by the larger scale feature\n",
        "\n",
        "**Common scaling methods:**\n",
        "1. **Standardization (Z-score):** $x' = \\frac{x - \\mu}{\\sigma}$ (mean=0, std=1)\n",
        "2. **Min-Max normalization:** $x' = \\frac{x - x_{min}}{x_{max} - x_{min}}$ (range 0-1)\n",
        "\n",
        "**Note:** Feature scaling is NOT needed for the Normal Equation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intermediate Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q5: How do you know if Gradient Descent has converged?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Methods to check convergence:**\n",
        "\n",
        "1. **Plot cost vs iterations:**\n",
        "   - Cost should decrease with each iteration\n",
        "   - Curve flattens when converged\n",
        "   - If cost increases, learning rate is too high\n",
        "\n",
        "2. **Automatic convergence test:**\n",
        "   - Stop when cost decreases by less than a threshold $\\epsilon$\n",
        "   - Example: Stop if $J(\\theta^{(t-1)}) - J(\\theta^{(t)}) < 10^{-6}$\n",
        "\n",
        "3. **Gradient magnitude:**\n",
        "   - At minimum, gradient approaches zero\n",
        "   - Stop when $||\\nabla J(\\theta)|| < \\epsilon$\n",
        "\n",
        "4. **Parameter change:**\n",
        "   - Stop when $||\\theta^{(t)} - \\theta^{(t-1)}|| < \\epsilon$\n",
        "\n",
        "**Red flags:**\n",
        "- Cost increases -> learning rate too high\n",
        "- Cost fluctuates wildly -> learning rate too high or SGD noise\n",
        "- Cost decreases very slowly -> learning rate too low\n",
        "- Cost plateaus early -> may be stuck in local minimum (not an issue for linear regression since MSE is convex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q6: What is the difference between convex and non-convex cost functions? Why does it matter?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Convex function:**\n",
        "- Has exactly ONE global minimum\n",
        "- No local minima\n",
        "- Bowl-shaped (any line between two points lies above the function)\n",
        "- **Linear regression MSE is convex**\n",
        "- Gradient descent guaranteed to find global minimum\n",
        "\n",
        "**Non-convex function:**\n",
        "- Has multiple local minima\n",
        "- Gradient descent may get stuck in a local minimum\n",
        "- Example: Neural network cost functions\n",
        "- Need techniques like momentum, random restarts, simulated annealing\n",
        "\n",
        "**Why it matters for linear regression:**\n",
        "- MSE with linear model is **always convex**\n",
        "- No local minima to worry about\n",
        "- GD will always converge to global minimum (with proper learning rate)\n",
        "- Both Normal Equation and GD give same result\n",
        "\n",
        "**Mathematical proof of convexity:**\n",
        "- Second derivative (Hessian) of MSE = $\\frac{2}{m}X^TX$\n",
        "- $X^TX$ is positive semi-definite\n",
        "- Therefore MSE is convex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q7: Explain the gradient computation step-by-step for linear regression.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Cost function:**\n",
        "$$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
        "\n",
        "Where $h_\\theta(x) = \\theta^Tx = X\\theta$\n",
        "\n",
        "**Step 1 - Compute predictions:**\n",
        "$$\\hat{y} = X\\theta$$\n",
        "\n",
        "**Step 2 - Compute errors:**\n",
        "$$e = \\hat{y} - y = X\\theta - y$$\n",
        "\n",
        "**Step 3 - Compute gradient:**\n",
        "\n",
        "Take the partial derivative of $J$ with respect to $\\theta_j$:\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\theta_j} = \\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$\n",
        "\n",
        "In vectorized form:\n",
        "$$\\nabla J(\\theta) = \\frac{1}{m}X^T(X\\theta - y)$$\n",
        "\n",
        "**Step 4 - Update parameters:**\n",
        "$$\\theta := \\theta - \\alpha \\cdot \\nabla J(\\theta)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q8: What is the computational complexity of Gradient Descent vs Normal Equation?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Normal Equation:**\n",
        "- $O(n^3)$ - dominated by matrix inversion of $(X^TX)^{-1}$\n",
        "- $X^TX$ computation: $O(mn^2)$\n",
        "- Matrix inversion: $O(n^3)$\n",
        "- Where n = number of features, m = number of samples\n",
        "- **Bottleneck: n**\n",
        "\n",
        "**Gradient Descent (one iteration):**\n",
        "- Matrix multiplication $X\\theta$: $O(mn)$\n",
        "- Gradient $X^T \\cdot errors$: $O(mn)$\n",
        "- Total per iteration: $O(mn)$\n",
        "- Total for k iterations: $O(kmn)$\n",
        "- **Bottleneck: number of iterations**\n",
        "\n",
        "**Comparison:**\n",
        "- When n is small: Normal Equation wins (one-shot)\n",
        "- When n is large (>10,000): GD wins ($mn$ vs $n^3$)\n",
        "- When m is very large: GD can use mini-batches\n",
        "\n",
        "**Memory:**\n",
        "- Normal Equation: Must store $X^TX$ ($n \\times n$ matrix)\n",
        "- GD: Only needs current batch in memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q9: What are learning rate schedules? Name a few.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Learning rate schedules **reduce the learning rate during training** to improve convergence.\n",
        "\n",
        "**Why?** Start with large steps (fast progress), end with small steps (precision).\n",
        "\n",
        "**Common schedules:**\n",
        "\n",
        "1. **Step Decay:**\n",
        "   - Reduce by factor every N epochs\n",
        "   - $\\alpha_t = \\alpha_0 \\cdot \\gamma^{\\lfloor t/N \\rfloor}$\n",
        "\n",
        "2. **Exponential Decay:**\n",
        "   - $\\alpha_t = \\alpha_0 \\cdot e^{-kt}$\n",
        "\n",
        "3. **1/t Decay:**\n",
        "   - $\\alpha_t = \\frac{\\alpha_0}{1 + kt}$\n",
        "\n",
        "4. **Cosine Annealing:**\n",
        "   - $\\alpha_t = \\alpha_{min} + \\frac{1}{2}(\\alpha_{max} - \\alpha_{min})(1 + \\cos(\\frac{t\\pi}{T}))$\n",
        "\n",
        "**Adaptive optimizers** (automatically adjust learning rate):\n",
        "- **AdaGrad**: Adapts per-parameter based on historical gradients\n",
        "- **RMSProp**: Uses exponential moving average of squared gradients\n",
        "- **Adam**: Combines momentum + RMSProp (most popular)\n",
        "\n",
        "**For linear regression:** Usually a fixed learning rate with feature scaling is sufficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q10: Describe how you would implement Gradient Descent from scratch.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Implementation approach:**\n",
        "\n",
        "1. **Initialization:** Add a bias column of ones to $X$. Initialize $\\theta$ to zeros. Set learning rate and max iterations.\n",
        "\n",
        "2. **Training loop (repeat for each iteration):**\n",
        "   - Compute predictions: $\\hat{y} = X_b \\theta$\n",
        "   - Compute errors: $e = \\hat{y} - y$\n",
        "   - Compute gradient: $\\nabla J = \\frac{1}{m} X_b^T e$\n",
        "   - Update parameters: $\\theta := \\theta - \\alpha \\nabla J$\n",
        "   - Track cost: $J = \\frac{1}{2m} \\sum e_i^2$\n",
        "\n",
        "3. **Prediction:** For new data, add bias column and compute $X_b \\theta$.\n",
        "\n",
        "4. **Evaluation ($R^2$ score):**\n",
        "$$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}$$\n",
        "\n",
        "**Key points to mention:**\n",
        "- Feature scaling before training\n",
        "- Initialize weights to zeros\n",
        "- Vectorized operations (no loops over features)\n",
        "- Track cost history for debugging\n",
        "- Could add early stopping for efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q11: What is momentum in Gradient Descent?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Momentum** accelerates gradient descent by adding a fraction of the previous update to the current one.\n",
        "\n",
        "**Standard GD:**\n",
        "$$\\theta := \\theta - \\alpha \\nabla J(\\theta)$$\n",
        "\n",
        "**GD with Momentum:**\n",
        "$$v_t = \\beta v_{t-1} + \\alpha \\nabla J(\\theta)$$\n",
        "$$\\theta := \\theta - v_t$$\n",
        "\n",
        "Where $\\beta$ is the momentum coefficient (typically 0.9).\n",
        "\n",
        "**Analogy:** A ball rolling downhill accumulates velocity. Momentum lets the update accumulate speed in consistent directions.\n",
        "\n",
        "**Benefits:**\n",
        "- Faster convergence (especially in narrow valleys)\n",
        "- Dampens oscillations across steep dimensions\n",
        "- Helps escape shallow local minima\n",
        "- Reduces zigzagging with SGD\n",
        "\n",
        "**Used in:** SGD with momentum, Adam optimizer, Nesterov Accelerated Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q12: What is the difference between Gradient Descent and Stochastic Gradient Descent?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "| Aspect | Batch GD | SGD |\n",
        "|--------|----------|-----|\n",
        "| **Samples per update** | All m samples | 1 sample |\n",
        "| **Gradient quality** | Exact | Noisy estimate |\n",
        "| **Convergence path** | Smooth | Noisy / Zigzag |\n",
        "| **Speed per epoch** | Slow | Fast |\n",
        "| **Memory** | High | Low |\n",
        "| **Online learning** | No | Yes |\n",
        "| **Local minima** | Can get stuck | Noise helps escape |\n",
        "\n",
        "**SGD advantages:**\n",
        "- Much faster per iteration\n",
        "- Can handle datasets that don't fit in memory\n",
        "- Noise acts as implicit regularization\n",
        "- Can update model as new data arrives\n",
        "\n",
        "**SGD challenges:**\n",
        "- Noisy updates (may not converge smoothly)\n",
        "- Need learning rate schedule for convergence\n",
        "- Final solution oscillates around minimum\n",
        "\n",
        "**In practice:** Mini-batch GD (batch_size = 32-256) is most common. It balances the benefits of both approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q13: Can Gradient Descent get stuck in local minima for Linear Regression?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**No**, not for linear regression.\n",
        "\n",
        "**Reason:** The MSE cost function for linear regression is **convex** (bowl-shaped). A convex function has exactly one minimum, which is the global minimum. There are no local minima to get stuck in.\n",
        "\n",
        "**Proof:** The Hessian matrix (second derivative) of the MSE cost function is:\n",
        "$$H = \\frac{2}{m}X^TX$$\n",
        "\n",
        "$X^TX$ is always positive semi-definite, which means the cost function is convex.\n",
        "\n",
        "**However, GD can still fail to converge if:**\n",
        "- Learning rate is too large (diverges)\n",
        "- Not enough iterations\n",
        "- Numerical issues (overflow/underflow)\n",
        "\n",
        "**Note:** For neural networks and other non-linear models, the cost function IS non-convex, and getting stuck in local minima (or saddle points) is a real concern."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q14: What is the vanishing/exploding gradient problem? Is it relevant to Linear Regression?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Vanishing Gradient:**\n",
        "- Gradients become extremely small during backpropagation\n",
        "- Parameters barely update\n",
        "- Training stalls\n",
        "\n",
        "**Exploding Gradient:**\n",
        "- Gradients become extremely large\n",
        "- Parameters update too aggressively\n",
        "- Numerical overflow, NaN values\n",
        "\n",
        "**Relevant to Linear Regression?**\n",
        "\n",
        "**Vanishing gradients: No.** Linear regression has no activation functions or deep layers that cause gradients to diminish.\n",
        "\n",
        "**Exploding gradients: Partially.** Can happen if:\n",
        "- Features are not scaled (very large values)\n",
        "- Learning rate is too high\n",
        "\n",
        "**Solution for linear regression:**\n",
        "- Feature scaling (standardization)\n",
        "- Appropriate learning rate\n",
        "- Gradient clipping (cap gradient magnitude)\n",
        "\n",
        "**This problem is mainly a deep learning concern** (deep neural networks with many layers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q15: Explain the Adam optimizer. Why is it so popular?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Adam (Adaptive Moment Estimation)** combines two ideas:\n",
        "1. **Momentum** (first moment: running average of gradients)\n",
        "2. **RMSProp** (second moment: running average of squared gradients)\n",
        "\n",
        "**Algorithm:**\n",
        "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1)g_t$$\n",
        "$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2$$\n",
        "$$\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$$\n",
        "$$\\theta := \\theta - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$$\n",
        "\n",
        "**Default hyperparameters:** $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon = 10^{-8}$\n",
        "\n",
        "**Why popular:**\n",
        "- Adaptive per-parameter learning rates\n",
        "- Works well out-of-the-box with default settings\n",
        "- Combines benefits of momentum and RMSProp\n",
        "- Handles sparse gradients well\n",
        "- Bias correction for initial iterations\n",
        "- Fast convergence\n",
        "\n",
        "**For linear regression:** Adam is overkill. Simple batch GD works fine. Adam shines in deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applied Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Features with Very Different Scales\n",
        "\n",
        "**Question:** Suppose the features in your training set have very different scales. What algorithms might suffer from this, and how? What can you do about it?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Algorithms that suffer from different feature scales:**\n",
        "\n",
        "**1. Gradient Descent (all variants)**\n",
        "- Features with large scales dominate the gradient computation.\n",
        "- The cost function surface becomes elongated (elliptical rather than circular).\n",
        "- GD oscillates along the steep direction and moves slowly along the flat direction.\n",
        "- This leads to **very slow convergence** or requires a tiny learning rate.\n",
        "\n",
        "**2. Regularized models (Ridge, Lasso, Elastic Net)**\n",
        "- Regularization penalizes coefficient magnitudes: $\\alpha \\sum |\\theta_j|$ or $\\alpha \\sum \\theta_j^2$.\n",
        "- A feature with a large scale naturally has a small coefficient, and vice versa.\n",
        "- Without scaling, the penalty treats features unfairly. It penalizes small-scale features disproportionately.\n",
        "\n",
        "**3. Distance-based algorithms** (KNN, SVM, K-Means)\n",
        "- Distance calculations are dominated by features with larger scales.\n",
        "- (Less relevant to this course, but important to know.)\n",
        "\n",
        "**What to do about it. Feature Scaling:**\n",
        "\n",
        "| Method | Formula | When to Use |\n",
        "|---|---|---|\n",
        "| **Standardization (Z-score)** | $x' = \\frac{x - \\mu}{\\sigma}$ | Most common default. Works well with GD and regularization. |\n",
        "| **Min-Max Normalization** | $x' = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}$ | When you need values in $[0, 1]$. Sensitive to outliers. |\n",
        "| **Robust Scaling** | $x' = \\frac{x - \\text{median}}{\\text{IQR}}$ | When the data has outliers. |\n",
        "\n",
        "**Important notes:**\n",
        "- Fit the scaler on the **training set only**, then transform both training and test sets.\n",
        "- The Normal Equation does **not** require feature scaling (it finds the exact solution algebraically).\n",
        "- Gradient Descent **always** benefits from feature scaling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Can Gradient Descent Get Stuck in a Local Minimum in Logistic Regression?\n",
        "\n",
        "**Question:** Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**No.** Gradient Descent **cannot** get stuck in a local minimum when training a Logistic Regression model.\n",
        "\n",
        "**Reason:** The cost function of Logistic Regression (Binary Cross-Entropy / Log Loss) is **convex**:\n",
        "\n",
        "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i)) \\right]$$\n",
        "\n",
        "**Properties of a convex function:**\n",
        "- It has a single **global minimum** (no local minima).\n",
        "- Any local minimum is also the global minimum.\n",
        "- Gradient Descent is guaranteed to converge to the global minimum (with an appropriate learning rate).\n",
        "\n",
        "**Key distinction:**\n",
        "| Function type | Local minima? | GD behavior |\n",
        "|---|---|---|\n",
        "| **Convex** (Logistic Regression, Linear Regression) | No. Only global minimum | Always converges to global optimum |\n",
        "| **Non-convex** (Neural Networks) | Yes. Many local minima and saddle points | Can get stuck |\n",
        "\n",
        "**Caveat:** While GD won't get stuck in a local minimum, it can still:\n",
        "- Converge **very slowly** if the learning rate is too small.\n",
        "- **Diverge** if the learning rate is too large.\n",
        "- Oscillate around the minimum without converging (if learning rate is not decayed in SGD/Mini-batch)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4: Do All Gradient Descent Algorithms Lead to the Same Model?\n",
        "\n",
        "**Question:** Do all Gradient Descent algorithms lead to the same model provided you let them run long enough?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**It depends on the cost function:**\n",
        "\n",
        "**If the cost function is convex** (e.g., Linear Regression, Logistic Regression):\n",
        "- **Batch Gradient Descent:** Yes. It will converge to the **global optimum** given a sufficiently small learning rate and enough iterations.\n",
        "- **Stochastic GD and Mini-batch GD:** Not exactly. They will **oscillate around** the global optimum but never settle precisely on it (due to the noisy gradient estimates). However:\n",
        "  - With a **decaying learning rate schedule**, they can converge to the global optimum.\n",
        "  - Without decay, they oscillate in a small region near the optimum. Close but not identical.\n",
        "\n",
        "**If the cost function is non-convex** (e.g., Neural Networks):\n",
        "- **No.** Different algorithms (and even different random initializations) can converge to **different local minima**.\n",
        "- Batch GD is deterministic (given the same initialization), but SGD and Mini-batch GD introduce randomness that leads to different solutions.\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "| Algorithm | Convex function | Non-convex function |\n",
        "|---|---|---|\n",
        "| **Batch GD** | Converges to global optimum | Depends on initialization |\n",
        "| **SGD** | Oscillates near global optimum (converges with LR decay) | May find different local minima |\n",
        "| **Mini-batch GD** | Oscillates near global optimum (converges with LR decay) | May find different local minima |\n",
        "\n",
        "**Practical takeaway:** For convex problems, all GD variants converge to the same solution *if* you use a proper learning rate schedule. For non-convex problems, the answer is no."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 5: Validation Error Consistently Going Up with Batch GD\n",
        "\n",
        "**Question:** Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "There are **two possible causes:**\n",
        "\n",
        "---\n",
        "\n",
        "**Cause 1: The learning rate is too high (most likely)**\n",
        "\n",
        "- If the learning rate is too large, Batch GD **overshoots** the minimum and diverges.\n",
        "- The cost function increases at each step instead of decreasing.\n",
        "- Both training and validation error would go up.\n",
        "\n",
        "**Fix:** Reduce the learning rate.\n",
        "\n",
        "---\n",
        "\n",
        "**Cause 2: The model is overfitting (if training error is going DOWN)**\n",
        "\n",
        "- If the **training error** is decreasing but the **validation error** is increasing, the model is overfitting.\n",
        "- The model is memorizing the training data instead of learning generalizable patterns.\n",
        "- This happens when the model is too complex for the available data.\n",
        "\n",
        "**Fixes:**\n",
        "- **Early stopping:** Monitor the validation error and stop training when it starts to increase (after a patience period).\n",
        "- **Regularization:** Add L1, L2, or Elastic Net penalty to constrain the model.\n",
        "- **Reduce model complexity:** Use fewer features or a lower polynomial degree.\n",
        "- **Get more training data** if possible.\n",
        "\n",
        "---\n",
        "\n",
        "**How to distinguish between the two causes:**\n",
        "\n",
        "| Cause | Training error | Validation error |\n",
        "|---|---|---|\n",
        "| Learning rate too high | Goes **up** | Goes **up** |\n",
        "| Overfitting | Goes **down** | Goes **up** |\n",
        "\n",
        "**Diagnostic steps:**\n",
        "1. Plot both training error and validation error.\n",
        "2. If both go up → reduce learning rate.\n",
        "3. If training goes down and validation goes up → overfitting → apply regularization or early stopping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 6: Stopping Mini-batch GD When Validation Error Goes Up\n",
        "\n",
        "**Question:** Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**No.** It is **not** a good idea to stop immediately.\n",
        "\n",
        "**Why not?**\n",
        "\n",
        "Mini-batch GD uses a **random subset** of the training data at each step, so the gradient estimates are **noisy**. This noise causes:\n",
        "- The training error to fluctuate from step to step.\n",
        "- The validation error to fluctuate as well. It may temporarily go up even though the overall trend is downward.\n",
        "\n",
        "If you stop at the first sign of an increase, you will likely stop **prematurely** and miss further improvements.\n",
        "\n",
        "**What to do instead. Early Stopping with Patience:**\n",
        "\n",
        "1. Track the **best validation error** seen so far.\n",
        "2. Set a **patience** parameter (e.g., 10-20 epochs).\n",
        "3. If the validation error has not improved for `patience` consecutive epochs, **then** stop.\n",
        "4. Restore the model parameters from the epoch with the **best** validation error.\n",
        "\n",
        "```\n",
        "best_val_error = ∞\n",
        "patience = 20\n",
        "patience_counter = 0\n",
        "best_θ = None\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    train_one_epoch()\n",
        "    val_error = compute_validation_error()\n",
        "    \n",
        "    if val_error < best_val_error:\n",
        "        best_val_error = val_error\n",
        "        best_θ = θ.copy()\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    \n",
        "    if patience_counter ≥ patience:\n",
        "        θ = best_θ  # Restore best model\n",
        "        break\n",
        "```\n",
        "\n",
        "**Key points:**\n",
        "- **Patience** absorbs the natural noise in Mini-batch GD.\n",
        "- Save and restore the best model, not the final one.\n",
        "- This technique is widely used in deep learning as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 7: Fastest vs. Converging GD Algorithms\n",
        "\n",
        "**Question:** Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Fastest to reach the vicinity of the optimum: Stochastic Gradient Descent (SGD)**\n",
        "\n",
        "- SGD updates parameters after **every single sample**, so it makes progress very quickly.\n",
        "- It can escape shallow local minima (useful for non-convex problems).\n",
        "- However, because each update is based on a single noisy sample, it **oscillates** around the optimum and never truly settles.\n",
        "\n",
        "**Actually converges: Batch Gradient Descent**\n",
        "\n",
        "- Batch GD computes the **exact gradient** over the entire dataset at each step.\n",
        "- With a fixed, sufficiently small learning rate, it converges smoothly and precisely to the optimum (for convex problems).\n",
        "- Drawback: each step is slow because it processes all $m$ samples.\n",
        "\n",
        "**Comparison of all three:**\n",
        "\n",
        "| Algorithm | Speed to vicinity | Convergence | Gradient quality |\n",
        "|---|---|---|---|\n",
        "| **Batch GD** | Slowest (full dataset per step) | **Converges** precisely | Exact |\n",
        "| **Mini-batch GD** | Fast | Oscillates near optimum | Approximate (less noisy than SGD) |\n",
        "| **SGD** | **Fastest** (one sample per step) | Oscillates around optimum | Very noisy |\n",
        "\n",
        "**How to make SGD and Mini-batch GD converge:**\n",
        "\n",
        "Use a **learning rate schedule** that decreases the learning rate over time:\n",
        "\n",
        "| Schedule | Formula | Effect |\n",
        "|---|---|---|\n",
        "| **Step decay** | Reduce $\\eta$ by factor every $k$ epochs | Simple, widely used |\n",
        "| **Exponential decay** | $\\eta_t = \\eta_0 \\cdot e^{-kt}$ | Smooth decrease |\n",
        "| **Inverse scaling** | $\\eta_t = \\frac{\\eta_0}{1 + kt}$ | Gradual decrease |\n",
        "| **1/t schedule** | $\\eta_t = \\frac{\\eta_0}{t}$ | Classic theoretical guarantee |\n",
        "\n",
        "**Key insight:** As the learning rate decreases toward zero, the noisy updates become smaller and smaller, allowing SGD and Mini-batch GD to converge. The conditions for guaranteed convergence are:\n",
        "\n",
        "$$\\sum_{t=1}^{\\infty} \\eta_t = \\infty \\quad \\text{and} \\quad \\sum_{t=1}^{\\infty} \\eta_t^2 < \\infty$$\n",
        "\n",
        "This ensures the steps are large enough to reach the optimum but small enough to settle down."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbformat_minor": 4,
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
