{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Topics - Solutions\n\nRolling and expanding windows, advanced MultiIndex operations, memory optimization, and query() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\nCreate a time series DataFrame and apply a rolling window calculation (e.g., 3-period moving average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\nimport numpy as np\n\n# Create time series data\nnp.random.seed(42)\ndates = pd.date_range('2023-01-01', periods=20, freq='D')\nvalues = np.random.randn(20).cumsum() + 100\n\nts_df = pd.DataFrame({\n    'Date': dates,\n    'Price': values\n})\nts_df.set_index('Date', inplace=True)\n\nprint(\"Original time series:\")\nprint(ts_df.head(10))\n\n# Apply rolling window\nts_df['MA_3'] = ts_df['Price'].rolling(window=3).mean()\nts_df['MA_5'] = ts_df['Price'].rolling(window=5).mean()\nts_df['Rolling_Std'] = ts_df['Price'].rolling(window=3).std()\n\nprint(\"\\nWith rolling statistics:\")\nprint(ts_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\nUse expanding() window to calculate cumulative statistics over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding window calculations\nts_df['Expanding_Mean'] = ts_df['Price'].expanding().mean()\nts_df['Expanding_Std'] = ts_df['Price'].expanding().std()\nts_df['Expanding_Min'] = ts_df['Price'].expanding().min()\nts_df['Expanding_Max'] = ts_df['Price'].expanding().max()\n\nprint(\"Expanding window statistics:\")\nprint(ts_df[['Price', 'Expanding_Mean', 'Expanding_Std', 'Expanding_Min', 'Expanding_Max']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\nCreate a DataFrame with MultiIndex and perform advanced indexing operations (cross-section, index slicing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MultiIndex DataFrame\nindices = pd.MultiIndex.from_tuples([\n    ('A', 'X', 1), ('A', 'X', 2), ('A', 'Y', 1), ('A', 'Y', 2),\n    ('B', 'X', 1), ('B', 'X', 2), ('B', 'Y', 1), ('B', 'Y', 2)\n], names=['Group', 'Category', 'Period'])\n\nmulti_df = pd.DataFrame({\n    'Value1': np.random.randn(8),\n    'Value2': np.random.randn(8)\n}, index=indices)\n\nprint(\"MultiIndex DataFrame:\")\nprint(multi_df)\n\n# Cross-section (xs)\nprint(\"\\nCross-section for Group 'A':\")\nprint(multi_df.xs('A', level='Group'))\n\nprint(\"\\nCross-section for Category 'X':\")\nprint(multi_df.xs('X', level='Category'))\n\n# Index slicing\nprint(\"\\nSlicing Group 'A', Category 'X':\")\nprint(multi_df.loc[('A', 'X'), :])\n\n# Multiple level selection\nprint(\"\\nSelect multiple groups:\")\nprint(multi_df.loc[(['A', 'B'], 'X'), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\nUse the query() method to filter data with complex conditions instead of boolean indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\nquery_df = pd.DataFrame({\n    'A': np.random.randn(100),\n    'B': np.random.randn(100),\n    'C': np.random.choice(['X', 'Y', 'Z'], 100),\n    'D': np.random.randint(1, 10, 100)\n})\n\nprint(\"Sample data:\")\nprint(query_df.head())\n\n# Complex query conditions\nprint(\"\\nQuery: A > 0 and B < 0 and C == 'X':\")\nresult1 = query_df.query(\"A > 0 and B < 0 and C == 'X'\")\nprint(f\"Rows found: {len(result1)}\")\nprint(result1.head())\n\n# Query with variables\nthreshold = 0.5\nprint(f\"\\nQuery: A > {threshold} and D >= 5:\")\nresult2 = query_df.query(\"A > @threshold and D >= 5\")\nprint(f\"Rows found: {len(result2)}\")\nprint(result2.head())\n\n# Query with isin\nvalid_categories = ['X', 'Y']\nresult3 = query_df.query(\"C in @valid_categories and D % 2 == 0\")\nprint(f\"\\nQuery with isin and modulo: {len(result3)} rows\")\nprint(result3.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\nOptimize memory usage of a DataFrame by converting object columns to categorical and downcasting numeric types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with inefficient types\noptim_df = pd.DataFrame({\n    'int_col': np.random.randint(0, 100, 10000),  # Can be int8\n    'float_col': np.random.randn(10000),  # Can be float32\n    'category_col': np.random.choice(['A', 'B', 'C'], 10000),  # Should be categorical\n    'big_int': np.random.randint(0, 1000, 10000)  # Can be int16\n})\n\nprint(\"Original memory usage:\")\nprint(optim_df.dtypes)\nprint(f\"Memory usage: {optim_df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n\n# Optimize memory\noptim_df_copy = optim_df.copy()\n\n# Downcast integers\noptim_df_copy['int_col'] = pd.to_numeric(optim_df_copy['int_col'], downcast='integer')\noptim_df_copy['big_int'] = pd.to_numeric(optim_df_copy['big_int'], downcast='integer')\n\n# Downcast floats\noptim_df_copy['float_col'] = pd.to_numeric(optim_df_copy['float_col'], downcast='float')\n\n# Convert to categorical\noptim_df_copy['category_col'] = optim_df_copy['category_col'].astype('category')\n\nprint(\"\\nOptimized memory usage:\")\nprint(optim_df_copy.dtypes)\nprint(f\"Memory usage: {optim_df_copy.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n\nmemory_saved = optim_df.memory_usage(deep=True).sum() - optim_df_copy.memory_usage(deep=True).sum()\nprint(f\"Memory saved: {memory_saved / 1024:.2f} KB ({memory_saved/optim_df.memory_usage(deep=True).sum()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\nImplement a custom rolling window function that calculates the range (max - min) over a window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom rolling function\ndef rolling_range(series):\n    return series.max() - series.min()\n\n# Apply custom function\nts_df['Rolling_Range'] = ts_df['Price'].rolling(window=5).apply(rolling_range, raw=False)\n\nprint(\"Custom rolling range calculation:\")\nprint(ts_df[['Price', 'Rolling_Range']].head(10))\n\n# Compare with built-in functions\nts_df['Rolling_Max'] = ts_df['Price'].rolling(window=5).max()\nts_df['Rolling_Min'] = ts_df['Price'].rolling(window=5).min()\nts_df['Range_Manual'] = ts_df['Rolling_Max'] - ts_df['Rolling_Min']\n\nprint(\"\\nVerification (should be equal):\")\nprint((ts_df['Rolling_Range'] == ts_df['Range_Manual']).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\nWork with MultiIndex DataFrames: swap levels, sort by multiple levels, and reset specific levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original MultiIndex DataFrame:\")\nprint(multi_df)\n\n# Swap levels\nswapped_df = multi_df.swaplevel('Group', 'Category')\nprint(\"\\nAfter swapping Group and Category levels:\")\nprint(swapped_df)\n\n# Sort by multiple levels\nsorted_df = multi_df.sort_index(level=['Category', 'Group'])\nprint(\"\\nSorted by Category then Group:\")\nprint(sorted_df)\n\n# Reset specific level\nreset_df = multi_df.reset_index(level='Period')\nprint(\"\\nAfter resetting Period level:\")\nprint(reset_df)\n\n# Reorder levels\nreordered_df = multi_df.reorder_levels(['Category', 'Group', 'Period'])\nprint(\"\\nReordered levels:\")\nprint(reordered_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\nUse eval() method for efficient evaluation of expressions on large DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create large DataFrame\neval_df = pd.DataFrame({\n    'A': np.random.randn(10000),\n    'B': np.random.randn(10000),\n    'C': np.random.randn(10000)\n})\n\nprint(\"Using eval() for efficient computation:\")\n\n# Traditional approach\ntraditional_result = eval_df['A'] + eval_df['B'] * eval_df['C']\n\n# Using eval()\neval_result = eval_df.eval('A + B * C')\n\nprint(f\"Results are equal: {(traditional_result == eval_result).all()}\")\n\n# More complex expression\neval_df.eval('D = A + B', inplace=True)\neval_df.eval('E = A * B + C', inplace=True)\neval_df.eval('F = (A > 0) & (B > 0)', inplace=True)\n\nprint(\"\\nNew columns created with eval():\")\nprint(eval_df[['A', 'B', 'C', 'D', 'E', 'F']].head())\n\n# Query combined with eval\nfiltered = eval_df.query('A > 0 and B > 0')\nprint(f\"\\nFiltered DataFrame shape: {filtered.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\nApply rolling correlation between two time series to detect changing relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two related time series\nnp.random.seed(42)\nn_periods = 100\ndates = pd.date_range('2023-01-01', periods=n_periods, freq='D')\n\n# Create series with changing correlation\nseries1 = np.random.randn(n_periods).cumsum()\nseries2 = np.concatenate([\n    series1[:50] + np.random.randn(50) * 0.2,  # High correlation first half\n    np.random.randn(50).cumsum()  # Low correlation second half\n])\n\ncorr_df = pd.DataFrame({\n    'Series1': series1,\n    'Series2': series2\n}, index=dates)\n\nprint(\"Time series data:\")\nprint(corr_df.head())\n\n# Calculate rolling correlation\nwindow_size = 20\ncorr_df['Rolling_Correlation'] = corr_df['Series1'].rolling(window=window_size).corr(corr_df['Series2'])\n\nprint(f\"\\nRolling correlation (window={window_size}):\")\nprint(corr_df[['Series1', 'Series2', 'Rolling_Correlation']].head(25))\n\n# Summary statistics of rolling correlation\nprint(f\"\\nRolling correlation statistics:\")\nprint(corr_df['Rolling_Correlation'].describe())\n\n# Find periods of high/low correlation\nhigh_corr = corr_df[corr_df['Rolling_Correlation'] > 0.8]\nlow_corr = corr_df[corr_df['Rolling_Correlation'] < 0.2]\n\nprint(f\"\\nPeriods with high correlation (>0.8): {len(high_corr)}\")\nprint(f\"Periods with low correlation (<0.2): {len(low_corr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10\nCreate a custom aggregation function using apply() with rolling windows for complex calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom aggregation function: calculate percentage of values above mean\ndef pct_above_mean(series):\n    if len(series) == 0:\n        return np.nan\n    mean_val = series.mean()\n    return (series > mean_val).sum() / len(series) * 100\n\n# Custom function: volatility (coefficient of variation)\ndef coefficient_of_variation(series):\n    if len(series) == 0 or series.mean() == 0:\n        return np.nan\n    return series.std() / abs(series.mean()) * 100\n\n# Apply custom functions to rolling windows\nts_df['Pct_Above_Mean'] = ts_df['Price'].rolling(window=7).apply(pct_above_mean, raw=False)\nts_df['CoV'] = ts_df['Price'].rolling(window=7).apply(coefficient_of_variation, raw=False)\n\nprint(\"Custom rolling aggregations:\")\nprint(ts_df[['Price', 'Pct_Above_Mean', 'CoV']].head(15))\n\n# Multiple custom functions at once\ndef rolling_stats(series):\n    return pd.Series({\n        'custom_mean': series.mean(),\n        'custom_std': series.std(),\n        'custom_skew': series.skew(),\n        'custom_range': series.max() - series.min()\n    })\n\n# Apply multiple custom functions\ncustom_stats = ts_df['Price'].rolling(window=5).apply(lambda x: pd.Series({\n    'Q25': x.quantile(0.25),\n    'Q75': x.quantile(0.75),\n    'IQR': x.quantile(0.75) - x.quantile(0.25)\n}), raw=False)\n\nprint(\"\\nMultiple custom statistics:\")\nif hasattr(custom_stats, 'columns'):\n    print(custom_stats.head(10))\nelse:\n    print(\"Custom stats calculated (single series)\")\n\n# Alternative approach for multiple stats\nts_df[['Q25_5', 'Q75_5']] = ts_df['Price'].rolling(window=5).quantile([0.25, 0.75]).unstack(level=-1)\nts_df['IQR_5'] = ts_df['Q75_5'] - ts_df['Q25_5']\n\nprint(\"\\nQuantile-based rolling statistics:\")\nprint(ts_df[['Price', 'Q25_5', 'Q75_5', 'IQR_5']].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}