{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input/Output Operations - Solutions\n\nReading and writing data with read_csv(), to_csv(), read_excel(), JSON, Parquet, chunking, and dtypes optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\nCreate a DataFrame and save it to CSV format, then read it back with specific data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\nimport numpy as np\nimport os\n\n# Create sample data\ndf_sample = pd.DataFrame({\n    'ID': [1, 2, 3, 4, 5],\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Score': [85.5, 90.0, 78.5, 92.0, 88.5],\n    'Grade': ['B', 'A', 'C', 'A', 'B']\n})\nprint(\"Original DataFrame:\")\nprint(df_sample)\nprint(df_sample.dtypes)\n\n# Save to CSV\ndf_sample.to_csv('sample_data.csv', index=False)\nprint(\"\\nSaved to CSV\")\n\n# Read back with specific dtypes\ndf_read = pd.read_csv('sample_data.csv', dtype={\n    'ID': 'int32',\n    'Name': 'string',\n    'Score': 'float32',\n    'Grade': 'category'\n})\nprint(\"\\nRead back with specific dtypes:\")\nprint(df_read.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\nRead a CSV file with custom parameters (separator, header, index_col, na_values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom CSV with different separator and missing values\ncustom_data = \"\"\"ID;Name;Score;Status\n1;Alice;85.5;Pass\n2;Bob;N/A;Pass\n3;Charlie;78.5;Fail\n4;David;92.0;Pass\n5;Eve;Missing;Pass\"\"\"\n\nwith open('custom_data.csv', 'w') as f:\n    f.write(custom_data)\n\n# Read with custom parameters\ndf_custom = pd.read_csv('custom_data.csv', \n                       sep=';', \n                       na_values=['N/A', 'Missing'],\n                       index_col='ID')\nprint(\"Custom CSV data:\")\nprint(df_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\nSave a DataFrame to Excel format with multiple sheets and read it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple DataFrames\ndf_sheet1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\ndf_sheet2 = pd.DataFrame({'X': [7, 8, 9], 'Y': [10, 11, 12]})\n\n# Save to Excel with multiple sheets\nwith pd.ExcelWriter('multi_sheet.xlsx') as writer:\n    df_sheet1.to_excel(writer, sheet_name='Sheet1', index=False)\n    df_sheet2.to_excel(writer, sheet_name='Sheet2', index=False)\n\nprint(\"Saved to Excel with multiple sheets\")\n\n# Read back specific sheets\ndf_read_sheet1 = pd.read_excel('multi_sheet.xlsx', sheet_name='Sheet1')\ndf_read_sheet2 = pd.read_excel('multi_sheet.xlsx', sheet_name='Sheet2')\n\nprint(\"\\nSheet1:\")\nprint(df_read_sheet1)\nprint(\"\\nSheet2:\")\nprint(df_read_sheet2)\n\n# Read all sheets at once\nall_sheets = pd.read_excel('multi_sheet.xlsx', sheet_name=None)\nprint(\"\\nAll sheets:\", list(all_sheets.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\nConvert a DataFrame to JSON format and read it back, exploring different orient options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = pd.DataFrame({\n    'Name': ['Alice', 'Bob'],\n    'Age': [25, 30],\n    'City': ['NYC', 'LA']\n})\n\nprint(\"Original DataFrame:\")\nprint(df_json)\n\n# Different JSON orientations\norientations = ['records', 'index', 'values', 'columns']\n\nfor orient in orientations:\n    json_str = df_json.to_json(orient=orient)\n    print(f\"\\nJSON with orient='{orient}':\")\n    print(json_str)\n    \n    # Read back\n    df_back = pd.read_json(json_str, orient=orient)\n    print(f\"Read back successfully: {df_back.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\nSave and read data in Parquet format, comparing file sizes with CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create larger dataset for comparison\nnp.random.seed(42)\nlarge_df = pd.DataFrame({\n    'ID': range(1000),\n    'Value1': np.random.randn(1000),\n    'Value2': np.random.randn(1000),\n    'Category': np.random.choice(['A', 'B', 'C'], 1000),\n    'Text': ['Sample text ' + str(i) for i in range(1000)]\n})\n\n# Save in different formats\nlarge_df.to_csv('large_data.csv', index=False)\nlarge_df.to_parquet('large_data.parquet', index=False)\n\nprint(\"File sizes:\")\nprint(f\"CSV: {os.path.getsize('large_data.csv')} bytes\")\nprint(f\"Parquet: {os.path.getsize('large_data.parquet')} bytes\")\n\n# Read back and verify\ndf_csv = pd.read_csv('large_data.csv')\ndf_parquet = pd.read_parquet('large_data.parquet')\n\nprint(f\"\\nCSV shape: {df_csv.shape}\")\nprint(f\"Parquet shape: {df_parquet.shape}\")\nprint(f\"DataFrames are equal: {df_csv.equals(df_parquet)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\nRead a large CSV file in chunks and process each chunk separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a large CSV file\nlarge_data = pd.DataFrame({\n    'ID': range(10000),\n    'Value': np.random.randn(10000)\n})\nlarge_data.to_csv('large_file.csv', index=False)\n\nprint(\"Created large CSV file with 10,000 rows\")\n\n# Read in chunks\nchunk_size = 1000\nchunk_results = []\n\nfor i, chunk in enumerate(pd.read_csv('large_file.csv', chunksize=chunk_size)):\n    # Process each chunk (example: calculate mean)\n    chunk_mean = chunk['Value'].mean()\n    chunk_results.append(chunk_mean)\n    print(f\"Chunk {i+1}: Shape {chunk.shape}, Mean: {chunk_mean:.4f}\")\n    \n    if i == 4:  # Show first 5 chunks only\n        break\n\nprint(f\"\\nOverall mean of processed chunks: {np.mean(chunk_results):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\nOptimize memory usage by specifying appropriate dtypes when reading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with different numeric ranges\noptim_data = pd.DataFrame({\n    'small_int': np.random.randint(0, 100, 1000),  # Can use int8\n    'large_int': np.random.randint(0, 1000000, 1000),  # Needs int32\n    'float_val': np.random.randn(1000),  # Can use float32\n    'category': np.random.choice(['A', 'B', 'C'], 1000)  # Can use category\n})\noptim_data.to_csv('optimization_test.csv', index=False)\n\n# Read with default dtypes\ndf_default = pd.read_csv('optimization_test.csv')\nprint(\"Default dtypes:\")\nprint(df_default.dtypes)\nprint(f\"Memory usage: {df_default.memory_usage(deep=True).sum()} bytes\")\n\n# Read with optimized dtypes\ndf_optimized = pd.read_csv('optimization_test.csv', dtype={\n    'small_int': 'int8',\n    'large_int': 'int32', \n    'float_val': 'float32',\n    'category': 'category'\n})\nprint(\"\\nOptimized dtypes:\")\nprint(df_optimized.dtypes)\nprint(f\"Memory usage: {df_optimized.memory_usage(deep=True).sum()} bytes\")\n\nmemory_saved = df_default.memory_usage(deep=True).sum() - df_optimized.memory_usage(deep=True).sum()\nprint(f\"\\nMemory saved: {memory_saved} bytes ({memory_saved/df_default.memory_usage(deep=True).sum()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\nRead only specific columns from a CSV file to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all columns\ndf_all = pd.read_csv('large_data.csv')\nprint(f\"All columns: {list(df_all.columns)}\")\nprint(f\"Shape: {df_all.shape}\")\nprint(f\"Memory usage: {df_all.memory_usage(deep=True).sum()} bytes\")\n\n# Read only specific columns\nselected_columns = ['ID', 'Value1', 'Category']\ndf_selected = pd.read_csv('large_data.csv', usecols=selected_columns)\nprint(f\"\\nSelected columns: {list(df_selected.columns)}\")\nprint(f\"Shape: {df_selected.shape}\")\nprint(f\"Memory usage: {df_selected.memory_usage(deep=True).sum()} bytes\")\n\nmemory_saved = df_all.memory_usage(deep=True).sum() - df_selected.memory_usage(deep=True).sum()\nprint(f\"\\nMemory saved: {memory_saved} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\nHandle encoding issues when reading text files with special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file with special characters\nspecial_text = \"Name,Description\\nCafé,Delicious café with résumé\\nNaïve,Naïve approach to piñata\"\n\n# Save with UTF-8 encoding\nwith open('special_chars.csv', 'w', encoding='utf-8') as f:\n    f.write(special_text)\n\nprint(\"Created file with special characters\")\n\n# Try reading with different encodings\ntry:\n    df_ascii = pd.read_csv('special_chars.csv', encoding='ascii')\n    print(\"ASCII encoding worked\")\nexcept UnicodeDecodeError:\n    print(\"ASCII encoding failed - as expected\")\n\n# Read with UTF-8\ndf_utf8 = pd.read_csv('special_chars.csv', encoding='utf-8')\nprint(\"\\nUTF-8 encoding:\")\nprint(df_utf8)\n\n# Read with error handling\ndf_ignore = pd.read_csv('special_chars.csv', encoding='ascii', encoding_errors='ignore')\nprint(\"\\nWith error='ignore' (characters removed):\")\nprint(df_ignore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10\nCompare file sizes and read speeds between different formats (CSV, Parquet, Excel) for the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n\n# Create test data\ntest_df = pd.DataFrame({\n    'ID': range(5000),\n    'Value1': np.random.randn(5000),\n    'Value2': np.random.randn(5000),\n    'Category': np.random.choice(['A', 'B', 'C', 'D'], 5000),\n    'Date': pd.date_range('2023-01-01', periods=5000, freq='H')\n})\n\nprint(f\"Test DataFrame shape: {test_df.shape}\")\n\n# Save in different formats and measure time\nformats = {\n    'CSV': (lambda: test_df.to_csv('speed_test.csv', index=False), \n            lambda: pd.read_csv('speed_test.csv')),\n    'Parquet': (lambda: test_df.to_parquet('speed_test.parquet', index=False), \n                lambda: pd.read_parquet('speed_test.parquet')),\n    'Excel': (lambda: test_df.to_excel('speed_test.xlsx', index=False), \n              lambda: pd.read_excel('speed_test.xlsx'))\n}\n\nresults = {}\n\nfor format_name, (save_func, read_func) in formats.items():\n    # Measure save time\n    start_time = time.time()\n    save_func()\n    save_time = time.time() - start_time\n    \n    # Measure file size\n    filename = f'speed_test.{format_name.lower()}'\n    if format_name == 'Excel':\n        filename = 'speed_test.xlsx'\n    elif format_name == 'Parquet':\n        filename = 'speed_test.parquet'\n    else:\n        filename = 'speed_test.csv'\n    \n    file_size = os.path.getsize(filename)\n    \n    # Measure read time\n    start_time = time.time()\n    df_read = read_func()\n    read_time = time.time() - start_time\n    \n    results[format_name] = {\n        'save_time': save_time,\n        'read_time': read_time,\n        'file_size': file_size,\n        'rows_read': len(df_read)\n    }\n\nprint(\"\\nPerformance Comparison:\")\nprint(f\"{'Format':<10} {'Save(s)':<10} {'Read(s)':<10} {'Size(KB)':<10} {'Rows':<10}\")\nprint(\"-\" * 50)\nfor format_name, stats in results.items():\n    print(f\"{format_name:<10} {stats['save_time']:<10.3f} {stats['read_time']:<10.3f} {stats['file_size']/1024:<10.1f} {stats['rows_read']:<10}\")\n\n# Clean up files\nfor filename in ['speed_test.csv', 'speed_test.parquet', 'speed_test.xlsx']:\n    if os.path.exists(filename):\n        os.remove(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}